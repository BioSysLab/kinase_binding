{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.callbacks import History, ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from data_analysis import calculate_metrics, load_weights_and_evaluate\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, roc_auc_score, auc, average_precision_score\n",
    "#import scikitplot as skplt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from NGF.preprocessing import tensorise_smiles\n",
    "from custom_layers.model_creator import encode_smiles, stage_creator\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, concatenate,Flatten\n",
    "from keras.models import Model, load_model\n",
    "import dill\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import xgboost as xgb\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(feature, squared):\n",
    "    \"\"\"Computes the pairwise distance matrix with numerical stability.\n",
    "\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    Args:\n",
    "      feature: 2-D Tensor of size [number of data, feature dimension].\n",
    "      squared: Boolean, whether or not to square the pairwise distances.\n",
    "\n",
    "    Returns:\n",
    "      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    pairwise_distances_squared = math_ops.add(\n",
    "        math_ops.reduce_sum(math_ops.square(feature), axis=[1], keepdims=True),\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.square(array_ops.transpose(feature)),\n",
    "            axis=[0],\n",
    "            keepdims=True)) - 2.0 * math_ops.matmul(feature,\n",
    "                                                    array_ops.transpose(feature))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    pairwise_distances_squared = math_ops.maximum(pairwise_distances_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = math_ops.less_equal(pairwise_distances_squared, 0.0)\n",
    "\n",
    "    # Optionally take the sqrt.\n",
    "    if squared:\n",
    "        pairwise_distances = pairwise_distances_squared\n",
    "    else:\n",
    "        pairwise_distances = math_ops.sqrt(\n",
    "            pairwise_distances_squared + math_ops.to_float(error_mask) * 1e-16)\n",
    "\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    pairwise_distances = math_ops.multiply(\n",
    "        pairwise_distances, math_ops.to_float(math_ops.logical_not(error_mask)))\n",
    "\n",
    "    num_data = array_ops.shape(feature)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = array_ops.ones_like(pairwise_distances) - array_ops.diag(\n",
    "        array_ops.ones([num_data]))\n",
    "    pairwise_distances = math_ops.multiply(pairwise_distances, mask_offdiagonals)\n",
    "    return pairwise_distances\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the maximum.\n",
    "\n",
    "    Returns:\n",
    "      masked_maximums: N-D `Tensor`.\n",
    "        The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = math_ops.reduce_min(data, dim, keepdims=True)\n",
    "    masked_maximums = math_ops.reduce_max(\n",
    "        math_ops.multiply(data - axis_minimums, mask), dim,\n",
    "        keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the minimum.\n",
    "\n",
    "    Returns:\n",
    "      masked_minimums: N-D `Tensor`.\n",
    "        The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = math_ops.reduce_max(data, dim, keepdims=True)\n",
    "    masked_minimums = math_ops.reduce_min(\n",
    "        math_ops.multiply(data - axis_maximums, mask), dim,\n",
    "        keepdims=True) + axis_maximums\n",
    "    return masked_minimums\n",
    "def triplet_loss_adapted_from_tf(y_true, y_pred):\n",
    "    del y_true\n",
    "    margin = 1.0\n",
    "    labels = y_pred[:, :1]\n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "    embeddings = y_pred[:, 1:]\n",
    "    \n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=False)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1 = 'p38'\n",
    "base_path_1 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_1 = base_path_1+f'/data/{target_1}/data.csv'\n",
    "df_p38=pd.read_csv(data_fpath_1).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_1+f'/data/{target_1}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_p38 = dill.load(in_f)\n",
    "\n",
    "with open(base_path_1+f'/data/{target_1}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_p38 = dill.load(in_f)\n",
    "    \n",
    "target_2 = 'akt1'\n",
    "base_path_2 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_2 = base_path_2+f'/data/{target_2}/data.csv'\n",
    "df_akt1 = pd.read_csv(data_fpath_2).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_2+f'/data/{target_2}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_akt1 = dill.load(in_f)\n",
    "with open(base_path_2+f'/data/{target_2}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_akt1 = dill.load(in_f)\n",
    "    \n",
    "target_3 = 'pi3k'\n",
    "base_path_3 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_3 = base_path_3+f'/data/{target_3}/data.csv'\n",
    "df_pi3k = pd.read_csv(data_fpath_3).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_3+f'/data/{target_3}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_pi3k = dill.load(in_f)\n",
    "with open(base_path_3+f'/data/{target_3}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_pi3k = dill.load(in_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_p38 = [df_p38.loc[train_val_folds_p38[0][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[1][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[2][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[3][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[4][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[5][0]],\n",
    "                df_p38.loc[train_test_folds_p38[0]]\n",
    "                 ]\n",
    "validation_p38 = [df_p38.loc[train_val_folds_p38[0][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[1][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[2][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[3][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[4][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[5][1]],\n",
    "                  df_p38.loc[train_test_folds_p38[1]]\n",
    "                   ]\n",
    "\n",
    "training_akt1 = [df_akt1.loc[train_val_folds_akt1[0][0]],\n",
    "                 df_akt1.loc[train_val_folds_akt1[1][0]],\n",
    "                 df_akt1.loc[train_val_folds_akt1[2][0]],\n",
    "                 df_akt1.loc[train_val_folds_akt1[3][0]],\n",
    "                 df_akt1.loc[train_val_folds_akt1[4][0]],\n",
    "                 df_akt1.loc[train_val_folds_akt1[5][0]],\n",
    "                 df_akt1.loc[train_test_folds_akt1[0]]\n",
    "                 ]\n",
    "validation_akt1 = [df_akt1.loc[train_val_folds_akt1[0][1]],\n",
    "                   df_akt1.loc[train_val_folds_akt1[1][1]],\n",
    "                   df_akt1.loc[train_val_folds_akt1[2][1]],\n",
    "                   df_akt1.loc[train_val_folds_akt1[3][1]],\n",
    "                   df_akt1.loc[train_val_folds_akt1[4][1]],\n",
    "                   df_akt1.loc[train_val_folds_akt1[5][1]],\n",
    "                   df_akt1.loc[train_test_folds_akt1[1]]\n",
    "                   ]\n",
    "\n",
    "training_pi3k = [df_pi3k.loc[train_val_folds_pi3k[0][0]],\n",
    "                 df_pi3k.loc[train_val_folds_pi3k[1][0]],\n",
    "                 df_pi3k.loc[train_val_folds_pi3k[2][0]],\n",
    "                 df_pi3k.loc[train_val_folds_pi3k[3][0]],\n",
    "                 df_pi3k.loc[train_val_folds_pi3k[4][0]],\n",
    "                 df_pi3k.loc[train_val_folds_pi3k[5][0]],\n",
    "                 df_pi3k.loc[train_test_folds_pi3k[0]]\n",
    "                 ]\n",
    "validation_pi3k = [df_pi3k.loc[train_val_folds_pi3k[0][1]],\n",
    "                   df_pi3k.loc[train_val_folds_pi3k[1][1]],\n",
    "                   df_pi3k.loc[train_val_folds_pi3k[2][1]],\n",
    "                   df_pi3k.loc[train_val_folds_pi3k[3][1]],\n",
    "                   df_pi3k.loc[train_val_folds_pi3k[4][1]],\n",
    "                   df_pi3k.loc[train_val_folds_pi3k[5][1]],\n",
    "                   df_pi3k.loc[train_test_folds_pi3k[1]]\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_generator_mining(bs,df_p1,df_p2,df_p3,\n",
    "                         Y_dummy_p1,Y_dummy_p2,Y_dummy_p3,\n",
    "                         smiles_list_p1,smiles_list_p2,smiles_list_p3,\n",
    "                         Y_p1,Y_p2,Y_p3,\n",
    "                         atoms_p1,bonds_p1,edges_p1,\n",
    "                         atoms_p2,bonds_p2,edges_p2,\n",
    "                         atoms_p3,bonds_p3,edges_p3,mode = 'train',aug = None):\n",
    "    \n",
    "    counter_1=int(0)\n",
    "    counter_2=int(0)\n",
    "    counter_3=int(0)\n",
    "    #Keep looping indefinetely\n",
    "    df_p1 = df_p1.reset_index(drop=True)\n",
    "    df_p2 = df_p2.reset_index(drop=True)\n",
    "    df_p3 = df_p3.reset_index(drop=True)\n",
    "    Y_p1 = np.array(list(Y_p1))\n",
    "    Y_p2 = np.array(list(Y_p2))\n",
    "    Y_p3 = np.array(list(Y_p3))\n",
    "    while True:\n",
    "        \n",
    "        #Initialize batches of inputs and outputs\n",
    "        ind1 = []\n",
    "        ind2 = []\n",
    "        ind3 = []\n",
    "        \n",
    "        d_p1=[]\n",
    "        d_p2=[]\n",
    "        d_p3=[]\n",
    "        \n",
    "        #Keep looping until we reach batch size\n",
    "        while len(ind3)<=bs: #doesn't matter if it is smi1 or smi2 since they have the same len\n",
    "            if counter_3==len(df_p3):\n",
    "                counter_3=int(0)\n",
    "                df_p3 = df_p3.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            smi_p3 = df_p3['rdkit'][counter_3]\n",
    "            ind3.append(smiles_list_p3.index(smi_p3))\n",
    "            d_p3.append(Y_dummy_p3[counter_3])\n",
    "            counter_3+=1\n",
    "            while len(ind1)<=bs:\n",
    "                if counter_1==len(df_p1):\n",
    "                    counter_1=int(0)\n",
    "                    df_p1 = df_p1.sample(frac=1).reset_index(drop=True)\n",
    "                \n",
    "                smi_p1 = df_p1['rdkit'][counter_1]\n",
    "                ind1.append(smiles_list_p1.index(smi_p1))\n",
    "                d_p1.append(Y_dummy_p1[counter_1])\n",
    "                counter_1+=1\n",
    "                \n",
    "                \n",
    "                while len(ind2)<=bs:\n",
    "            # check to see if you reached the end of the frame\n",
    "                    if counter_2==len(df_p2):\n",
    "                        counter_2=int(0)\n",
    "                        df_p2 = df_p2.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "                    smi_p2 = df_p2['rdkit'][counter_2]\n",
    "                    ind2.append(smiles_list_p2.index(smi_p2))\n",
    "                    d_p2.append(Y_dummy_p2[counter_2])\n",
    "                    counter_2+=1\n",
    "                # if we are evaluating we should now break from our \n",
    "                # loop to ensure we don't continue to fill up the batch from samples at the beginning of the file\n",
    "                \n",
    "                   \n",
    "            \n",
    "            \n",
    "\n",
    "            #smi_p1 = df_p1['rdkit'][counter]\n",
    "            #smi_p2 = df_p2['rdkit'][counter]\n",
    "            #smi_p3 = df_p3['rdkit'][counter]\n",
    "            #ind1.append(smiles_list_p1.index(smi_p1))\n",
    "            #ind2.append(smiles_list_p2.index(smi_p2))\n",
    "            #ind3.append(smiles_list_p3.index(smi_p3))\n",
    "            #d_p1.append(Y_dummy_p1[counter])\n",
    "            #d_p2.append(Y_dummy_p2[counter])\n",
    "            #d_p3.append(Y_dummy_p3[counter])\n",
    "            #counter+=1\n",
    "            \n",
    "        atoms_target_1 = np.array(atoms_p1[ind1],dtype = 'float32')\n",
    "        bonds_target_1 = np.array(bonds_p1[ind1],dtype = 'float32')\n",
    "        edges_target_1 = np.array(edges_p1[ind1],dtype = 'int32')\n",
    "        labels_target_1 = Y_p1[ind1]\n",
    "        \n",
    "        atoms_target_2 = np.array(atoms_p2[ind2],dtype = 'float32')\n",
    "        bonds_target_2 = np.array(bonds_p2[ind2],dtype = 'float32')\n",
    "        edges_target_2 = np.array(edges_p2[ind2],dtype = 'int32')\n",
    "        labels_target_2 = Y_p2[ind2]\n",
    "        \n",
    "        atoms_target_3 = np.array(atoms_p3[ind3],dtype = 'float32')\n",
    "        bonds_target_3 = np.array(bonds_p3[ind3],dtype = 'float32')\n",
    "        edges_target_3 = np.array(edges_p3[ind3],dtype = 'int32')\n",
    "        labels_target_3 = Y_p3[ind3]\n",
    "        \n",
    "        yield ({'atom_inputs_1':atoms_target_1,\n",
    "                'bond_inputs_1':bonds_target_1,\n",
    "                'edge_inputs_1':edges_target_1,\n",
    "                'labels_inputs_1':labels_target_1,\n",
    "                'atom_inputs_2':atoms_target_2,\n",
    "                'bond_inputs_2':bonds_target_2,\n",
    "                'edge_inputs_2':edges_target_2,\n",
    "                'labels_inputs_2':labels_target_2,\n",
    "                'atom_inputs_3':atoms_target_3,\n",
    "                'bond_inputs_3':bonds_target_3,\n",
    "                'edge_inputs_3':edges_target_3,\n",
    "                'labels_inputs_3':labels_target_3},{'Protein_1':np.array(d_p1,dtype = 'float32'),\n",
    "                                                    'Protein_2':np.array(d_p2,dtype = 'float32'),\n",
    "                                                    'Protein_3':np.array(d_p3,dtype = 'float32')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(encoder_params):\n",
    "        model_enc_1 = stage_creator(encoder_params, 1, conv=True)[0]\n",
    "        model_enc_2 = stage_creator(encoder_params, 2, conv=True)[0]\n",
    "        model_enc_3 = stage_creator(encoder_params, 3, conv=True)[0]\n",
    "\n",
    "        model_enc_fp_1 = stage_creator(encoder_params, 1, conv=False)[1]\n",
    "        model_enc_fp_2 = stage_creator(encoder_params, 2, conv=False)[1]\n",
    "        model_enc_fp_3 = stage_creator(encoder_params, 3, conv=False)[1]\n",
    "\n",
    "        atoms, bonds, edges = encode_smiles(encoder_params[\"max_atoms\"],\n",
    "                                            encoder_params[\"num_atom_features\"],\n",
    "                                            encoder_params[\"max_degree\"],\n",
    "                                            encoder_params[\"num_bond_features\"])\n",
    "\n",
    "        graph_conv_1 = model_enc_1([atoms, bonds, edges])\n",
    "        graph_conv_2 = model_enc_2([graph_conv_1, bonds, edges])\n",
    "        graph_conv_3 = model_enc_3([graph_conv_2, bonds, edges])\n",
    "\n",
    "        fingerprint_1 = model_enc_fp_1([graph_conv_1, bonds, edges])\n",
    "        fingerprint_1 = Lambda(lambda x: K.sum(x, axis=1), output_shape=lambda s: (s[0], s[2]))(fingerprint_1)\n",
    "\n",
    "        fingerprint_2 = model_enc_fp_2([graph_conv_2, bonds, edges])\n",
    "        fingerprint_2 = Lambda(lambda x: K.sum(x, axis=1), output_shape=lambda s: (s[0], s[2]))(fingerprint_2)\n",
    "\n",
    "        fingerprint_3 = model_enc_fp_3([graph_conv_3, bonds, edges])\n",
    "        fingerprint_3 = Lambda(lambda x: K.sum(x, axis=1), output_shape=lambda s: (s[0], s[2]))(fingerprint_3)\n",
    "\n",
    "        final_fingerprint = keras.layers.add([fingerprint_1, fingerprint_2, fingerprint_3])\n",
    "\n",
    "        return Model([atoms, bonds, edges], [final_fingerprint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_params, encoder, verbose=False):\n",
    "        atoms_1 = Input(name='atom_inputs_1',\n",
    "                      shape=(model_params['max_atoms'], model_params['num_atom_features']), dtype='float32')\n",
    "        bonds_1 = Input(name='bond_inputs_1', shape=(\n",
    "            model_params['max_atoms'], model_params['max_degree'], model_params['num_bond_features']),\n",
    "                      dtype='float32')\n",
    "        edges_1 = Input(name='edge_inputs_1', shape=(model_params['max_atoms'], model_params['max_degree']),\n",
    "                      dtype='int32')\n",
    "        encode_drug_1 = encoder([atoms_1, bonds_1, edges_1])\n",
    "        \n",
    "        atoms_2 = Input(name='atom_inputs_2',\n",
    "                      shape=(model_params['max_atoms'], model_params['num_atom_features']), dtype='float32')\n",
    "        bonds_2 = Input(name='bond_inputs_2', shape=(\n",
    "            model_params['max_atoms'], model_params['max_degree'], model_params['num_bond_features']),\n",
    "                      dtype='float32')\n",
    "        edges_2 = Input(name='edge_inputs_2', shape=(model_params['max_atoms'], model_params['max_degree']),\n",
    "                      dtype='int32')\n",
    "        encode_drug_2 = encoder([atoms_2, bonds_2, edges_2])\n",
    "        \n",
    "        atoms_3 = Input(name='atom_inputs_3',\n",
    "                      shape=(model_params['max_atoms'], model_params['num_atom_features']), dtype='float32')\n",
    "        bonds_3 = Input(name='bond_inputs_3', shape=(\n",
    "            model_params['max_atoms'], model_params['max_degree'], model_params['num_bond_features']),\n",
    "                      dtype='float32')\n",
    "        edges_3 = Input(name='edge_inputs_3', shape=(model_params['max_atoms'], model_params['max_degree']),\n",
    "                      dtype='int32')\n",
    "        encode_drug_3 = encoder([atoms_3, bonds_3, edges_3])\n",
    "        \n",
    "        conc = keras.layers.Concatenate(axis = -1)([encode_drug_1,encode_drug_2,encode_drug_3])\n",
    "        # Fully connected\n",
    "        FC1 = Dense(model_params[\"dense_size\"][0], activation='relu',kernel_initializer='random_normal')(conc)\n",
    "        FC2 = Dropout(model_params[\"dropout_rate\"][0])(FC1)\n",
    "        FC2 = Dense(model_params[\"dense_size\"][1], activation='relu',kernel_initializer='random_normal')(FC2)\n",
    "        FC2 = Dropout(model_params[\"dropout_rate\"][1])(FC2)\n",
    "        FC2 = Dense(model_params[\"dense_size\"][2], activation = None,kernel_initializer='random_normal')(FC2)\n",
    "        \n",
    "        \n",
    "        embeddings_conc = Lambda(lambda x: K.l2_normalize(x,axis=1),name = 'Embeddings_Concatenated')(FC2)\n",
    "        embeddings_1 = crop(1,0,int(model_params['dense_size'][2]/3),name = 'Target_1')(embeddings_conc) #0 to 255(256)\n",
    "        embeddings_2 = crop(1,int(model_params['dense_size'][2]/3),2*int(model_params['dense_size'][2]/3),name = 'Target_2')(embeddings_conc) #256 to 511(256)\n",
    "        embeddings_3 = crop(1,2*int(model_params['dense_size'][2]/3),3*int(model_params['dense_size'][2]/3),name = 'Target_3')(embeddings_conc)\n",
    "        \n",
    "        gcn_model = Model(inputs=[atoms_1, bonds_1, edges_1,\n",
    "                                  atoms_2, bonds_2, edges_2,\n",
    "                                  atoms_3, bonds_3, edges_3], \n",
    "                                  outputs = [embeddings_1,\n",
    "                                             embeddings_2,\n",
    "                                             embeddings_3]\n",
    "                         )\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            #print('encoder')\n",
    "            #encoder.summary()\n",
    "            print('GCN_model')\n",
    "            gcn_model.summary()\n",
    "        \n",
    "            \n",
    "        return gcn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mining(model_params,gcn_model):\n",
    "        #First Target\n",
    "        atoms_1 = Input(name='atom_inputs_1',shape=(model_params['max_atoms'], \n",
    "                                                    model_params['num_atom_features']), \n",
    "                                                    dtype='float32')\n",
    "        bonds_1 = Input(name='bond_inputs_1', shape=(model_params['max_atoms'], \n",
    "                                                     model_params['max_degree'], \n",
    "                                                     model_params['num_bond_features']),\n",
    "                                                     dtype='float32')\n",
    "        edges_1 = Input(name='edge_inputs_1', shape=(model_params['max_atoms'], \n",
    "                                                     model_params['max_degree']),\n",
    "                                                     dtype='int32')\n",
    "        \n",
    "        labels_1 = Input(name = 'labels_inputs_1',shape = (1,),dtype = 'float32')\n",
    "        \n",
    "        #Second Target\n",
    "        atoms_2 = Input(name='atom_inputs_2',shape=(model_params['max_atoms'], \n",
    "                                                    model_params['num_atom_features']), \n",
    "                                                    dtype='float32')\n",
    "        bonds_2 = Input(name='bond_inputs_2', shape=(model_params['max_atoms'], \n",
    "                                                     model_params['max_degree'], \n",
    "                                                     model_params['num_bond_features']),\n",
    "                                                     dtype='float32')\n",
    "        edges_2 = Input(name='edge_inputs_2', shape=(model_params['max_atoms'], \n",
    "                                                     model_params['max_degree']),\n",
    "                                                     dtype='int32')\n",
    "        \n",
    "        labels_2 = Input(name = 'labels_inputs_2',shape = (1,),dtype = 'float32')\n",
    "        \n",
    "        #Third Target\n",
    "        atoms_3 = Input(name='atom_inputs_3',shape=(model_params['max_atoms'], \n",
    "                                                    model_params['num_atom_features']), \n",
    "                                                    dtype='float32')\n",
    "        bonds_3 = Input(name='bond_inputs_3', shape=(model_params['max_atoms'], \n",
    "                                                     model_params['max_degree'], \n",
    "                                                     model_params['num_bond_features']),\n",
    "                                                     dtype='float32')\n",
    "        edges_3 = Input(name='edge_inputs_3', shape=(model_params['max_atoms'], model_params['max_degree']),\n",
    "                      dtype='int32')\n",
    "        \n",
    "        labels_3 = Input(name = 'labels_inputs_3',shape = (1,),dtype = 'float32')\n",
    "        encoded_1,encoded_2,encoded_3 = gcn_model([atoms_1,bonds_1,edges_1,\n",
    "                                                   atoms_2,bonds_2,edges_2,\n",
    "                                                   atoms_3,bonds_3,edges_3])\n",
    "        \n",
    "        labels_plus_embeddings_1 = concatenate([labels_1, encoded_1],name = 'Protein_1')\n",
    "        labels_plus_embeddings_2 = concatenate([labels_2, encoded_2],name = 'Protein_2')\n",
    "        labels_plus_embeddings_3 = concatenate([labels_3, encoded_3],name = 'Protein_3')\n",
    "        \n",
    "        mining_net = Model(inputs = [atoms_1,bonds_1,edges_1,labels_1,\n",
    "                                     atoms_2,bonds_2,edges_2,labels_2,\n",
    "                                     atoms_3,bonds_3,edges_3,labels_3],\n",
    "                           outputs = [labels_plus_embeddings_1,\n",
    "                                      labels_plus_embeddings_2,\n",
    "                                      labels_plus_embeddings_3])\n",
    "        adam = keras.optimizers.Adam(lr = model_params[\"lr\"], \n",
    "                                     beta_1=0.9, \n",
    "                                     beta_2=0.999, \n",
    "                                     decay=0.0, \n",
    "                                     amsgrad=False)\n",
    "    \n",
    "    \n",
    "        mining_net.compile(optimizer=adam , loss = {'Protein_1' : triplet_loss_adapted_from_tf,\n",
    "                                                    'Protein_2' : triplet_loss_adapted_from_tf,\n",
    "                                                    'Protein_3' : triplet_loss_adapted_from_tf})\n",
    "        mining_net.summary()\n",
    "        return mining_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_gcn_input(input_data):\n",
    "        x_atoms_cold, x_bonds_cold, x_edges_cold = tensorise_smiles(input_data['rdkit'],\n",
    "                                                                    max_degree=model_params['max_degree'],\n",
    "                                                                    max_atoms=model_params['max_atoms'])\n",
    "        return [x_atoms_cold, x_bonds_cold, x_edges_cold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss',patience=8, min_delta=0)\n",
    "rlr = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=4, verbose=1, min_lr=0.0000001)\n",
    "model_params = {\n",
    "        \"num_layers\" : 3,\n",
    "        \"max_atoms\" : 70,\n",
    "        \"num_atom_features\" : 62,\n",
    "        \"num_atom_features_original\" : 62,\n",
    "        \"num_bond_features\" : 6,\n",
    "        \"max_degree\" : 5,\n",
    "        \"conv_width\" : [int(80), int(100), int(160)], #[int(56), int(88), int(136)],\n",
    "        \"fp_length\" : [int(152.0), int(152.0), int(152.0)],\n",
    "        \"activ_enc\" : \"selu\",\n",
    "        \"activ_dec\" : \"selu\",\n",
    "        \"learning_rates\" : [0.001,0.001,0.001],\n",
    "        \"learning_rates_fp\": [0.005,0.005,0.005],\n",
    "        \"losses_conv\" : {\n",
    "                    \"neighbor_output\": \"mean_squared_error\",\n",
    "                    \"self_output\": \"mean_squared_error\",\n",
    "                    },\n",
    "        \"lossWeights\" : {\"neighbor_output\": 1.0, \"self_output\": 1.0},\n",
    "        \"metrics\" : \"mse\",\n",
    "        \"loss_fp\" : \"mean_squared_error\",\n",
    "        \"enc_layer_names\" : [\"enc_1\", \"enc_2\", \"enc_3\"],\n",
    "        'callbacks' : [es,rlr],\n",
    "        'adam_decay': 0.0005329142291371636,\n",
    "        'beta': 5,\n",
    "        'p': 0.004465204118126482,\n",
    "        'dense_size' : [int(3*300), int(3*200), int(3*100)], \n",
    "        'dropout_rate' : [0.27225175676555935, 0.27225175676555935],\n",
    "        'lr' : 0.0008110012706176706,\n",
    "        'batch_size' : int(288.0),\n",
    "        'n_epochs' : int(30),\n",
    "        'margin' : 1\n",
    "        }\n",
    "xgb_hyper = {\n",
    "        \"colsample_bylevel\" : 0.4371082812232264,\n",
    "        \"colsample_bytree\" : 0.4179415558635843,\n",
    "        \"gamma\" : 0.919836526180396,\n",
    "        \"eta\" : 0.41409388868400826,\n",
    "        \"max_delta_step\" : int(2),\n",
    "        \"max_depth\" : int(7),\n",
    "        \"min_child_weight\" : int(20),\n",
    "        \"alpha\" : 0.15030685758880047,\n",
    "        \"lambda\" : 12.306130216692438,\n",
    "        \"subsample\" : 0.6038298323514097,\n",
    "        \"max_bin\" : int(48.0),\n",
    "        \"eval_metric\":'auc',\n",
    "        \"objective\":'binary:logistic',\n",
    "        \"booster\":'gbtree',\n",
    "        #\"single_precision_histogram\" : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(dimension, start, end,name):\n",
    "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
    "    # example : to crop tensor x[:, :, 5:10]\n",
    "    # call slice(2, 5, 10) as you want to crop on the second dimension\n",
    "    def func(x):\n",
    "        if dimension == 0:\n",
    "            return x[start: end]\n",
    "        if dimension == 1:\n",
    "            return x[:, start: end]\n",
    "        if dimension == 2:\n",
    "            return x[:, :, start: end]\n",
    "        if dimension == 3:\n",
    "            return x[:, :, :, start: end]\n",
    "        if dimension == 4:\n",
    "            return x[:, :, :, :, start: end]\n",
    "    return Lambda(func,name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "atom_inputs_1 (InputLayer)      (None, 70, 62)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bond_inputs_1 (InputLayer)      (None, 70, 5, 6)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_inputs_1 (InputLayer)      (None, 70, 5)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "atom_inputs_2 (InputLayer)      (None, 70, 62)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bond_inputs_2 (InputLayer)      (None, 70, 5, 6)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_inputs_2 (InputLayer)      (None, 70, 5)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "atom_inputs_3 (InputLayer)      (None, 70, 62)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bond_inputs_3 (InputLayer)      (None, 70, 5, 6)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_inputs_3 (InputLayer)      (None, 70, 5)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "labels_inputs_1 (InputLayer)    (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_11 (Model)                [(None, 100), (None, 1378296     atom_inputs_1[0][0]              \n",
      "                                                                 bond_inputs_1[0][0]              \n",
      "                                                                 edge_inputs_1[0][0]              \n",
      "                                                                 atom_inputs_2[0][0]              \n",
      "                                                                 bond_inputs_2[0][0]              \n",
      "                                                                 edge_inputs_2[0][0]              \n",
      "                                                                 atom_inputs_3[0][0]              \n",
      "                                                                 bond_inputs_3[0][0]              \n",
      "                                                                 edge_inputs_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "labels_inputs_2 (InputLayer)    (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "labels_inputs_3 (InputLayer)    (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Protein_1 (Concatenate)         (None, 101)          0           labels_inputs_1[0][0]            \n",
      "                                                                 model_11[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Protein_2 (Concatenate)         (None, 101)          0           labels_inputs_2[0][0]            \n",
      "                                                                 model_11[1][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "Protein_3 (Concatenate)         (None, 101)          0           labels_inputs_3[0][0]            \n",
      "                                                                 model_11[1][2]                   \n",
      "==================================================================================================\n",
      "Total params: 1,378,296\n",
      "Trainable params: 1,376,704\n",
      "Non-trainable params: 1,592\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "47/47 [==============================] - 18s 381ms/step - loss: 2.9947 - Protein_1_loss: 0.9984 - Protein_2_loss: 0.9982 - Protein_3_loss: 0.9981\n",
      "Epoch 2/30\n",
      "47/47 [==============================] - 10s 202ms/step - loss: 2.9888 - Protein_1_loss: 0.9967 - Protein_2_loss: 0.9956 - Protein_3_loss: 0.9965\n",
      "Epoch 3/30\n",
      "47/47 [==============================] - 9s 202ms/step - loss: 2.9738 - Protein_1_loss: 0.9946 - Protein_2_loss: 0.9862 - Protein_3_loss: 0.9930\n",
      "Epoch 4/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.9656 - Protein_1_loss: 0.9953 - Protein_2_loss: 0.9817 - Protein_3_loss: 0.9887\n",
      "Epoch 5/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.9577 - Protein_1_loss: 0.9960 - Protein_2_loss: 0.9747 - Protein_3_loss: 0.9870\n",
      "Epoch 6/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.9563 - Protein_1_loss: 0.9956 - Protein_2_loss: 0.9736 - Protein_3_loss: 0.9872\n",
      "Epoch 7/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.9526 - Protein_1_loss: 0.9952 - Protein_2_loss: 0.9721 - Protein_3_loss: 0.9854\n",
      "Epoch 8/30\n",
      "47/47 [==============================] - 10s 204ms/step - loss: 2.9498 - Protein_1_loss: 0.9950 - Protein_2_loss: 0.9696 - Protein_3_loss: 0.9852\n",
      "Epoch 9/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.9434 - Protein_1_loss: 0.9949 - Protein_2_loss: 0.9638 - Protein_3_loss: 0.9847\n",
      "Epoch 10/30\n",
      "47/47 [==============================] - 10s 204ms/step - loss: 2.9420 - Protein_1_loss: 0.9948 - Protein_2_loss: 0.9624 - Protein_3_loss: 0.9848\n",
      "Epoch 11/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.9371 - Protein_1_loss: 0.9948 - Protein_2_loss: 0.9569 - Protein_3_loss: 0.9854\n",
      "Epoch 12/30\n",
      "47/47 [==============================] - 10s 204ms/step - loss: 2.9243 - Protein_1_loss: 0.9953 - Protein_2_loss: 0.9434 - Protein_3_loss: 0.9857\n",
      "Epoch 13/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.9237 - Protein_1_loss: 0.9950 - Protein_2_loss: 0.9427 - Protein_3_loss: 0.9860\n",
      "Epoch 14/30\n",
      "47/47 [==============================] - 10s 204ms/step - loss: 2.9135 - Protein_1_loss: 0.9947 - Protein_2_loss: 0.9317 - Protein_3_loss: 0.9870\n",
      "Epoch 15/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.9038 - Protein_1_loss: 0.9944 - Protein_2_loss: 0.9226 - Protein_3_loss: 0.9868\n",
      "Epoch 16/30\n",
      "47/47 [==============================] - 10s 204ms/step - loss: 2.8757 - Protein_1_loss: 0.9940 - Protein_2_loss: 0.8907 - Protein_3_loss: 0.9910\n",
      "Epoch 17/30\n",
      "47/47 [==============================] - 10s 204ms/step - loss: 2.8833 - Protein_1_loss: 0.9939 - Protein_2_loss: 0.8982 - Protein_3_loss: 0.9912\n",
      "Epoch 18/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.8620 - Protein_1_loss: 0.9940 - Protein_2_loss: 0.8765 - Protein_3_loss: 0.9915\n",
      "Epoch 19/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.8516 - Protein_1_loss: 0.9938 - Protein_2_loss: 0.8644 - Protein_3_loss: 0.9933\n",
      "Epoch 20/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.8380 - Protein_1_loss: 0.9937 - Protein_2_loss: 0.8508 - Protein_3_loss: 0.9935\n",
      "Epoch 21/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.8297 - Protein_1_loss: 0.9930 - Protein_2_loss: 0.8422 - Protein_3_loss: 0.9944\n",
      "Epoch 22/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.8252 - Protein_1_loss: 0.9936 - Protein_2_loss: 0.8380 - Protein_3_loss: 0.9937\n",
      "Epoch 23/30\n",
      "47/47 [==============================] - 10s 204ms/step - loss: 2.8127 - Protein_1_loss: 0.9935 - Protein_2_loss: 0.8240 - Protein_3_loss: 0.9952\n",
      "Epoch 24/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.8418 - Protein_1_loss: 0.9929 - Protein_2_loss: 0.8532 - Protein_3_loss: 0.9957\n",
      "Epoch 25/30\n",
      "47/47 [==============================] - 10s 202ms/step - loss: 2.7689 - Protein_1_loss: 0.9935 - Protein_2_loss: 0.7792 - Protein_3_loss: 0.9962\n",
      "Epoch 26/30\n",
      "47/47 [==============================] - 10s 203ms/step - loss: 2.8060 - Protein_1_loss: 0.9932 - Protein_2_loss: 0.8155 - Protein_3_loss: 0.9972\n",
      "Epoch 27/30\n",
      "47/47 [==============================] - 10s 204ms/step - loss: 2.7688 - Protein_1_loss: 0.9932 - Protein_2_loss: 0.7778 - Protein_3_loss: 0.9977\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 28/30\n",
      "47/47 [==============================] - 10s 205ms/step - loss: 2.7728 - Protein_1_loss: 0.9935 - Protein_2_loss: 0.7816 - Protein_3_loss: 0.9977\n",
      "Epoch 29/30\n",
      "47/47 [==============================] - 10s 204ms/step - loss: 2.7756 - Protein_1_loss: 0.9931 - Protein_2_loss: 0.7848 - Protein_3_loss: 0.9977\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 30/30\n",
      "47/47 [==============================] - 10s 204ms/step - loss: 2.7260 - Protein_1_loss: 0.9936 - Protein_2_loss: 0.7345 - Protein_3_loss: 0.9978\n",
      "[0]\ttrain-auc:0.62060\teval-auc:0.57118\n",
      "[1]\ttrain-auc:0.63473\teval-auc:0.58671\n",
      "[2]\ttrain-auc:0.64959\teval-auc:0.61395\n",
      "[3]\ttrain-auc:0.66421\teval-auc:0.62295\n",
      "[4]\ttrain-auc:0.67821\teval-auc:0.61192\n",
      "[5]\ttrain-auc:0.68499\teval-auc:0.60787\n",
      "[6]\ttrain-auc:0.69302\teval-auc:0.61180\n",
      "[7]\ttrain-auc:0.70068\teval-auc:0.61155\n",
      "[8]\ttrain-auc:0.71055\teval-auc:0.60587\n",
      "[9]\ttrain-auc:0.71197\teval-auc:0.60757\n",
      "[10]\ttrain-auc:0.71895\teval-auc:0.60120\n",
      "[11]\ttrain-auc:0.72594\teval-auc:0.60595\n",
      "[12]\ttrain-auc:0.73273\teval-auc:0.60418\n",
      "[13]\ttrain-auc:0.73893\teval-auc:0.59767\n",
      "[14]\ttrain-auc:0.74680\teval-auc:0.61451\n",
      "[15]\ttrain-auc:0.75119\teval-auc:0.60922\n",
      "[16]\ttrain-auc:0.75271\teval-auc:0.60607\n",
      "[17]\ttrain-auc:0.75770\teval-auc:0.61356\n",
      "[18]\ttrain-auc:0.76112\teval-auc:0.61425\n",
      "[19]\ttrain-auc:0.76399\teval-auc:0.61404\n",
      "[20]\ttrain-auc:0.77038\teval-auc:0.62650\n",
      "[21]\ttrain-auc:0.77292\teval-auc:0.63429\n",
      "[22]\ttrain-auc:0.77834\teval-auc:0.63630\n",
      "[23]\ttrain-auc:0.77870\teval-auc:0.63178\n",
      "[24]\ttrain-auc:0.78300\teval-auc:0.63746\n",
      "[25]\ttrain-auc:0.78459\teval-auc:0.64143\n",
      "[26]\ttrain-auc:0.79111\teval-auc:0.64247\n",
      "[27]\ttrain-auc:0.79402\teval-auc:0.63264\n",
      "[28]\ttrain-auc:0.79561\teval-auc:0.62916\n",
      "[29]\ttrain-auc:0.79801\teval-auc:0.63418\n",
      "[30]\ttrain-auc:0.80019\teval-auc:0.63118\n",
      "[31]\ttrain-auc:0.80222\teval-auc:0.63270\n",
      "[32]\ttrain-auc:0.80426\teval-auc:0.62593\n",
      "[33]\ttrain-auc:0.80678\teval-auc:0.61844\n",
      "[34]\ttrain-auc:0.81095\teval-auc:0.62108\n",
      "[35]\ttrain-auc:0.81225\teval-auc:0.62040\n",
      "[36]\ttrain-auc:0.81451\teval-auc:0.62952\n",
      "[37]\ttrain-auc:0.81681\teval-auc:0.62944\n",
      "[38]\ttrain-auc:0.81822\teval-auc:0.63101\n",
      "[39]\ttrain-auc:0.82130\teval-auc:0.62666\n",
      "[40]\ttrain-auc:0.82454\teval-auc:0.62463\n",
      "[41]\ttrain-auc:0.82844\teval-auc:0.62095\n",
      "[42]\ttrain-auc:0.83142\teval-auc:0.61889\n",
      "[43]\ttrain-auc:0.83229\teval-auc:0.62357\n",
      "[44]\ttrain-auc:0.83520\teval-auc:0.62311\n",
      "[45]\ttrain-auc:0.83686\teval-auc:0.62584\n",
      "[46]\ttrain-auc:0.84151\teval-auc:0.62623\n",
      "[47]\ttrain-auc:0.84022\teval-auc:0.62594\n",
      "[48]\ttrain-auc:0.84210\teval-auc:0.62882\n",
      "[49]\ttrain-auc:0.84722\teval-auc:0.62211\n",
      "[50]\ttrain-auc:0.84998\teval-auc:0.61435\n",
      "[51]\ttrain-auc:0.85275\teval-auc:0.61055\n",
      "[52]\ttrain-auc:0.85362\teval-auc:0.61082\n",
      "[53]\ttrain-auc:0.85338\teval-auc:0.61087\n",
      "[54]\ttrain-auc:0.85656\teval-auc:0.61825\n",
      "[55]\ttrain-auc:0.85774\teval-auc:0.62322\n",
      "[56]\ttrain-auc:0.86010\teval-auc:0.62240\n",
      "[57]\ttrain-auc:0.86172\teval-auc:0.62056\n",
      "[58]\ttrain-auc:0.86171\teval-auc:0.62145\n",
      "[59]\ttrain-auc:0.86266\teval-auc:0.61284\n",
      "[60]\ttrain-auc:0.86362\teval-auc:0.61104\n",
      "[61]\ttrain-auc:0.86386\teval-auc:0.61437\n",
      "[62]\ttrain-auc:0.86508\teval-auc:0.61907\n",
      "[63]\ttrain-auc:0.86630\teval-auc:0.61615\n",
      "[64]\ttrain-auc:0.86783\teval-auc:0.61356\n",
      "[65]\ttrain-auc:0.87089\teval-auc:0.61382\n",
      "[66]\ttrain-auc:0.87247\teval-auc:0.61730\n",
      "[67]\ttrain-auc:0.87292\teval-auc:0.62348\n",
      "[68]\ttrain-auc:0.87521\teval-auc:0.62348\n",
      "[69]\ttrain-auc:0.87561\teval-auc:0.62050\n",
      "[70]\ttrain-auc:0.87629\teval-auc:0.62531\n",
      "[71]\ttrain-auc:0.87908\teval-auc:0.62668\n",
      "[72]\ttrain-auc:0.88060\teval-auc:0.62399\n",
      "[73]\ttrain-auc:0.88163\teval-auc:0.62628\n",
      "[74]\ttrain-auc:0.88314\teval-auc:0.62190\n",
      "[75]\ttrain-auc:0.88477\teval-auc:0.62188\n",
      "[76]\ttrain-auc:0.88565\teval-auc:0.62121\n",
      "[77]\ttrain-auc:0.88673\teval-auc:0.61427\n",
      "[78]\ttrain-auc:0.88822\teval-auc:0.61749\n",
      "[79]\ttrain-auc:0.89079\teval-auc:0.61329\n",
      "[80]\ttrain-auc:0.89028\teval-auc:0.61253\n",
      "[81]\ttrain-auc:0.89070\teval-auc:0.61371\n",
      "[82]\ttrain-auc:0.89257\teval-auc:0.61840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83]\ttrain-auc:0.89410\teval-auc:0.62104\n",
      "[84]\ttrain-auc:0.89673\teval-auc:0.62454\n",
      "[85]\ttrain-auc:0.89731\teval-auc:0.62309\n",
      "[86]\ttrain-auc:0.89986\teval-auc:0.62929\n",
      "[87]\ttrain-auc:0.90041\teval-auc:0.62850\n",
      "[88]\ttrain-auc:0.90140\teval-auc:0.62676\n",
      "[89]\ttrain-auc:0.90024\teval-auc:0.62514\n",
      "[90]\ttrain-auc:0.90111\teval-auc:0.62701\n",
      "[91]\ttrain-auc:0.90228\teval-auc:0.62463\n",
      "[92]\ttrain-auc:0.90595\teval-auc:0.62300\n",
      "[93]\ttrain-auc:0.90703\teval-auc:0.62651\n",
      "[94]\ttrain-auc:0.90709\teval-auc:0.62486\n",
      "[95]\ttrain-auc:0.90833\teval-auc:0.62336\n",
      "[96]\ttrain-auc:0.90863\teval-auc:0.62541\n",
      "[97]\ttrain-auc:0.90835\teval-auc:0.62732\n",
      "[98]\ttrain-auc:0.91216\teval-auc:0.62631\n",
      "[99]\ttrain-auc:0.91140\teval-auc:0.62575\n",
      "[100]\ttrain-auc:0.91245\teval-auc:0.62146\n",
      "[101]\ttrain-auc:0.91342\teval-auc:0.61941\n",
      "[102]\ttrain-auc:0.91384\teval-auc:0.62039\n",
      "[103]\ttrain-auc:0.91383\teval-auc:0.62493\n",
      "[104]\ttrain-auc:0.91373\teval-auc:0.62929\n",
      "[105]\ttrain-auc:0.91590\teval-auc:0.62826\n",
      "[106]\ttrain-auc:0.91943\teval-auc:0.62241\n",
      "[107]\ttrain-auc:0.91953\teval-auc:0.62295\n",
      "[108]\ttrain-auc:0.92017\teval-auc:0.62831\n",
      "[109]\ttrain-auc:0.91929\teval-auc:0.62511\n",
      "[110]\ttrain-auc:0.92041\teval-auc:0.61951\n",
      "[111]\ttrain-auc:0.92069\teval-auc:0.61958\n",
      "[112]\ttrain-auc:0.92120\teval-auc:0.62420\n",
      "[113]\ttrain-auc:0.92294\teval-auc:0.62536\n",
      "[114]\ttrain-auc:0.92236\teval-auc:0.61850\n",
      "[115]\ttrain-auc:0.92349\teval-auc:0.61975\n",
      "[116]\ttrain-auc:0.92429\teval-auc:0.61901\n",
      "[117]\ttrain-auc:0.92586\teval-auc:0.61542\n",
      "[118]\ttrain-auc:0.92799\teval-auc:0.62232\n",
      "[119]\ttrain-auc:0.93122\teval-auc:0.61645\n",
      "[120]\ttrain-auc:0.93050\teval-auc:0.61780\n",
      "[121]\ttrain-auc:0.93134\teval-auc:0.61398\n",
      "[122]\ttrain-auc:0.93237\teval-auc:0.61249\n",
      "[123]\ttrain-auc:0.93247\teval-auc:0.61030\n",
      "[124]\ttrain-auc:0.93370\teval-auc:0.61090\n",
      "[125]\ttrain-auc:0.93530\teval-auc:0.61196\n",
      "[126]\ttrain-auc:0.93535\teval-auc:0.61381\n",
      "[127]\ttrain-auc:0.93569\teval-auc:0.61322\n",
      "[128]\ttrain-auc:0.93598\teval-auc:0.61051\n",
      "[129]\ttrain-auc:0.93624\teval-auc:0.60578\n",
      "[130]\ttrain-auc:0.93699\teval-auc:0.60318\n",
      "[131]\ttrain-auc:0.93707\teval-auc:0.60412\n",
      "[132]\ttrain-auc:0.93783\teval-auc:0.60660\n",
      "[133]\ttrain-auc:0.93927\teval-auc:0.60933\n",
      "[134]\ttrain-auc:0.93999\teval-auc:0.60901\n",
      "[135]\ttrain-auc:0.93947\teval-auc:0.61218\n",
      "[136]\ttrain-auc:0.93936\teval-auc:0.61292\n",
      "[137]\ttrain-auc:0.93913\teval-auc:0.61373\n",
      "[138]\ttrain-auc:0.93966\teval-auc:0.60991\n",
      "[139]\ttrain-auc:0.94006\teval-auc:0.60896\n",
      "[140]\ttrain-auc:0.94110\teval-auc:0.60373\n",
      "[141]\ttrain-auc:0.94305\teval-auc:0.60615\n",
      "[142]\ttrain-auc:0.94217\teval-auc:0.60750\n",
      "[143]\ttrain-auc:0.94335\teval-auc:0.61005\n",
      "[144]\ttrain-auc:0.94262\teval-auc:0.60862\n",
      "[145]\ttrain-auc:0.94247\teval-auc:0.60705\n",
      "[146]\ttrain-auc:0.94331\teval-auc:0.60620\n",
      "[147]\ttrain-auc:0.94462\teval-auc:0.60663\n",
      "[148]\ttrain-auc:0.94591\teval-auc:0.60780\n",
      "[149]\ttrain-auc:0.94598\teval-auc:0.60575\n",
      "[150]\ttrain-auc:0.94804\teval-auc:0.60051\n",
      "[151]\ttrain-auc:0.94750\teval-auc:0.60601\n",
      "[152]\ttrain-auc:0.94773\teval-auc:0.60508\n",
      "[153]\ttrain-auc:0.94870\teval-auc:0.60334\n",
      "[154]\ttrain-auc:0.94931\teval-auc:0.60373\n",
      "[155]\ttrain-auc:0.94940\teval-auc:0.60508\n",
      "[156]\ttrain-auc:0.95061\teval-auc:0.60441\n",
      "[157]\ttrain-auc:0.95255\teval-auc:0.60233\n",
      "[158]\ttrain-auc:0.95232\teval-auc:0.60033\n",
      "[159]\ttrain-auc:0.95227\teval-auc:0.60090\n",
      "[160]\ttrain-auc:0.95095\teval-auc:0.59675\n",
      "[161]\ttrain-auc:0.95181\teval-auc:0.59606\n",
      "[162]\ttrain-auc:0.95218\teval-auc:0.59713\n",
      "[163]\ttrain-auc:0.95261\teval-auc:0.60346\n",
      "[164]\ttrain-auc:0.95242\teval-auc:0.60649\n",
      "[165]\ttrain-auc:0.95285\teval-auc:0.61121\n",
      "[166]\ttrain-auc:0.95360\teval-auc:0.60697\n",
      "[167]\ttrain-auc:0.95342\teval-auc:0.60269\n",
      "[168]\ttrain-auc:0.95273\teval-auc:0.60279\n",
      "[169]\ttrain-auc:0.95389\teval-auc:0.60542\n",
      "[170]\ttrain-auc:0.95530\teval-auc:0.59998\n",
      "[171]\ttrain-auc:0.95525\teval-auc:0.60020\n",
      "[172]\ttrain-auc:0.95508\teval-auc:0.60239\n",
      "[173]\ttrain-auc:0.95600\teval-auc:0.60113\n",
      "[174]\ttrain-auc:0.95727\teval-auc:0.59615\n",
      "[175]\ttrain-auc:0.95750\teval-auc:0.59613\n",
      "[176]\ttrain-auc:0.95738\teval-auc:0.59432\n",
      "[177]\ttrain-auc:0.95778\teval-auc:0.59011\n",
      "[178]\ttrain-auc:0.95917\teval-auc:0.59321\n",
      "[179]\ttrain-auc:0.95963\teval-auc:0.59070\n",
      "[180]\ttrain-auc:0.95997\teval-auc:0.59186\n",
      "[181]\ttrain-auc:0.95982\teval-auc:0.59405\n",
      "[182]\ttrain-auc:0.95971\teval-auc:0.59483\n",
      "[183]\ttrain-auc:0.96008\teval-auc:0.59037\n",
      "[184]\ttrain-auc:0.96049\teval-auc:0.59467\n",
      "[185]\ttrain-auc:0.95994\teval-auc:0.59158\n",
      "[186]\ttrain-auc:0.96060\teval-auc:0.59068\n",
      "[187]\ttrain-auc:0.95979\teval-auc:0.58959\n",
      "[188]\ttrain-auc:0.95949\teval-auc:0.59310\n",
      "[189]\ttrain-auc:0.95982\teval-auc:0.59488\n",
      "[190]\ttrain-auc:0.96051\teval-auc:0.59453\n",
      "[191]\ttrain-auc:0.95965\teval-auc:0.59713\n",
      "[192]\ttrain-auc:0.95974\teval-auc:0.59456\n",
      "[193]\ttrain-auc:0.95963\teval-auc:0.59244\n",
      "[194]\ttrain-auc:0.95972\teval-auc:0.59107\n",
      "[195]\ttrain-auc:0.96034\teval-auc:0.59189\n",
      "[196]\ttrain-auc:0.96179\teval-auc:0.59494\n",
      "[197]\ttrain-auc:0.96144\teval-auc:0.59241\n",
      "[198]\ttrain-auc:0.96315\teval-auc:0.58719\n",
      "[199]\ttrain-auc:0.96369\teval-auc:0.58407\n",
      "[200]\ttrain-auc:0.96447\teval-auc:0.58639\n",
      "[201]\ttrain-auc:0.96572\teval-auc:0.57928\n",
      "[202]\ttrain-auc:0.96516\teval-auc:0.58228\n",
      "[203]\ttrain-auc:0.96566\teval-auc:0.57942\n",
      "[204]\ttrain-auc:0.96602\teval-auc:0.57714\n",
      "[205]\ttrain-auc:0.96590\teval-auc:0.57875\n",
      "[206]\ttrain-auc:0.96660\teval-auc:0.58167\n",
      "[207]\ttrain-auc:0.96666\teval-auc:0.58334\n",
      "[208]\ttrain-auc:0.96714\teval-auc:0.58514\n",
      "[209]\ttrain-auc:0.96771\teval-auc:0.58211\n",
      "[210]\ttrain-auc:0.96742\teval-auc:0.58172\n",
      "[211]\ttrain-auc:0.96724\teval-auc:0.58844\n",
      "[212]\ttrain-auc:0.96795\teval-auc:0.58542\n",
      "[213]\ttrain-auc:0.96752\teval-auc:0.58228\n",
      "[214]\ttrain-auc:0.96765\teval-auc:0.58287\n",
      "[215]\ttrain-auc:0.96806\teval-auc:0.58206\n",
      "[216]\ttrain-auc:0.96843\teval-auc:0.58065\n",
      "[217]\ttrain-auc:0.96802\teval-auc:0.58177\n",
      "[218]\ttrain-auc:0.96716\teval-auc:0.58206\n",
      "[219]\ttrain-auc:0.96739\teval-auc:0.57958\n",
      "[220]\ttrain-auc:0.96748\teval-auc:0.57877\n",
      "[221]\ttrain-auc:0.96746\teval-auc:0.58365\n",
      "[222]\ttrain-auc:0.96811\teval-auc:0.58636\n",
      "[223]\ttrain-auc:0.96769\teval-auc:0.58716\n",
      "[224]\ttrain-auc:0.96832\teval-auc:0.58480\n",
      "[225]\ttrain-auc:0.96950\teval-auc:0.58250\n",
      "[226]\ttrain-auc:0.96929\teval-auc:0.58122\n",
      "[227]\ttrain-auc:0.96977\teval-auc:0.57680\n",
      "[228]\ttrain-auc:0.96932\teval-auc:0.57660\n",
      "[229]\ttrain-auc:0.96979\teval-auc:0.57925\n",
      "[230]\ttrain-auc:0.97032\teval-auc:0.58535\n",
      "[231]\ttrain-auc:0.97076\teval-auc:0.58146\n",
      "[232]\ttrain-auc:0.97127\teval-auc:0.58080\n",
      "[233]\ttrain-auc:0.97132\teval-auc:0.58082\n",
      "[234]\ttrain-auc:0.97185\teval-auc:0.57905\n",
      "[235]\ttrain-auc:0.97246\teval-auc:0.57751\n",
      "[236]\ttrain-auc:0.97276\teval-auc:0.57905\n",
      "[237]\ttrain-auc:0.97331\teval-auc:0.58065\n",
      "[238]\ttrain-auc:0.97294\teval-auc:0.58143\n",
      "[239]\ttrain-auc:0.97263\teval-auc:0.58317\n",
      "[240]\ttrain-auc:0.97204\teval-auc:0.58400\n",
      "[241]\ttrain-auc:0.97202\teval-auc:0.58466\n",
      "[242]\ttrain-auc:0.97174\teval-auc:0.58501\n",
      "[243]\ttrain-auc:0.97203\teval-auc:0.58677\n",
      "[244]\ttrain-auc:0.97242\teval-auc:0.58422\n",
      "[245]\ttrain-auc:0.97316\teval-auc:0.58360\n",
      "[246]\ttrain-auc:0.97311\teval-auc:0.58152\n",
      "[247]\ttrain-auc:0.97271\teval-auc:0.58125\n",
      "[248]\ttrain-auc:0.97244\teval-auc:0.58209\n",
      "[249]\ttrain-auc:0.97352\teval-auc:0.58048\n",
      "[250]\ttrain-auc:0.97337\teval-auc:0.58456\n",
      "[251]\ttrain-auc:0.97447\teval-auc:0.58435\n",
      "[252]\ttrain-auc:0.97487\teval-auc:0.58503\n",
      "[253]\ttrain-auc:0.97429\teval-auc:0.58686\n",
      "[254]\ttrain-auc:0.97427\teval-auc:0.58391\n",
      "[255]\ttrain-auc:0.97411\teval-auc:0.58192\n",
      "[256]\ttrain-auc:0.97487\teval-auc:0.58160\n",
      "[257]\ttrain-auc:0.97532\teval-auc:0.58413\n",
      "[258]\ttrain-auc:0.97560\teval-auc:0.57857\n",
      "[259]\ttrain-auc:0.97538\teval-auc:0.57613\n",
      "[260]\ttrain-auc:0.97607\teval-auc:0.57885\n",
      "[261]\ttrain-auc:0.97647\teval-auc:0.58015\n",
      "[262]\ttrain-auc:0.97684\teval-auc:0.58115\n",
      "[263]\ttrain-auc:0.97736\teval-auc:0.58118\n",
      "[264]\ttrain-auc:0.97738\teval-auc:0.58052\n",
      "[265]\ttrain-auc:0.97732\teval-auc:0.58334\n",
      "[266]\ttrain-auc:0.97720\teval-auc:0.58306\n",
      "[267]\ttrain-auc:0.97635\teval-auc:0.58540\n",
      "[268]\ttrain-auc:0.97704\teval-auc:0.58357\n",
      "[269]\ttrain-auc:0.97723\teval-auc:0.58253\n",
      "[270]\ttrain-auc:0.97676\teval-auc:0.58784\n",
      "[271]\ttrain-auc:0.97640\teval-auc:0.58458\n",
      "[272]\ttrain-auc:0.97669\teval-auc:0.58164\n",
      "[273]\ttrain-auc:0.97660\teval-auc:0.58683\n",
      "[274]\ttrain-auc:0.97716\teval-auc:0.58289\n",
      "[275]\ttrain-auc:0.97777\teval-auc:0.57972\n",
      "[276]\ttrain-auc:0.97771\teval-auc:0.58032\n",
      "[277]\ttrain-auc:0.97782\teval-auc:0.57787\n",
      "[278]\ttrain-auc:0.97817\teval-auc:0.57843\n",
      "[279]\ttrain-auc:0.97838\teval-auc:0.58021\n",
      "[280]\ttrain-auc:0.97808\teval-auc:0.58056\n",
      "[281]\ttrain-auc:0.97886\teval-auc:0.57941\n",
      "[282]\ttrain-auc:0.97937\teval-auc:0.58257\n",
      "[283]\ttrain-auc:0.97999\teval-auc:0.58087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[284]\ttrain-auc:0.97994\teval-auc:0.58317\n",
      "[285]\ttrain-auc:0.98006\teval-auc:0.58776\n",
      "[286]\ttrain-auc:0.98032\teval-auc:0.58657\n",
      "[287]\ttrain-auc:0.98077\teval-auc:0.58846\n",
      "[288]\ttrain-auc:0.98049\teval-auc:0.58876\n",
      "[289]\ttrain-auc:0.98058\teval-auc:0.58475\n",
      "[290]\ttrain-auc:0.98078\teval-auc:0.58276\n",
      "[291]\ttrain-auc:0.98104\teval-auc:0.58045\n",
      "[292]\ttrain-auc:0.98100\teval-auc:0.58000\n",
      "[293]\ttrain-auc:0.98058\teval-auc:0.58037\n",
      "[294]\ttrain-auc:0.98096\teval-auc:0.57712\n",
      "[295]\ttrain-auc:0.98142\teval-auc:0.57879\n",
      "[296]\ttrain-auc:0.98129\teval-auc:0.58085\n",
      "[297]\ttrain-auc:0.98139\teval-auc:0.58261\n",
      "[298]\ttrain-auc:0.98115\teval-auc:0.57919\n",
      "[299]\ttrain-auc:0.98165\teval-auc:0.57821\n",
      "[0]\ttrain-auc:0.94955\teval-auc:0.81528\n",
      "[1]\ttrain-auc:0.96709\teval-auc:0.81639\n",
      "[2]\ttrain-auc:0.97361\teval-auc:0.84136\n",
      "[3]\ttrain-auc:0.97647\teval-auc:0.85069\n",
      "[4]\ttrain-auc:0.97724\teval-auc:0.85217\n",
      "[5]\ttrain-auc:0.97871\teval-auc:0.85691\n",
      "[6]\ttrain-auc:0.97890\teval-auc:0.85640\n",
      "[7]\ttrain-auc:0.97912\teval-auc:0.85648\n",
      "[8]\ttrain-auc:0.97934\teval-auc:0.85678\n",
      "[9]\ttrain-auc:0.97932\teval-auc:0.85704\n",
      "[10]\ttrain-auc:0.97946\teval-auc:0.86088\n",
      "[11]\ttrain-auc:0.97960\teval-auc:0.85994\n",
      "[12]\ttrain-auc:0.98005\teval-auc:0.86255\n",
      "[13]\ttrain-auc:0.98011\teval-auc:0.86703\n",
      "[14]\ttrain-auc:0.98064\teval-auc:0.86757\n",
      "[15]\ttrain-auc:0.98073\teval-auc:0.86688\n",
      "[16]\ttrain-auc:0.98081\teval-auc:0.86637\n",
      "[17]\ttrain-auc:0.98094\teval-auc:0.86654\n",
      "[18]\ttrain-auc:0.98130\teval-auc:0.86985\n",
      "[19]\ttrain-auc:0.98151\teval-auc:0.86983\n",
      "[20]\ttrain-auc:0.98151\teval-auc:0.87013\n",
      "[21]\ttrain-auc:0.98184\teval-auc:0.87051\n",
      "[22]\ttrain-auc:0.98213\teval-auc:0.87130\n",
      "[23]\ttrain-auc:0.98217\teval-auc:0.87111\n",
      "[24]\ttrain-auc:0.98218\teval-auc:0.87069\n",
      "[25]\ttrain-auc:0.98241\teval-auc:0.87086\n",
      "[26]\ttrain-auc:0.98247\teval-auc:0.87133\n",
      "[27]\ttrain-auc:0.98257\teval-auc:0.87133\n",
      "[28]\ttrain-auc:0.98273\teval-auc:0.87013\n",
      "[29]\ttrain-auc:0.98250\teval-auc:0.87045\n",
      "[30]\ttrain-auc:0.98270\teval-auc:0.86972\n",
      "[31]\ttrain-auc:0.98265\teval-auc:0.86955\n",
      "[32]\ttrain-auc:0.98288\teval-auc:0.86898\n",
      "[33]\ttrain-auc:0.98299\teval-auc:0.86945\n",
      "[34]\ttrain-auc:0.98320\teval-auc:0.86966\n",
      "[35]\ttrain-auc:0.98373\teval-auc:0.86878\n",
      "[36]\ttrain-auc:0.98360\teval-auc:0.86851\n",
      "[37]\ttrain-auc:0.98364\teval-auc:0.86878\n",
      "[38]\ttrain-auc:0.98362\teval-auc:0.86859\n",
      "[39]\ttrain-auc:0.98364\teval-auc:0.86846\n",
      "[40]\ttrain-auc:0.98368\teval-auc:0.86784\n",
      "[41]\ttrain-auc:0.98402\teval-auc:0.86814\n",
      "[42]\ttrain-auc:0.98400\teval-auc:0.86801\n",
      "[43]\ttrain-auc:0.98364\teval-auc:0.86793\n",
      "[44]\ttrain-auc:0.98405\teval-auc:0.86780\n",
      "[45]\ttrain-auc:0.98424\teval-auc:0.86801\n",
      "[46]\ttrain-auc:0.98420\teval-auc:0.86836\n",
      "[47]\ttrain-auc:0.98415\teval-auc:0.86930\n",
      "[48]\ttrain-auc:0.98417\teval-auc:0.86930\n",
      "[49]\ttrain-auc:0.98432\teval-auc:0.86891\n",
      "[50]\ttrain-auc:0.98439\teval-auc:0.86793\n",
      "[51]\ttrain-auc:0.98439\teval-auc:0.86793\n",
      "[52]\ttrain-auc:0.98452\teval-auc:0.86797\n",
      "[53]\ttrain-auc:0.98450\teval-auc:0.86631\n",
      "[54]\ttrain-auc:0.98464\teval-auc:0.86605\n",
      "[55]\ttrain-auc:0.98482\teval-auc:0.86592\n",
      "[56]\ttrain-auc:0.98508\teval-auc:0.86543\n",
      "[57]\ttrain-auc:0.98503\teval-auc:0.86436\n",
      "[58]\ttrain-auc:0.98497\teval-auc:0.86443\n",
      "[59]\ttrain-auc:0.98489\teval-auc:0.86426\n",
      "[60]\ttrain-auc:0.98484\teval-auc:0.86443\n",
      "[61]\ttrain-auc:0.98510\teval-auc:0.86374\n",
      "[62]\ttrain-auc:0.98495\teval-auc:0.86532\n",
      "[63]\ttrain-auc:0.98489\teval-auc:0.86464\n",
      "[64]\ttrain-auc:0.98498\teval-auc:0.86451\n",
      "[65]\ttrain-auc:0.98518\teval-auc:0.86428\n",
      "[66]\ttrain-auc:0.98535\teval-auc:0.86330\n",
      "[67]\ttrain-auc:0.98525\teval-auc:0.86351\n",
      "[68]\ttrain-auc:0.98543\teval-auc:0.86500\n",
      "[69]\ttrain-auc:0.98531\teval-auc:0.86483\n",
      "[70]\ttrain-auc:0.98541\teval-auc:0.86364\n",
      "[71]\ttrain-auc:0.98541\teval-auc:0.86364\n",
      "[72]\ttrain-auc:0.98529\teval-auc:0.86462\n",
      "[73]\ttrain-auc:0.98519\teval-auc:0.86530\n",
      "[74]\ttrain-auc:0.98529\teval-auc:0.86406\n",
      "[75]\ttrain-auc:0.98553\teval-auc:0.86503\n",
      "[76]\ttrain-auc:0.98570\teval-auc:0.86498\n",
      "[77]\ttrain-auc:0.98572\teval-auc:0.86656\n",
      "[78]\ttrain-auc:0.98569\teval-auc:0.86746\n",
      "[79]\ttrain-auc:0.98586\teval-auc:0.86631\n",
      "[80]\ttrain-auc:0.98586\teval-auc:0.86631\n",
      "[81]\ttrain-auc:0.98581\teval-auc:0.86725\n",
      "[82]\ttrain-auc:0.98607\teval-auc:0.86703\n",
      "[83]\ttrain-auc:0.98610\teval-auc:0.86827\n",
      "[84]\ttrain-auc:0.98598\teval-auc:0.86934\n",
      "[85]\ttrain-auc:0.98623\teval-auc:0.86682\n",
      "[86]\ttrain-auc:0.98614\teval-auc:0.86708\n",
      "[87]\ttrain-auc:0.98599\teval-auc:0.86716\n",
      "[88]\ttrain-auc:0.98620\teval-auc:0.86733\n",
      "[89]\ttrain-auc:0.98635\teval-auc:0.86729\n",
      "[90]\ttrain-auc:0.98616\teval-auc:0.86791\n",
      "[91]\ttrain-auc:0.98602\teval-auc:0.86825\n",
      "[92]\ttrain-auc:0.98625\teval-auc:0.86727\n",
      "[93]\ttrain-auc:0.98645\teval-auc:0.86573\n",
      "[94]\ttrain-auc:0.98650\teval-auc:0.86740\n",
      "[95]\ttrain-auc:0.98624\teval-auc:0.86735\n",
      "[96]\ttrain-auc:0.98645\teval-auc:0.86846\n",
      "[97]\ttrain-auc:0.98664\teval-auc:0.86859\n",
      "[98]\ttrain-auc:0.98657\teval-auc:0.86889\n",
      "[99]\ttrain-auc:0.98657\teval-auc:0.86889\n",
      "[100]\ttrain-auc:0.98663\teval-auc:0.86838\n",
      "[101]\ttrain-auc:0.98679\teval-auc:0.86520\n",
      "[102]\ttrain-auc:0.98677\teval-auc:0.86498\n",
      "[103]\ttrain-auc:0.98661\teval-auc:0.86596\n",
      "[104]\ttrain-auc:0.98674\teval-auc:0.86562\n",
      "[105]\ttrain-auc:0.98674\teval-auc:0.86562\n",
      "[106]\ttrain-auc:0.98658\teval-auc:0.86584\n",
      "[107]\ttrain-auc:0.98683\teval-auc:0.86562\n",
      "[108]\ttrain-auc:0.98699\teval-auc:0.86413\n",
      "[109]\ttrain-auc:0.98694\teval-auc:0.86596\n",
      "[110]\ttrain-auc:0.98694\teval-auc:0.86596\n",
      "[111]\ttrain-auc:0.98708\teval-auc:0.86490\n",
      "[112]\ttrain-auc:0.98688\teval-auc:0.86511\n",
      "[113]\ttrain-auc:0.98687\teval-auc:0.86455\n",
      "[114]\ttrain-auc:0.98711\teval-auc:0.86396\n",
      "[115]\ttrain-auc:0.98715\teval-auc:0.86588\n",
      "[116]\ttrain-auc:0.98738\teval-auc:0.86562\n",
      "[117]\ttrain-auc:0.98757\teval-auc:0.86434\n",
      "[118]\ttrain-auc:0.98764\teval-auc:0.86308\n",
      "[119]\ttrain-auc:0.98764\teval-auc:0.86308\n",
      "[120]\ttrain-auc:0.98750\teval-auc:0.86338\n",
      "[121]\ttrain-auc:0.98750\teval-auc:0.86338\n",
      "[122]\ttrain-auc:0.98750\teval-auc:0.86402\n",
      "[123]\ttrain-auc:0.98762\teval-auc:0.86308\n",
      "[124]\ttrain-auc:0.98746\teval-auc:0.86432\n",
      "[125]\ttrain-auc:0.98725\teval-auc:0.86543\n",
      "[126]\ttrain-auc:0.98738\teval-auc:0.86526\n",
      "[127]\ttrain-auc:0.98732\teval-auc:0.86590\n",
      "[128]\ttrain-auc:0.98733\teval-auc:0.86633\n",
      "[129]\ttrain-auc:0.98750\teval-auc:0.86569\n",
      "[130]\ttrain-auc:0.98735\teval-auc:0.86543\n",
      "[131]\ttrain-auc:0.98752\teval-auc:0.86618\n",
      "[132]\ttrain-auc:0.98752\teval-auc:0.86618\n",
      "[133]\ttrain-auc:0.98752\teval-auc:0.86618\n",
      "[134]\ttrain-auc:0.98780\teval-auc:0.86592\n",
      "[135]\ttrain-auc:0.98772\teval-auc:0.86699\n",
      "[136]\ttrain-auc:0.98804\teval-auc:0.86639\n",
      "[137]\ttrain-auc:0.98813\teval-auc:0.86609\n",
      "[138]\ttrain-auc:0.98806\teval-auc:0.86686\n",
      "[139]\ttrain-auc:0.98821\teval-auc:0.86665\n",
      "[140]\ttrain-auc:0.98825\teval-auc:0.86757\n",
      "[141]\ttrain-auc:0.98813\teval-auc:0.86821\n",
      "[142]\ttrain-auc:0.98790\teval-auc:0.86838\n",
      "[143]\ttrain-auc:0.98804\teval-auc:0.86808\n",
      "[144]\ttrain-auc:0.98827\teval-auc:0.86778\n",
      "[145]\ttrain-auc:0.98827\teval-auc:0.86778\n",
      "[146]\ttrain-auc:0.98804\teval-auc:0.86876\n",
      "[147]\ttrain-auc:0.98830\teval-auc:0.86898\n",
      "[148]\ttrain-auc:0.98801\teval-auc:0.86898\n",
      "[149]\ttrain-auc:0.98786\teval-auc:0.86923\n",
      "[150]\ttrain-auc:0.98795\teval-auc:0.86968\n",
      "[151]\ttrain-auc:0.98788\teval-auc:0.86934\n",
      "[152]\ttrain-auc:0.98800\teval-auc:0.86814\n",
      "[153]\ttrain-auc:0.98821\teval-auc:0.86806\n",
      "[154]\ttrain-auc:0.98810\teval-auc:0.86806\n",
      "[155]\ttrain-auc:0.98806\teval-auc:0.86789\n",
      "[156]\ttrain-auc:0.98789\teval-auc:0.86750\n",
      "[157]\ttrain-auc:0.98795\teval-auc:0.86761\n",
      "[158]\ttrain-auc:0.98788\teval-auc:0.86795\n",
      "[159]\ttrain-auc:0.98819\teval-auc:0.86825\n",
      "[160]\ttrain-auc:0.98821\teval-auc:0.86731\n",
      "[161]\ttrain-auc:0.98821\teval-auc:0.86731\n",
      "[162]\ttrain-auc:0.98821\teval-auc:0.86731\n",
      "[163]\ttrain-auc:0.98821\teval-auc:0.86731\n",
      "[164]\ttrain-auc:0.98840\teval-auc:0.86641\n",
      "[165]\ttrain-auc:0.98820\teval-auc:0.86635\n",
      "[166]\ttrain-auc:0.98840\teval-auc:0.86524\n",
      "[167]\ttrain-auc:0.98869\teval-auc:0.86511\n",
      "[168]\ttrain-auc:0.98849\teval-auc:0.86524\n",
      "[169]\ttrain-auc:0.98838\teval-auc:0.86622\n",
      "[170]\ttrain-auc:0.98860\teval-auc:0.86537\n",
      "[171]\ttrain-auc:0.98844\teval-auc:0.86550\n",
      "[172]\ttrain-auc:0.98870\teval-auc:0.86562\n",
      "[173]\ttrain-auc:0.98883\teval-auc:0.86287\n",
      "[174]\ttrain-auc:0.98908\teval-auc:0.86210\n",
      "[175]\ttrain-auc:0.98905\teval-auc:0.86184\n",
      "[176]\ttrain-auc:0.98915\teval-auc:0.86432\n",
      "[177]\ttrain-auc:0.98895\teval-auc:0.86479\n",
      "[178]\ttrain-auc:0.98885\teval-auc:0.86487\n",
      "[179]\ttrain-auc:0.98912\teval-auc:0.86355\n",
      "[180]\ttrain-auc:0.98899\teval-auc:0.86445\n",
      "[181]\ttrain-auc:0.98921\teval-auc:0.86389\n",
      "[182]\ttrain-auc:0.98922\teval-auc:0.86517\n",
      "[183]\ttrain-auc:0.98916\teval-auc:0.86526\n",
      "[184]\ttrain-auc:0.98916\teval-auc:0.86526\n",
      "[185]\ttrain-auc:0.98937\teval-auc:0.86475\n",
      "[186]\ttrain-auc:0.98950\teval-auc:0.86537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[187]\ttrain-auc:0.98940\teval-auc:0.86592\n",
      "[188]\ttrain-auc:0.98936\teval-auc:0.86479\n",
      "[189]\ttrain-auc:0.98924\teval-auc:0.86464\n",
      "[190]\ttrain-auc:0.98937\teval-auc:0.86272\n",
      "[191]\ttrain-auc:0.98951\teval-auc:0.85853\n",
      "[192]\ttrain-auc:0.98938\teval-auc:0.85909\n",
      "[193]\ttrain-auc:0.98932\teval-auc:0.85943\n",
      "[194]\ttrain-auc:0.98932\teval-auc:0.85943\n",
      "[195]\ttrain-auc:0.98915\teval-auc:0.85998\n",
      "[196]\ttrain-auc:0.98915\teval-auc:0.85998\n",
      "[197]\ttrain-auc:0.98940\teval-auc:0.85986\n",
      "[198]\ttrain-auc:0.98935\teval-auc:0.86216\n",
      "[199]\ttrain-auc:0.98956\teval-auc:0.86220\n",
      "[200]\ttrain-auc:0.98975\teval-auc:0.86161\n",
      "[201]\ttrain-auc:0.99001\teval-auc:0.86131\n",
      "[202]\ttrain-auc:0.99014\teval-auc:0.86050\n",
      "[203]\ttrain-auc:0.99014\teval-auc:0.86050\n",
      "[204]\ttrain-auc:0.99006\teval-auc:0.86097\n",
      "[205]\ttrain-auc:0.98986\teval-auc:0.86114\n",
      "[206]\ttrain-auc:0.98986\teval-auc:0.86114\n",
      "[207]\ttrain-auc:0.99002\teval-auc:0.86118\n",
      "[208]\ttrain-auc:0.99002\teval-auc:0.86118\n",
      "[209]\ttrain-auc:0.99002\teval-auc:0.86118\n",
      "[210]\ttrain-auc:0.98982\teval-auc:0.86139\n",
      "[211]\ttrain-auc:0.98982\teval-auc:0.86139\n",
      "[212]\ttrain-auc:0.98973\teval-auc:0.86178\n",
      "[213]\ttrain-auc:0.99004\teval-auc:0.85704\n",
      "[214]\ttrain-auc:0.99004\teval-auc:0.85704\n",
      "[215]\ttrain-auc:0.99004\teval-auc:0.85704\n",
      "[216]\ttrain-auc:0.99004\teval-auc:0.85704\n",
      "[217]\ttrain-auc:0.98982\teval-auc:0.85716\n",
      "[218]\ttrain-auc:0.98982\teval-auc:0.85716\n",
      "[219]\ttrain-auc:0.98995\teval-auc:0.85733\n",
      "[220]\ttrain-auc:0.99005\teval-auc:0.85682\n",
      "[221]\ttrain-auc:0.99018\teval-auc:0.85610\n",
      "[222]\ttrain-auc:0.99021\teval-auc:0.85435\n",
      "[223]\ttrain-auc:0.99007\teval-auc:0.85498\n",
      "[224]\ttrain-auc:0.99001\teval-auc:0.85575\n",
      "[225]\ttrain-auc:0.98998\teval-auc:0.85917\n",
      "[226]\ttrain-auc:0.98983\teval-auc:0.85887\n",
      "[227]\ttrain-auc:0.98998\teval-auc:0.85960\n",
      "[228]\ttrain-auc:0.98990\teval-auc:0.86238\n",
      "[229]\ttrain-auc:0.98990\teval-auc:0.86238\n",
      "[230]\ttrain-auc:0.98989\teval-auc:0.86359\n",
      "[231]\ttrain-auc:0.98973\teval-auc:0.86289\n",
      "[232]\ttrain-auc:0.99001\teval-auc:0.86233\n",
      "[233]\ttrain-auc:0.99014\teval-auc:0.86084\n",
      "[234]\ttrain-auc:0.99014\teval-auc:0.86084\n",
      "[235]\ttrain-auc:0.99001\teval-auc:0.86101\n",
      "[236]\ttrain-auc:0.99001\teval-auc:0.86101\n",
      "[237]\ttrain-auc:0.99001\teval-auc:0.86101\n",
      "[238]\ttrain-auc:0.99001\teval-auc:0.86101\n",
      "[239]\ttrain-auc:0.99017\teval-auc:0.86118\n",
      "[240]\ttrain-auc:0.99000\teval-auc:0.86161\n",
      "[241]\ttrain-auc:0.99017\teval-auc:0.86225\n",
      "[242]\ttrain-auc:0.99017\teval-auc:0.86225\n",
      "[243]\ttrain-auc:0.99036\teval-auc:0.86152\n",
      "[244]\ttrain-auc:0.99046\teval-auc:0.86225\n",
      "[245]\ttrain-auc:0.99046\teval-auc:0.86225\n",
      "[246]\ttrain-auc:0.99046\teval-auc:0.86225\n",
      "[247]\ttrain-auc:0.99046\teval-auc:0.86225\n",
      "[248]\ttrain-auc:0.99046\teval-auc:0.86225\n",
      "[249]\ttrain-auc:0.99046\teval-auc:0.86225\n",
      "[250]\ttrain-auc:0.99037\teval-auc:0.86285\n",
      "[251]\ttrain-auc:0.99037\teval-auc:0.86285\n",
      "[252]\ttrain-auc:0.99037\teval-auc:0.86285\n",
      "[253]\ttrain-auc:0.99041\teval-auc:0.86507\n",
      "[254]\ttrain-auc:0.99041\teval-auc:0.86507\n",
      "[255]\ttrain-auc:0.99041\teval-auc:0.86507\n",
      "[256]\ttrain-auc:0.99041\teval-auc:0.86507\n",
      "[257]\ttrain-auc:0.99041\teval-auc:0.86648\n",
      "[258]\ttrain-auc:0.99027\teval-auc:0.86656\n",
      "[259]\ttrain-auc:0.99027\teval-auc:0.86656\n",
      "[260]\ttrain-auc:0.99026\teval-auc:0.86605\n",
      "[261]\ttrain-auc:0.99049\teval-auc:0.86639\n",
      "[262]\ttrain-auc:0.99049\teval-auc:0.86639\n",
      "[263]\ttrain-auc:0.99047\teval-auc:0.86665\n",
      "[264]\ttrain-auc:0.99036\teval-auc:0.86716\n",
      "[265]\ttrain-auc:0.99032\teval-auc:0.86729\n",
      "[266]\ttrain-auc:0.99032\teval-auc:0.86729\n",
      "[267]\ttrain-auc:0.99032\teval-auc:0.86729\n",
      "[268]\ttrain-auc:0.99052\teval-auc:0.86708\n",
      "[269]\ttrain-auc:0.99037\teval-auc:0.86801\n",
      "[270]\ttrain-auc:0.99062\teval-auc:0.86814\n",
      "[271]\ttrain-auc:0.99080\teval-auc:0.86673\n",
      "[272]\ttrain-auc:0.99071\teval-auc:0.86695\n",
      "[273]\ttrain-auc:0.99071\teval-auc:0.86695\n",
      "[274]\ttrain-auc:0.99071\teval-auc:0.86695\n",
      "[275]\ttrain-auc:0.99088\teval-auc:0.86609\n",
      "[276]\ttrain-auc:0.99076\teval-auc:0.86643\n",
      "[277]\ttrain-auc:0.99076\teval-auc:0.86643\n",
      "[278]\ttrain-auc:0.99076\teval-auc:0.86643\n",
      "[279]\ttrain-auc:0.99068\teval-auc:0.86725\n",
      "[280]\ttrain-auc:0.99059\teval-auc:0.86921\n",
      "[281]\ttrain-auc:0.99070\teval-auc:0.86789\n",
      "[282]\ttrain-auc:0.99087\teval-auc:0.86592\n",
      "[283]\ttrain-auc:0.99100\teval-auc:0.86579\n",
      "[284]\ttrain-auc:0.99092\teval-auc:0.86584\n",
      "[285]\ttrain-auc:0.99110\teval-auc:0.86571\n",
      "[286]\ttrain-auc:0.99116\teval-auc:0.86746\n",
      "[287]\ttrain-auc:0.99116\teval-auc:0.86746\n",
      "[288]\ttrain-auc:0.99107\teval-auc:0.86763\n",
      "[289]\ttrain-auc:0.99107\teval-auc:0.86763\n",
      "[290]\ttrain-auc:0.99112\teval-auc:0.86776\n",
      "[291]\ttrain-auc:0.99112\teval-auc:0.86776\n",
      "[292]\ttrain-auc:0.99096\teval-auc:0.86819\n",
      "[293]\ttrain-auc:0.99104\teval-auc:0.86759\n",
      "[294]\ttrain-auc:0.99104\teval-auc:0.86759\n",
      "[295]\ttrain-auc:0.99104\teval-auc:0.86759\n",
      "[296]\ttrain-auc:0.99131\teval-auc:0.86532\n",
      "[297]\ttrain-auc:0.99121\teval-auc:0.86631\n",
      "[298]\ttrain-auc:0.99121\teval-auc:0.86631\n",
      "[299]\ttrain-auc:0.99144\teval-auc:0.86575\n",
      "[0]\ttrain-auc:0.80234\teval-auc:0.67466\n",
      "[1]\ttrain-auc:0.83057\teval-auc:0.69420\n",
      "[2]\ttrain-auc:0.83966\teval-auc:0.69949\n",
      "[3]\ttrain-auc:0.84622\teval-auc:0.71575\n",
      "[4]\ttrain-auc:0.85013\teval-auc:0.71327\n",
      "[5]\ttrain-auc:0.85194\teval-auc:0.71727\n",
      "[6]\ttrain-auc:0.85380\teval-auc:0.71782\n",
      "[7]\ttrain-auc:0.85621\teval-auc:0.71738\n",
      "[8]\ttrain-auc:0.85818\teval-auc:0.71864\n",
      "[9]\ttrain-auc:0.86099\teval-auc:0.72054\n",
      "[10]\ttrain-auc:0.86363\teval-auc:0.71935\n",
      "[11]\ttrain-auc:0.86577\teval-auc:0.71771\n",
      "[12]\ttrain-auc:0.86801\teval-auc:0.71908\n",
      "[13]\ttrain-auc:0.86945\teval-auc:0.71700\n",
      "[14]\ttrain-auc:0.87142\teval-auc:0.71879\n",
      "[15]\ttrain-auc:0.87181\teval-auc:0.71898\n",
      "[16]\ttrain-auc:0.87354\teval-auc:0.72008\n",
      "[17]\ttrain-auc:0.87557\teval-auc:0.71941\n",
      "[18]\ttrain-auc:0.87715\teval-auc:0.71831\n",
      "[19]\ttrain-auc:0.87906\teval-auc:0.71986\n",
      "[20]\ttrain-auc:0.88035\teval-auc:0.72093\n",
      "[21]\ttrain-auc:0.88097\teval-auc:0.71897\n",
      "[22]\ttrain-auc:0.88235\teval-auc:0.71803\n",
      "[23]\ttrain-auc:0.88295\teval-auc:0.71766\n",
      "[24]\ttrain-auc:0.88467\teval-auc:0.71752\n",
      "[25]\ttrain-auc:0.88501\teval-auc:0.71871\n",
      "[26]\ttrain-auc:0.88642\teval-auc:0.72012\n",
      "[27]\ttrain-auc:0.88689\teval-auc:0.72082\n",
      "[28]\ttrain-auc:0.88823\teval-auc:0.71851\n",
      "[29]\ttrain-auc:0.88972\teval-auc:0.71796\n",
      "[30]\ttrain-auc:0.89091\teval-auc:0.71894\n",
      "[31]\ttrain-auc:0.89226\teval-auc:0.71903\n",
      "[32]\ttrain-auc:0.89232\teval-auc:0.72072\n",
      "[33]\ttrain-auc:0.89331\teval-auc:0.72135\n",
      "[34]\ttrain-auc:0.89397\teval-auc:0.72098\n",
      "[35]\ttrain-auc:0.89492\teval-auc:0.71846\n",
      "[36]\ttrain-auc:0.89595\teval-auc:0.72077\n",
      "[37]\ttrain-auc:0.89593\teval-auc:0.71978\n",
      "[38]\ttrain-auc:0.89651\teval-auc:0.71886\n",
      "[39]\ttrain-auc:0.89794\teval-auc:0.71993\n",
      "[40]\ttrain-auc:0.89990\teval-auc:0.72175\n",
      "[41]\ttrain-auc:0.90153\teval-auc:0.72333\n",
      "[42]\ttrain-auc:0.90227\teval-auc:0.72372\n",
      "[43]\ttrain-auc:0.90306\teval-auc:0.72468\n",
      "[44]\ttrain-auc:0.90458\teval-auc:0.72279\n",
      "[45]\ttrain-auc:0.90573\teval-auc:0.72267\n",
      "[46]\ttrain-auc:0.90689\teval-auc:0.72385\n",
      "[47]\ttrain-auc:0.90738\teval-auc:0.72515\n",
      "[48]\ttrain-auc:0.90752\teval-auc:0.72563\n",
      "[49]\ttrain-auc:0.90721\teval-auc:0.72840\n",
      "[50]\ttrain-auc:0.90830\teval-auc:0.72641\n",
      "[51]\ttrain-auc:0.90927\teval-auc:0.72626\n",
      "[52]\ttrain-auc:0.91021\teval-auc:0.72615\n",
      "[53]\ttrain-auc:0.91098\teval-auc:0.72608\n",
      "[54]\ttrain-auc:0.90993\teval-auc:0.72625\n",
      "[55]\ttrain-auc:0.90933\teval-auc:0.72652\n",
      "[56]\ttrain-auc:0.91083\teval-auc:0.72767\n",
      "[57]\ttrain-auc:0.91280\teval-auc:0.72725\n",
      "[58]\ttrain-auc:0.91330\teval-auc:0.72626\n",
      "[59]\ttrain-auc:0.91448\teval-auc:0.72639\n",
      "[60]\ttrain-auc:0.91526\teval-auc:0.72630\n",
      "[61]\ttrain-auc:0.91551\teval-auc:0.72651\n",
      "[62]\ttrain-auc:0.91549\teval-auc:0.72632\n",
      "[63]\ttrain-auc:0.91591\teval-auc:0.72764\n",
      "[64]\ttrain-auc:0.91660\teval-auc:0.72515\n",
      "[65]\ttrain-auc:0.91753\teval-auc:0.72338\n",
      "[66]\ttrain-auc:0.91750\teval-auc:0.72410\n",
      "[67]\ttrain-auc:0.91842\teval-auc:0.72449\n",
      "[68]\ttrain-auc:0.92048\teval-auc:0.72338\n",
      "[69]\ttrain-auc:0.92059\teval-auc:0.72251\n",
      "[70]\ttrain-auc:0.92069\teval-auc:0.72487\n",
      "[71]\ttrain-auc:0.92017\teval-auc:0.72324\n",
      "[72]\ttrain-auc:0.92053\teval-auc:0.72240\n",
      "[73]\ttrain-auc:0.92151\teval-auc:0.72218\n",
      "[74]\ttrain-auc:0.92116\teval-auc:0.72320\n",
      "[75]\ttrain-auc:0.92185\teval-auc:0.72396\n",
      "[76]\ttrain-auc:0.92294\teval-auc:0.72453\n",
      "[77]\ttrain-auc:0.92353\teval-auc:0.72355\n",
      "[78]\ttrain-auc:0.92395\teval-auc:0.72301\n",
      "[79]\ttrain-auc:0.92503\teval-auc:0.72188\n",
      "[80]\ttrain-auc:0.92562\teval-auc:0.72278\n",
      "[81]\ttrain-auc:0.92498\teval-auc:0.72203\n",
      "[82]\ttrain-auc:0.92570\teval-auc:0.72323\n",
      "[83]\ttrain-auc:0.92749\teval-auc:0.72431\n",
      "[84]\ttrain-auc:0.92755\teval-auc:0.72357\n",
      "[85]\ttrain-auc:0.92837\teval-auc:0.72332\n",
      "[86]\ttrain-auc:0.92900\teval-auc:0.72473\n",
      "[87]\ttrain-auc:0.92997\teval-auc:0.72409\n",
      "[88]\ttrain-auc:0.93081\teval-auc:0.72300\n",
      "[89]\ttrain-auc:0.93141\teval-auc:0.72276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90]\ttrain-auc:0.93243\teval-auc:0.72416\n",
      "[91]\ttrain-auc:0.93240\teval-auc:0.72387\n",
      "[92]\ttrain-auc:0.93384\teval-auc:0.72221\n",
      "[93]\ttrain-auc:0.93471\teval-auc:0.72248\n",
      "[94]\ttrain-auc:0.93549\teval-auc:0.72412\n",
      "[95]\ttrain-auc:0.93511\teval-auc:0.72430\n",
      "[96]\ttrain-auc:0.93576\teval-auc:0.72180\n",
      "[97]\ttrain-auc:0.93611\teval-auc:0.72140\n",
      "[98]\ttrain-auc:0.93687\teval-auc:0.71999\n",
      "[99]\ttrain-auc:0.93717\teval-auc:0.72156\n",
      "[100]\ttrain-auc:0.93685\teval-auc:0.72223\n",
      "[101]\ttrain-auc:0.93626\teval-auc:0.72207\n",
      "[102]\ttrain-auc:0.93704\teval-auc:0.72294\n",
      "[103]\ttrain-auc:0.93835\teval-auc:0.72335\n",
      "[104]\ttrain-auc:0.93918\teval-auc:0.72332\n",
      "[105]\ttrain-auc:0.93999\teval-auc:0.72385\n",
      "[106]\ttrain-auc:0.94020\teval-auc:0.72440\n",
      "[107]\ttrain-auc:0.94084\teval-auc:0.72443\n",
      "[108]\ttrain-auc:0.94091\teval-auc:0.72619\n",
      "[109]\ttrain-auc:0.94190\teval-auc:0.72429\n",
      "[110]\ttrain-auc:0.94230\teval-auc:0.72560\n",
      "[111]\ttrain-auc:0.94236\teval-auc:0.72384\n",
      "[112]\ttrain-auc:0.94284\teval-auc:0.72329\n",
      "[113]\ttrain-auc:0.94215\teval-auc:0.72260\n",
      "[114]\ttrain-auc:0.94329\teval-auc:0.72544\n",
      "[115]\ttrain-auc:0.94367\teval-auc:0.72590\n",
      "[116]\ttrain-auc:0.94398\teval-auc:0.72748\n",
      "[117]\ttrain-auc:0.94287\teval-auc:0.72785\n",
      "[118]\ttrain-auc:0.94405\teval-auc:0.72862\n",
      "[119]\ttrain-auc:0.94469\teval-auc:0.72774\n",
      "[120]\ttrain-auc:0.94409\teval-auc:0.72877\n",
      "[121]\ttrain-auc:0.94505\teval-auc:0.72706\n",
      "[122]\ttrain-auc:0.94535\teval-auc:0.72899\n",
      "[123]\ttrain-auc:0.94450\teval-auc:0.72867\n",
      "[124]\ttrain-auc:0.94484\teval-auc:0.72853\n",
      "[125]\ttrain-auc:0.94459\teval-auc:0.73076\n",
      "[126]\ttrain-auc:0.94625\teval-auc:0.72957\n",
      "[127]\ttrain-auc:0.94631\teval-auc:0.72780\n",
      "[128]\ttrain-auc:0.94676\teval-auc:0.72735\n",
      "[129]\ttrain-auc:0.94703\teval-auc:0.72675\n",
      "[130]\ttrain-auc:0.94798\teval-auc:0.72700\n",
      "[131]\ttrain-auc:0.94928\teval-auc:0.72547\n",
      "[132]\ttrain-auc:0.94939\teval-auc:0.72536\n",
      "[133]\ttrain-auc:0.94991\teval-auc:0.72633\n",
      "[134]\ttrain-auc:0.94995\teval-auc:0.72688\n",
      "[135]\ttrain-auc:0.95086\teval-auc:0.72842\n",
      "[136]\ttrain-auc:0.95132\teval-auc:0.72932\n",
      "[137]\ttrain-auc:0.95179\teval-auc:0.72915\n",
      "[138]\ttrain-auc:0.95174\teval-auc:0.72821\n",
      "[139]\ttrain-auc:0.95262\teval-auc:0.72823\n",
      "[140]\ttrain-auc:0.95292\teval-auc:0.72792\n",
      "[141]\ttrain-auc:0.95265\teval-auc:0.72713\n",
      "[142]\ttrain-auc:0.95352\teval-auc:0.72761\n",
      "[143]\ttrain-auc:0.95416\teval-auc:0.72643\n",
      "[144]\ttrain-auc:0.95442\teval-auc:0.72691\n",
      "[145]\ttrain-auc:0.95430\teval-auc:0.72589\n",
      "[146]\ttrain-auc:0.95410\teval-auc:0.72573\n",
      "[147]\ttrain-auc:0.95437\teval-auc:0.72545\n",
      "[148]\ttrain-auc:0.95433\teval-auc:0.72564\n",
      "[149]\ttrain-auc:0.95477\teval-auc:0.72634\n",
      "[150]\ttrain-auc:0.95633\teval-auc:0.72459\n",
      "[151]\ttrain-auc:0.95605\teval-auc:0.72324\n",
      "[152]\ttrain-auc:0.95608\teval-auc:0.72370\n",
      "[153]\ttrain-auc:0.95647\teval-auc:0.72457\n",
      "[154]\ttrain-auc:0.95670\teval-auc:0.72487\n",
      "[155]\ttrain-auc:0.95668\teval-auc:0.72370\n",
      "[156]\ttrain-auc:0.95659\teval-auc:0.72468\n",
      "[157]\ttrain-auc:0.95740\teval-auc:0.72713\n",
      "[158]\ttrain-auc:0.95733\teval-auc:0.72589\n",
      "[159]\ttrain-auc:0.95717\teval-auc:0.72642\n",
      "[160]\ttrain-auc:0.95676\teval-auc:0.72558\n",
      "[161]\ttrain-auc:0.95695\teval-auc:0.72579\n",
      "[162]\ttrain-auc:0.95725\teval-auc:0.72394\n",
      "[163]\ttrain-auc:0.95742\teval-auc:0.72245\n",
      "[164]\ttrain-auc:0.95752\teval-auc:0.72311\n",
      "[165]\ttrain-auc:0.95776\teval-auc:0.72206\n",
      "[166]\ttrain-auc:0.95873\teval-auc:0.72441\n",
      "[167]\ttrain-auc:0.95842\teval-auc:0.72623\n",
      "[168]\ttrain-auc:0.95846\teval-auc:0.72707\n",
      "[169]\ttrain-auc:0.95872\teval-auc:0.72530\n",
      "[170]\ttrain-auc:0.95957\teval-auc:0.72623\n",
      "[171]\ttrain-auc:0.95951\teval-auc:0.72508\n",
      "[172]\ttrain-auc:0.95981\teval-auc:0.72628\n",
      "[173]\ttrain-auc:0.96018\teval-auc:0.72634\n",
      "[174]\ttrain-auc:0.96147\teval-auc:0.72488\n",
      "[175]\ttrain-auc:0.96152\teval-auc:0.72402\n",
      "[176]\ttrain-auc:0.96174\teval-auc:0.72434\n",
      "[177]\ttrain-auc:0.96163\teval-auc:0.72232\n",
      "[178]\ttrain-auc:0.96159\teval-auc:0.72380\n",
      "[179]\ttrain-auc:0.96167\teval-auc:0.72386\n",
      "[180]\ttrain-auc:0.96202\teval-auc:0.72317\n",
      "[181]\ttrain-auc:0.96238\teval-auc:0.72139\n",
      "[182]\ttrain-auc:0.96217\teval-auc:0.72275\n",
      "[183]\ttrain-auc:0.96232\teval-auc:0.72070\n",
      "[184]\ttrain-auc:0.96269\teval-auc:0.72009\n",
      "[185]\ttrain-auc:0.96360\teval-auc:0.72234\n",
      "[186]\ttrain-auc:0.96397\teval-auc:0.72215\n",
      "[187]\ttrain-auc:0.96450\teval-auc:0.72396\n",
      "[188]\ttrain-auc:0.96434\teval-auc:0.72307\n",
      "[189]\ttrain-auc:0.96494\teval-auc:0.72378\n",
      "[190]\ttrain-auc:0.96548\teval-auc:0.72264\n",
      "[191]\ttrain-auc:0.96518\teval-auc:0.72199\n",
      "[192]\ttrain-auc:0.96566\teval-auc:0.72340\n",
      "[193]\ttrain-auc:0.96564\teval-auc:0.72159\n",
      "[194]\ttrain-auc:0.96581\teval-auc:0.72238\n",
      "[195]\ttrain-auc:0.96641\teval-auc:0.72242\n",
      "[196]\ttrain-auc:0.96688\teval-auc:0.72301\n",
      "[197]\ttrain-auc:0.96658\teval-auc:0.72361\n",
      "[198]\ttrain-auc:0.96609\teval-auc:0.72396\n",
      "[199]\ttrain-auc:0.96617\teval-auc:0.72465\n",
      "[200]\ttrain-auc:0.96592\teval-auc:0.72316\n",
      "[201]\ttrain-auc:0.96642\teval-auc:0.72352\n",
      "[202]\ttrain-auc:0.96689\teval-auc:0.72337\n",
      "[203]\ttrain-auc:0.96714\teval-auc:0.72183\n",
      "[204]\ttrain-auc:0.96686\teval-auc:0.72297\n",
      "[205]\ttrain-auc:0.96705\teval-auc:0.72414\n",
      "[206]\ttrain-auc:0.96744\teval-auc:0.72474\n",
      "[207]\ttrain-auc:0.96721\teval-auc:0.72514\n",
      "[208]\ttrain-auc:0.96725\teval-auc:0.72581\n",
      "[209]\ttrain-auc:0.96797\teval-auc:0.72571\n",
      "[210]\ttrain-auc:0.96859\teval-auc:0.72628\n",
      "[211]\ttrain-auc:0.96791\teval-auc:0.72635\n",
      "[212]\ttrain-auc:0.96779\teval-auc:0.72614\n",
      "[213]\ttrain-auc:0.96760\teval-auc:0.72452\n",
      "[214]\ttrain-auc:0.96825\teval-auc:0.72468\n",
      "[215]\ttrain-auc:0.96776\teval-auc:0.72458\n",
      "[216]\ttrain-auc:0.96844\teval-auc:0.72480\n",
      "[217]\ttrain-auc:0.96881\teval-auc:0.72458\n",
      "[218]\ttrain-auc:0.96885\teval-auc:0.72446\n",
      "[219]\ttrain-auc:0.96908\teval-auc:0.72328\n",
      "[220]\ttrain-auc:0.96853\teval-auc:0.72213\n",
      "[221]\ttrain-auc:0.96857\teval-auc:0.72201\n",
      "[222]\ttrain-auc:0.96847\teval-auc:0.72224\n",
      "[223]\ttrain-auc:0.96886\teval-auc:0.72211\n",
      "[224]\ttrain-auc:0.96921\teval-auc:0.72303\n",
      "[225]\ttrain-auc:0.96986\teval-auc:0.72306\n",
      "[226]\ttrain-auc:0.97092\teval-auc:0.72094\n",
      "[227]\ttrain-auc:0.97103\teval-auc:0.72050\n",
      "[228]\ttrain-auc:0.97083\teval-auc:0.72237\n",
      "[229]\ttrain-auc:0.97093\teval-auc:0.72202\n",
      "[230]\ttrain-auc:0.97134\teval-auc:0.72351\n",
      "[231]\ttrain-auc:0.97116\teval-auc:0.72233\n",
      "[232]\ttrain-auc:0.97139\teval-auc:0.72316\n",
      "[233]\ttrain-auc:0.97174\teval-auc:0.72131\n",
      "[234]\ttrain-auc:0.97137\teval-auc:0.72148\n",
      "[235]\ttrain-auc:0.97154\teval-auc:0.72160\n",
      "[236]\ttrain-auc:0.97106\teval-auc:0.72264\n",
      "[237]\ttrain-auc:0.97127\teval-auc:0.72262\n",
      "[238]\ttrain-auc:0.97157\teval-auc:0.72335\n",
      "[239]\ttrain-auc:0.97143\teval-auc:0.72198\n",
      "[240]\ttrain-auc:0.97199\teval-auc:0.72069\n",
      "[241]\ttrain-auc:0.97209\teval-auc:0.72081\n",
      "[242]\ttrain-auc:0.97272\teval-auc:0.72075\n",
      "[243]\ttrain-auc:0.97309\teval-auc:0.72028\n",
      "[244]\ttrain-auc:0.97279\teval-auc:0.71958\n",
      "[245]\ttrain-auc:0.97309\teval-auc:0.72027\n",
      "[246]\ttrain-auc:0.97311\teval-auc:0.71955\n",
      "[247]\ttrain-auc:0.97318\teval-auc:0.71996\n",
      "[248]\ttrain-auc:0.97358\teval-auc:0.72017\n",
      "[249]\ttrain-auc:0.97402\teval-auc:0.72031\n",
      "[250]\ttrain-auc:0.97387\teval-auc:0.72014\n",
      "[251]\ttrain-auc:0.97434\teval-auc:0.72018\n",
      "[252]\ttrain-auc:0.97434\teval-auc:0.72126\n",
      "[253]\ttrain-auc:0.97466\teval-auc:0.72218\n",
      "[254]\ttrain-auc:0.97429\teval-auc:0.72123\n",
      "[255]\ttrain-auc:0.97436\teval-auc:0.72126\n",
      "[256]\ttrain-auc:0.97463\teval-auc:0.72129\n",
      "[257]\ttrain-auc:0.97517\teval-auc:0.72164\n",
      "[258]\ttrain-auc:0.97566\teval-auc:0.72142\n",
      "[259]\ttrain-auc:0.97534\teval-auc:0.72115\n",
      "[260]\ttrain-auc:0.97539\teval-auc:0.72135\n",
      "[261]\ttrain-auc:0.97601\teval-auc:0.72208\n",
      "[262]\ttrain-auc:0.97503\teval-auc:0.72381\n",
      "[263]\ttrain-auc:0.97546\teval-auc:0.72471\n",
      "[264]\ttrain-auc:0.97575\teval-auc:0.72533\n",
      "[265]\ttrain-auc:0.97631\teval-auc:0.72514\n",
      "[266]\ttrain-auc:0.97621\teval-auc:0.72587\n",
      "[267]\ttrain-auc:0.97643\teval-auc:0.72384\n",
      "[268]\ttrain-auc:0.97720\teval-auc:0.72416\n",
      "[269]\ttrain-auc:0.97666\teval-auc:0.72401\n",
      "[270]\ttrain-auc:0.97677\teval-auc:0.72378\n",
      "[271]\ttrain-auc:0.97724\teval-auc:0.72274\n",
      "[272]\ttrain-auc:0.97783\teval-auc:0.72261\n",
      "[273]\ttrain-auc:0.97735\teval-auc:0.72324\n",
      "[274]\ttrain-auc:0.97717\teval-auc:0.72331\n",
      "[275]\ttrain-auc:0.97691\teval-auc:0.72327\n",
      "[276]\ttrain-auc:0.97702\teval-auc:0.72316\n",
      "[277]\ttrain-auc:0.97729\teval-auc:0.72334\n",
      "[278]\ttrain-auc:0.97775\teval-auc:0.72283\n",
      "[279]\ttrain-auc:0.97736\teval-auc:0.72183\n",
      "[280]\ttrain-auc:0.97766\teval-auc:0.72142\n",
      "[281]\ttrain-auc:0.97776\teval-auc:0.72248\n",
      "[282]\ttrain-auc:0.97762\teval-auc:0.72354\n",
      "[283]\ttrain-auc:0.97767\teval-auc:0.72278\n",
      "[284]\ttrain-auc:0.97792\teval-auc:0.72280\n",
      "[285]\ttrain-auc:0.97773\teval-auc:0.72363\n",
      "[286]\ttrain-auc:0.97787\teval-auc:0.72312\n",
      "[287]\ttrain-auc:0.97799\teval-auc:0.72299\n",
      "[288]\ttrain-auc:0.97803\teval-auc:0.72288\n",
      "[289]\ttrain-auc:0.97903\teval-auc:0.72310\n",
      "[290]\ttrain-auc:0.97965\teval-auc:0.72175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[291]\ttrain-auc:0.97993\teval-auc:0.72115\n",
      "[292]\ttrain-auc:0.98039\teval-auc:0.72055\n",
      "[293]\ttrain-auc:0.98010\teval-auc:0.72088\n",
      "[294]\ttrain-auc:0.98035\teval-auc:0.72011\n",
      "[295]\ttrain-auc:0.98031\teval-auc:0.72134\n",
      "[296]\ttrain-auc:0.98028\teval-auc:0.72136\n",
      "[297]\ttrain-auc:0.98032\teval-auc:0.72210\n",
      "[298]\ttrain-auc:0.98069\teval-auc:0.72194\n",
      "[299]\ttrain-auc:0.98070\teval-auc:0.72200\n"
     ]
    }
   ],
   "source": [
    "train_p1 = {}\n",
    "train_p2 = {}\n",
    "train_p3 = {}\n",
    "val_p1 = {}\n",
    "val_p2 = {}\n",
    "val_p3 = {}\n",
    "es2 = EarlyStopping(monitor='loss',patience=15, min_delta=0)\n",
    "rlr2 = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=2, verbose=1, min_lr=0.000000001)\n",
    "for i in range(0,1):#range(len(training_p38)):\n",
    "    \n",
    "        #First Target\n",
    "        X_atoms_cold_p38,X_bonds_cold_p38,X_edges_cold_p38 = dataframe_to_gcn_input(validation_p38[i])\n",
    "        Y_cold_p38 = validation_p38[i].Binary\n",
    "        Y_dummy_cold_p38 = np.empty((X_atoms_cold_p38.shape[0],int(model_params['dense_size'][2]/3)+1))\n",
    "        X_atoms_train_p38, X_bonds_train_p38, X_edges_train_p38 = dataframe_to_gcn_input(training_p38[i])\n",
    "        Y_p38 = training_p38[i].Binary\n",
    "        Y_dummy_train_p38 = np.empty((X_atoms_train_p38.shape[0],int(model_params['dense_size'][2]/3)+1))\n",
    "        smiles_list_p38 = list(training_p38[i]['rdkit'])\n",
    "        \n",
    "        #Second Target\n",
    "        X_atoms_cold_akt1, X_bonds_cold_akt1, X_edges_cold_akt1 = dataframe_to_gcn_input(validation_akt1[i])\n",
    "        Y_cold_akt1 = validation_akt1[i].Binary\n",
    "        Y_dummy_cold_akt1 = np.empty((X_atoms_cold_akt1.shape[0],int(model_params['dense_size'][2]/3)+1))\n",
    "        X_atoms_train_akt1, X_bonds_train_akt1, X_edges_train_akt1 = dataframe_to_gcn_input(training_akt1[i])\n",
    "        Y_akt1 = training_akt1[i].Binary\n",
    "        Y_dummy_train_akt1 = np.empty((X_atoms_train_akt1.shape[0],int(model_params['dense_size'][2]/3)+1))\n",
    "        smiles_list_akt1 = list(training_akt1[i]['rdkit'])\n",
    "        \n",
    "        #Third Target\n",
    "        X_atoms_cold_pi3k,X_bonds_cold_pi3k,X_edges_cold_pi3k = dataframe_to_gcn_input(validation_pi3k[i])\n",
    "        Y_cold_pi3k = validation_pi3k[i].Binary\n",
    "        Y_dummy_cold_pi3k = np.empty((X_atoms_cold_pi3k.shape[0],int(model_params['dense_size'][2]/3)+1))\n",
    "        X_atoms_train_pi3k, X_bonds_train_pi3k, X_edges_train_pi3k = dataframe_to_gcn_input(training_pi3k[i])\n",
    "        Y_pi3k = training_pi3k[i].Binary\n",
    "        Y_dummy_train_pi3k = np.empty((X_atoms_train_pi3k.shape[0],int(model_params['dense_size'][2]/3)+1))\n",
    "        smiles_list_pi3k = list(training_pi3k[i]['rdkit'])\n",
    "        \n",
    "        gcn_encoder = build_encoder(model_params)\n",
    "        gcn_model = build_model(model_params,gcn_encoder)\n",
    "        gcn_mining = build_mining(model_params,gcn_model)\n",
    "        train_GEN = csv_generator_mining(model_params['batch_size'],training_p38[i],training_akt1[i],training_pi3k[i],\n",
    "                                  Y_dummy_train_p38,Y_dummy_train_akt1,Y_dummy_train_pi3k,\n",
    "                                  smiles_list_p38,smiles_list_akt1,smiles_list_pi3k,\n",
    "                                  Y_p38,Y_akt1,Y_pi3k,\n",
    "                                  X_atoms_train_p38, X_bonds_train_p38, X_edges_train_p38,\n",
    "                                  X_atoms_train_akt1, X_bonds_train_akt1, X_edges_train_akt1,\n",
    "                                  X_atoms_train_pi3k, X_bonds_train_pi3k, X_edges_train_pi3k)\n",
    "        NUM_TRAIN = np.sum([len(training_p38[i]),len(training_akt1[i]),len(training_pi3k[i])])\n",
    "        gcn_mining.fit_generator(train_GEN,\n",
    "                      steps_per_epoch = ceil(2*NUM_TRAIN/model_params['batch_size']),\n",
    "                      epochs = model_params['n_epochs'],\n",
    "                      shuffle = True,\n",
    "                      validation_data = None,\n",
    "                      callbacks = [es2,rlr2])\n",
    "        emb_train_1,t1,t2 = gcn_model.predict([X_atoms_train_p38,X_bonds_train_p38,X_edges_train_p38,\n",
    "                                        X_atoms_train_p38,X_bonds_train_p38,X_edges_train_p38,\n",
    "                                        X_atoms_train_p38,X_bonds_train_p38,X_edges_train_p38])\n",
    "        t1,emb_train_2,t2 = gcn_model.predict([X_atoms_train_akt1,X_bonds_train_akt1,X_edges_train_akt1,\n",
    "                                        X_atoms_train_akt1,X_bonds_train_akt1,X_edges_train_akt1,\n",
    "                                        X_atoms_train_akt1,X_bonds_train_akt1,X_edges_train_akt1])\n",
    "        t1,t2,emb_train_3 = gcn_model.predict([X_atoms_train_pi3k,X_bonds_train_pi3k,X_edges_train_pi3k,\n",
    "                                        X_atoms_train_pi3k,X_bonds_train_pi3k,X_edges_train_pi3k,\n",
    "                                        X_atoms_train_pi3k,X_bonds_train_pi3k,X_edges_train_pi3k])\n",
    "\n",
    "\n",
    "        emb_val_1,t1,t2 = gcn_model.predict([X_atoms_cold_p38,X_bonds_cold_p38,X_edges_cold_p38,\n",
    "                                            X_atoms_cold_p38,X_bonds_cold_p38,X_edges_cold_p38,\n",
    "                                            X_atoms_cold_p38,X_bonds_cold_p38,X_edges_cold_p38])\n",
    "        t1,emb_val_2,t2 = gcn_model.predict([X_atoms_cold_akt1,X_bonds_cold_akt1,X_edges_cold_akt1,\n",
    "                                            X_atoms_cold_akt1,X_bonds_cold_akt1,X_edges_cold_akt1,\n",
    "                                            X_atoms_cold_akt1,X_bonds_cold_akt1,X_edges_cold_akt1])\n",
    "        t1,t2,emb_val_3 = gcn_model.predict([X_atoms_cold_pi3k,X_bonds_cold_pi3k,X_edges_cold_pi3k,\n",
    "                                            X_atoms_cold_pi3k,X_bonds_cold_pi3k,X_edges_cold_pi3k,\n",
    "                                            X_atoms_cold_pi3k,X_bonds_cold_pi3k,X_edges_cold_pi3k])\n",
    "        \n",
    "        del t1,t2, gcn_mining,gcn_encoder,gcn_model,X_atoms_cold_p38,X_bonds_cold_p38,X_edges_cold_p38\n",
    "        del X_atoms_train_p38, X_bonds_train_p38, X_edges_train_p38,X_atoms_cold_pi3k,X_bonds_cold_pi3k,X_edges_cold_pi3k\n",
    "        del X_atoms_train_pi3k, X_bonds_train_pi3k, X_edges_train_pi3k,X_atoms_cold_akt1, X_bonds_cold_akt1, X_edges_cold_akt1\n",
    "        del X_atoms_train_akt1, X_bonds_train_akt1, X_edges_train_akt1,train_GEN\n",
    "        \n",
    "        #First\n",
    "        dmatrix_train_1 = xgb.DMatrix(data = emb_train_1,label = Y_p38)\n",
    "        dmatrix_cold_1  = xgb.DMatrix(data = emb_val_1,label = Y_cold_p38)\n",
    "        evalist_1 = [(dmatrix_train_1,'train'),(dmatrix_cold_1,'eval')]\n",
    "        xgb_model_1 = xgb.train(xgb_hyper,dmatrix_train_1,300,evalist_1,verbose_eval=True)\n",
    "        xgb_pred_cold_1 = xgb_model_1.predict(dmatrix_cold_1)\n",
    "        xgb_pred_train_1 = xgb_model_1.predict(dmatrix_train_1)\n",
    "        \n",
    "        #Second\n",
    "        dmatrix_train_2 = xgb.DMatrix(data = emb_train_2,label = Y_akt1)\n",
    "        dmatrix_cold_2  = xgb.DMatrix(data = emb_val_2,label = Y_cold_akt1)\n",
    "        evalist_2 = [(dmatrix_train_2,'train'),(dmatrix_cold_2,'eval')]\n",
    "        xgb_model_2 = xgb.train(xgb_hyper,dmatrix_train_2,300,evalist_2,verbose_eval=True)\n",
    "        xgb_pred_cold_2 = xgb_model_2.predict(dmatrix_cold_2)\n",
    "        xgb_pred_train_2 = xgb_model_2.predict(dmatrix_train_2)\n",
    "        \n",
    "        #Third\n",
    "        dmatrix_train_3 = xgb.DMatrix(data = emb_train_3,label = Y_pi3k)\n",
    "        dmatrix_cold_3  = xgb.DMatrix(data = emb_val_3,label = Y_cold_pi3k)\n",
    "        evalist_3 = [(dmatrix_train_3,'train'),(dmatrix_cold_3,'eval')]\n",
    "        xgb_model_3 = xgb.train(xgb_hyper,dmatrix_train_3,300,evalist_3,verbose_eval=True)\n",
    "        xgb_pred_cold_3 = xgb_model_3.predict(dmatrix_cold_3)\n",
    "        xgb_pred_train_3 = xgb_model_3.predict(dmatrix_train_3)\n",
    "        \n",
    "        if i < 6:\n",
    "            val_p1['Val_%s'%i] = calculate_metrics(np.array(Y_cold_p38),xgb_pred_cold_1)\n",
    "            val_p2['Val_%s'%i] = calculate_metrics(np.array(Y_cold_akt1),xgb_pred_cold_2)\n",
    "            val_p3['Val_%s'%i] = calculate_metrics(np.array(Y_cold_pi3k),xgb_pred_cold_3)\n",
    "            \n",
    "            train_p1['Fold_%s'%i] = calculate_metrics(np.array(Y_p38),xgb_pred_train_1)\n",
    "            train_p2['Fold_%s'%i] = calculate_metrics(np.array(Y_akt1),xgb_pred_train_2)\n",
    "            train_p3['Fold_%s'%i] = calculate_metrics(np.array(Y_pi3k),xgb_pred_train_3)\n",
    "        elif i == 6:\n",
    "            val_p1['Test'] = calculate_metrics(np.array(Y_cold_p38),xgb_pred_cold_1)\n",
    "            val_p2['Test'] = calculate_metrics(np.array(Y_cold_akt1),xgb_pred_cold_2)\n",
    "            val_p3['Test'] = calculate_metrics(np.array(Y_cold_pi3k),xgb_pred_cold_3)\n",
    "            \n",
    "            train_p1['Test'] = calculate_metrics(np.array(Y_p38),xgb_pred_train_1)\n",
    "            train_p2['Test'] = calculate_metrics(np.array(Y_akt1),xgb_pred_train_2)\n",
    "            train_p3['Test'] = calculate_metrics(np.array(Y_pi3k),xgb_pred_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>fn</th>\n",
       "      <th>fp</th>\n",
       "      <th>map</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>tn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Val_0</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>24.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.865027</td>\n",
       "      <td>0.747126</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.865751</td>\n",
       "      <td>108.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       accuracy    fn    fp       map  precision    recall   roc_auc     tn  \\\n",
       "Val_0  0.777778  24.0  44.0  0.865027   0.747126  0.844156  0.865751  108.0   \n",
       "\n",
       "          tp  \n",
       "Val_0  130.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(val_p2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>fn</th>\n",
       "      <th>fp</th>\n",
       "      <th>map</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>tn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fold_0</th>\n",
       "      <td>0.947644</td>\n",
       "      <td>37.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.980781</td>\n",
       "      <td>0.930421</td>\n",
       "      <td>0.939542</td>\n",
       "      <td>0.987977</td>\n",
       "      <td>873.0</td>\n",
       "      <td>575.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy    fn    fp       map  precision    recall   roc_auc     tn  \\\n",
       "Fold_0  0.947644  37.0  43.0  0.980781   0.930421  0.939542  0.987977  873.0   \n",
       "\n",
       "           tp  \n",
       "Fold_0  575.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_p2).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
