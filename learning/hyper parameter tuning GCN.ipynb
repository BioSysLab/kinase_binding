{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.callbacks import History, ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from data_analysis import calculate_metrics, load_weights_and_evaluate\n",
    "from model_builders import GCN_pretraining\n",
    "from hyperparameter_tuning_GCN import objective\n",
    "from functools import partial\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model callbacks on training\n",
    "es = EarlyStopping(monitor='loss',patience=8, min_delta=0)\n",
    "rlr = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=4, verbose=1, min_lr=0.0000001)\n",
    "\n",
    "model_params = {\n",
    "        \"num_layers\" : 3,\n",
    "        \"max_atoms\" : 70,\n",
    "        \"num_atom_features\" : 62,\n",
    "        \"num_atom_features_original\" : 62,\n",
    "        \"num_bond_features\" : 6,\n",
    "        \"max_degree\" : 5,\n",
    "        \"conv_width\" : [32,64,96],\n",
    "        \"fp_length\" : [96,96,96],\n",
    "        \"activ_enc\" : \"selu\",\n",
    "        \"activ_dec\" : \"selu\",\n",
    "        \"learning_rates\" : [0.001,0.001,0.001],\n",
    "        \"learning_rates_fp\": [0.005,0.005,0.005],\n",
    "        \"losses_conv\" : {\n",
    "                    \"neighbor_output\": \"mean_squared_error\",\n",
    "                    \"self_output\": \"mean_squared_error\",\n",
    "                    },\n",
    "        \"lossWeights\" : {\"neighbor_output\": 1.0, \"self_output\": 1.0},\n",
    "        \"metrics\" : \"mse\",\n",
    "        \"loss_fp\" : \"mean_squared_error\",\n",
    "        \"enc_layer_names\" : [\"enc_1\", \"enc_2\", \"enc_3\"],\n",
    "        'callbacks' : [es,rlr],\n",
    "        'adam_decay': 0.0005329142291371636,\n",
    "        'beta': 5,\n",
    "        'p': 0.004465204118126482,\n",
    "        'dense_size' : [96,96,48],\n",
    "        'dropout_rate' : [0.1,0.1],\n",
    "        'lr' : 0.001,\n",
    "        'batch_size' : int(64),\n",
    "        'n_epochs' : int(5)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fspace = {\n",
    "    'conv1' : hp.quniform('conv1', 32, 96, 8),\n",
    "    'conv2' : hp.quniform('conv2', 48, 128, 8),\n",
    "    'conv3' : hp.quniform('conv3', 64, 168, 8),\n",
    "    'fp' : hp.quniform('fp', 64, 196, 8),\n",
    "    'dense1' : hp.quniform('dense1',96,256,32),\n",
    "    'dense2' : hp.quniform('dense2',96,256,32),\n",
    "    'dense3' : hp.quniform('dense3',48,128,32),\n",
    "    'dropout_rate' : hp.uniform('dropout_rate',0.1,0.5),\n",
    "    'lr' : hp.uniform('lr',0.0005,0.01),\n",
    "    'n_epochs' : hp.quniform('n_epochs',15,40,5) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1 = 'pi3k'\n",
    "base_path_1 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_1 = base_path_1+f'/data/{target_1}/data.csv'\n",
    "df_p38=pd.read_csv(data_fpath_1).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_1+f'/data/{target_1}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_p38 = dill.load(in_f)\n",
    "\n",
    "with open(base_path_1+f'/data/{target_1}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_p38 = dill.load(in_f)\n",
    "\n",
    "training_p38 = [df_p38.loc[train_val_folds_p38[0][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[1][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[2][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[3][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[4][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[5][0]],\n",
    "                 df_p38.loc[train_test_folds_p38[0]]\n",
    "                 ]\n",
    "validation_p38 = [df_p38.loc[train_val_folds_p38[0][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[1][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[2][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[3][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[4][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[5][1]],\n",
    "                   df_p38.loc[train_test_folds_p38[1]]\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin_objective = partial(objective, train_sets = training_p38, val_sets = validation_p38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials():\n",
    "\n",
    "    trials_step = 0  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    max_trials = 0  # initial max_trials. put something small to not have to wait\n",
    "\n",
    "    \n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(\"gcn.hyperopt\", \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    best = fmin(fn = fmin_objective, space = fspace, algo=tpe.suggest, max_evals=max_trials, trials=trials)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    \n",
    "    # save the trials object\n",
    "    with open(\"gcn.hyperopt\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "    return(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved Trials! Loading...\n",
      "Rerunning from 555 trials to 555 (+0) trials\n",
      "100%|████████████████████████████████████████████████████████████████████████| 555/555 [00:00<?, ?trial/s, best loss=?]\n",
      "Best: {'conv1': 96.0, 'conv2': 104.0, 'conv3': 120.0, 'dense1': 256.0, 'dense2': 192.0, 'dense3': 96.0, 'dropout_rate': 0.3554537312557061, 'fp': 160.0, 'lr': 0.007037117031430456, 'n_epochs': 35.0}\n"
     ]
    }
   ],
   "source": [
    "trials = run_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = trials.trials[0]['result']['loss']\n",
    "for i in range(1,len(trials.trials)):\n",
    "    if (trials.trials[i]['result']['loss'] <=  best_loss):\n",
    "        best_loss = trials.trials[i]['result']['loss']\n",
    "        index = i\n",
    "best_params = trials.trials[index]['misc']['vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameter_tuning_GCN import GCN_hyper\n",
    "es = EarlyStopping(monitor='loss',patience=8, min_delta=0)\n",
    "rlr = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=4, verbose=1, min_lr=0.0000001)\n",
    "gcn_best = {\n",
    "        \"num_layers\" : 3,\n",
    "        \"max_atoms\" : 70,\n",
    "        \"num_atom_features\" : 62,\n",
    "        \"num_atom_features_original\" : 62,\n",
    "        \"num_bond_features\" : 6,\n",
    "        \"max_degree\" : 5,\n",
    "        \"conv_width\" : [int(best_params['conv1'][0]), int(best_params['conv2'][0]), int(best_params['conv3'][0])],\n",
    "        \"fp_length\" : [int(best_params['fp'][0]), int(best_params['fp'][0]), int(best_params['fp'][0])],\n",
    "        \"activ_enc\" : \"selu\",\n",
    "        \"activ_dec\" : \"selu\",\n",
    "        \"learning_rates\" : [0.001,0.001,0.001],\n",
    "        \"learning_rates_fp\": [0.005,0.005,0.005],\n",
    "        \"losses_conv\" : {\n",
    "                    \"neighbor_output\": \"mean_squared_error\",\n",
    "                    \"self_output\": \"mean_squared_error\",\n",
    "                    },\n",
    "        \"lossWeights\" : {\"neighbor_output\": 1.0, \"self_output\": 1.0},\n",
    "        \"metrics\" : \"mse\",\n",
    "        \"loss_fp\" : \"mean_squared_error\",\n",
    "        \"enc_layer_names\" : [\"enc_1\", \"enc_2\", \"enc_3\"],\n",
    "        'callbacks' : [es,rlr],\n",
    "        'adam_decay': 0.0005329142291371636,\n",
    "        'beta': 5,\n",
    "        'p': 0.004465204118126482,\n",
    "        'dense_size' : [int(best_params['dense1'][0]), int(best_params['dense2'][0]), int(best_params['dense3'][0])],\n",
    "        'dropout_rate' : [best_params['dropout_rate'][0], best_params['dropout_rate'][0]],\n",
    "        'lr' : best_params['lr'][0],\n",
    "        'batch_size' : int(64),\n",
    "        'n_epochs' : int(best_params['n_epochs'][0])\n",
    "        }\n",
    "gcn = GCN_hyper(gcn_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2680 samples, validate on 537 samples\n",
      "Epoch 1/35\n",
      "2680/2680 [==============================] - 6s 2ms/step - loss: 0.6679 - acc: 0.6243 - val_loss: 0.6557 - val_acc: 0.6127\n",
      "Epoch 2/35\n",
      "2680/2680 [==============================] - 2s 602us/step - loss: 0.6044 - acc: 0.6563 - val_loss: 0.6266 - val_acc: 0.6704\n",
      "Epoch 3/35\n",
      "2680/2680 [==============================] - 2s 657us/step - loss: 0.5432 - acc: 0.7366 - val_loss: 0.6573 - val_acc: 0.6480\n",
      "Epoch 4/35\n",
      "2680/2680 [==============================] - 1s 554us/step - loss: 0.5417 - acc: 0.7272 - val_loss: 0.7599 - val_acc: 0.6238\n",
      "Epoch 5/35\n",
      "2680/2680 [==============================] - 2s 561us/step - loss: 0.5004 - acc: 0.7563 - val_loss: 0.6342 - val_acc: 0.6909\n",
      "Epoch 6/35\n",
      "2680/2680 [==============================] - 2s 575us/step - loss: 0.5039 - acc: 0.7612 - val_loss: 0.5663 - val_acc: 0.7244\n",
      "Epoch 7/35\n",
      "2680/2680 [==============================] - 2s 570us/step - loss: 0.4831 - acc: 0.7799 - val_loss: 0.6176 - val_acc: 0.6890\n",
      "Epoch 8/35\n",
      "2680/2680 [==============================] - 1s 535us/step - loss: 0.4629 - acc: 0.7858 - val_loss: 0.6582 - val_acc: 0.6946\n",
      "Epoch 9/35\n",
      "2680/2680 [==============================] - 2s 579us/step - loss: 0.4891 - acc: 0.7623 - val_loss: 0.5576 - val_acc: 0.7356\n",
      "Epoch 10/35\n",
      "2680/2680 [==============================] - 2s 577us/step - loss: 0.4759 - acc: 0.7739 - val_loss: 0.5974 - val_acc: 0.7393\n",
      "Epoch 11/35\n",
      "2680/2680 [==============================] - 1s 514us/step - loss: 0.4693 - acc: 0.7795 - val_loss: 0.6003 - val_acc: 0.7002\n",
      "Epoch 12/35\n",
      "2680/2680 [==============================] - 1s 538us/step - loss: 0.4531 - acc: 0.7951 - val_loss: 0.6403 - val_acc: 0.6723\n",
      "Epoch 13/35\n",
      "2680/2680 [==============================] - 1s 549us/step - loss: 0.4485 - acc: 0.7940 - val_loss: 0.5713 - val_acc: 0.7337\n",
      "Epoch 14/35\n",
      "2680/2680 [==============================] - 1s 553us/step - loss: 0.4433 - acc: 0.7940 - val_loss: 0.5521 - val_acc: 0.7337\n",
      "Epoch 15/35\n",
      "2680/2680 [==============================] - 1s 550us/step - loss: 0.4275 - acc: 0.8123 - val_loss: 0.5769 - val_acc: 0.7430\n",
      "Epoch 16/35\n",
      "2680/2680 [==============================] - 1s 552us/step - loss: 0.4225 - acc: 0.8123 - val_loss: 0.6155 - val_acc: 0.7169\n",
      "Epoch 17/35\n",
      "2680/2680 [==============================] - 1s 551us/step - loss: 0.4268 - acc: 0.8101 - val_loss: 0.5764 - val_acc: 0.7318\n",
      "Epoch 18/35\n",
      "2680/2680 [==============================] - 1s 550us/step - loss: 0.4505 - acc: 0.7940 - val_loss: 0.6484 - val_acc: 0.7412\n",
      "Epoch 19/35\n",
      "2680/2680 [==============================] - 1s 535us/step - loss: 0.4215 - acc: 0.8104 - val_loss: 0.5802 - val_acc: 0.7412\n",
      "Epoch 20/35\n",
      "2680/2680 [==============================] - 1s 535us/step - loss: 0.4369 - acc: 0.8045 - val_loss: 0.5448 - val_acc: 0.7430\n",
      "Epoch 21/35\n",
      "2680/2680 [==============================] - 2s 566us/step - loss: 0.4098 - acc: 0.8235 - val_loss: 0.5434 - val_acc: 0.7412\n",
      "Epoch 22/35\n",
      "2680/2680 [==============================] - 2s 568us/step - loss: 0.4085 - acc: 0.8209 - val_loss: 0.6104 - val_acc: 0.7318\n",
      "Epoch 23/35\n",
      "2680/2680 [==============================] - 1s 538us/step - loss: 0.3968 - acc: 0.8246 - val_loss: 0.5614 - val_acc: 0.7318\n",
      "Epoch 24/35\n",
      "2680/2680 [==============================] - 1s 559us/step - loss: 0.3843 - acc: 0.8336 - val_loss: 0.5633 - val_acc: 0.7169\n",
      "Epoch 25/35\n",
      "2680/2680 [==============================] - 1s 545us/step - loss: 0.3842 - acc: 0.8373 - val_loss: 0.5158 - val_acc: 0.7393\n",
      "Epoch 26/35\n",
      "2680/2680 [==============================] - 1s 541us/step - loss: 0.3853 - acc: 0.8388 - val_loss: 0.6072 - val_acc: 0.7449\n",
      "Epoch 27/35\n",
      "2680/2680 [==============================] - 1s 556us/step - loss: 0.4077 - acc: 0.8343 - val_loss: 0.5342 - val_acc: 0.7393\n",
      "Epoch 28/35\n",
      "2680/2680 [==============================] - 1s 548us/step - loss: 0.3947 - acc: 0.8328 - val_loss: 0.5890 - val_acc: 0.7467\n",
      "Epoch 29/35\n",
      "2680/2680 [==============================] - 2s 670us/step - loss: 0.3919 - acc: 0.8239 - val_loss: 0.5393 - val_acc: 0.7467\n",
      "Epoch 30/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.3862 - acc: 0.8336 - val_loss: 0.5448 - val_acc: 0.7449\n",
      "Epoch 31/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.3865 - acc: 0.8351 - val_loss: 0.5433 - val_acc: 0.7412\n",
      "Epoch 32/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.3805 - acc: 0.8451 - val_loss: 0.5449 - val_acc: 0.7281\n",
      "Epoch 33/35\n",
      "2680/2680 [==============================] - 3s 981us/step - loss: 0.3741 - acc: 0.8325 - val_loss: 0.5625 - val_acc: 0.7486\n",
      "Epoch 34/35\n",
      "2680/2680 [==============================] - 2s 567us/step - loss: 0.3668 - acc: 0.8463 - val_loss: 0.5296 - val_acc: 0.7467\n",
      "Epoch 35/35\n",
      "2680/2680 [==============================] - 1s 559us/step - loss: 0.3586 - acc: 0.8459 - val_loss: 0.5182 - val_acc: 0.7467\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2680 samples, validate on 537 samples\n",
      "Epoch 1/35\n",
      "2680/2680 [==============================] - 6s 2ms/step - loss: 0.6699 - acc: 0.6030 - val_loss: 0.6538 - val_acc: 0.6834\n",
      "Epoch 2/35\n",
      "2680/2680 [==============================] - 1s 528us/step - loss: 0.6057 - acc: 0.6377 - val_loss: 0.5249 - val_acc: 0.7523\n",
      "Epoch 3/35\n",
      "2680/2680 [==============================] - 1s 531us/step - loss: 0.5481 - acc: 0.7116 - val_loss: 0.5130 - val_acc: 0.7728\n",
      "Epoch 4/35\n",
      "2680/2680 [==============================] - 1s 533us/step - loss: 0.4941 - acc: 0.7582 - val_loss: 0.5013 - val_acc: 0.7747\n",
      "Epoch 5/35\n",
      "2680/2680 [==============================] - 1s 546us/step - loss: 0.4807 - acc: 0.7657 - val_loss: 0.5677 - val_acc: 0.7486\n",
      "Epoch 6/35\n",
      "2680/2680 [==============================] - 1s 538us/step - loss: 0.4870 - acc: 0.7560 - val_loss: 0.4769 - val_acc: 0.7821\n",
      "Epoch 7/35\n",
      "2680/2680 [==============================] - 1s 538us/step - loss: 0.4702 - acc: 0.7728 - val_loss: 0.5107 - val_acc: 0.7784\n",
      "Epoch 8/35\n",
      "2680/2680 [==============================] - 1s 557us/step - loss: 0.4589 - acc: 0.7821 - val_loss: 0.4846 - val_acc: 0.7877\n",
      "Epoch 9/35\n",
      "2680/2680 [==============================] - 1s 534us/step - loss: 0.4575 - acc: 0.7873 - val_loss: 0.4794 - val_acc: 0.7952\n",
      "Epoch 10/35\n",
      "2680/2680 [==============================] - 1s 541us/step - loss: 0.4571 - acc: 0.7743 - val_loss: 0.5204 - val_acc: 0.7561\n",
      "Epoch 11/35\n",
      "2680/2680 [==============================] - 2s 570us/step - loss: 0.4656 - acc: 0.7672 - val_loss: 0.5043 - val_acc: 0.7952\n",
      "Epoch 12/35\n",
      "2680/2680 [==============================] - 1s 560us/step - loss: 0.4595 - acc: 0.7896 - val_loss: 0.4966 - val_acc: 0.7728\n",
      "Epoch 13/35\n",
      "2680/2680 [==============================] - 1s 536us/step - loss: 0.4372 - acc: 0.7966 - val_loss: 0.4754 - val_acc: 0.7933\n",
      "Epoch 14/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4410 - acc: 0.7903 - val_loss: 0.4898 - val_acc: 0.8082\n",
      "Epoch 15/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4254 - acc: 0.8004 - val_loss: 0.5255 - val_acc: 0.7821\n",
      "Epoch 16/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4327 - acc: 0.7896 - val_loss: 0.5114 - val_acc: 0.7635\n",
      "Epoch 17/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4091 - acc: 0.7940 - val_loss: 0.4920 - val_acc: 0.7952\n",
      "Epoch 18/35\n",
      "2680/2680 [==============================] - 2s 605us/step - loss: 0.4131 - acc: 0.8082 - val_loss: 0.4549 - val_acc: 0.8026\n",
      "Epoch 19/35\n",
      "2680/2680 [==============================] - 1s 547us/step - loss: 0.3996 - acc: 0.8116 - val_loss: 0.4685 - val_acc: 0.8026\n",
      "Epoch 20/35\n",
      "2680/2680 [==============================] - 1s 550us/step - loss: 0.3997 - acc: 0.8213 - val_loss: 0.5149 - val_acc: 0.7486\n",
      "Epoch 21/35\n",
      "2680/2680 [==============================] - 1s 545us/step - loss: 0.3820 - acc: 0.8287 - val_loss: 0.5853 - val_acc: 0.7877\n",
      "Epoch 22/35\n",
      "2680/2680 [==============================] - 1s 533us/step - loss: 0.4271 - acc: 0.8056 - val_loss: 0.4750 - val_acc: 0.7840\n",
      "Epoch 23/35\n",
      "2680/2680 [==============================] - 1s 550us/step - loss: 0.4104 - acc: 0.8146 - val_loss: 0.4747 - val_acc: 0.7896\n",
      "Epoch 24/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4147 - acc: 0.8127 - val_loss: 0.4925 - val_acc: 0.7933\n",
      "Epoch 25/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.3905 - acc: 0.8209 - val_loss: 0.4887 - val_acc: 0.8063\n",
      "Epoch 26/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.3841 - acc: 0.8313 - val_loss: 0.4603 - val_acc: 0.8063\n",
      "Epoch 27/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.3890 - acc: 0.8220 - val_loss: 0.4729 - val_acc: 0.8119\n",
      "Epoch 28/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.3657 - acc: 0.8328 - val_loss: 0.4415 - val_acc: 0.8156\n",
      "Epoch 29/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.3892 - acc: 0.8295 - val_loss: 0.5422 - val_acc: 0.7989\n",
      "Epoch 30/35\n",
      "2680/2680 [==============================] - 1s 553us/step - loss: 0.3751 - acc: 0.8313 - val_loss: 0.4302 - val_acc: 0.8026\n",
      "Epoch 31/35\n",
      "2680/2680 [==============================] - 1s 556us/step - loss: 0.3871 - acc: 0.8354 - val_loss: 0.4571 - val_acc: 0.7933\n",
      "Epoch 32/35\n",
      "2680/2680 [==============================] - 1s 550us/step - loss: 0.3830 - acc: 0.8246 - val_loss: 0.6089 - val_acc: 0.7654\n",
      "Epoch 33/35\n",
      "2680/2680 [==============================] - 1s 556us/step - loss: 0.3934 - acc: 0.8261 - val_loss: 0.4796 - val_acc: 0.7989\n",
      "Epoch 34/35\n",
      "2680/2680 [==============================] - 1s 560us/step - loss: 0.3534 - acc: 0.8493 - val_loss: 0.4774 - val_acc: 0.7933\n",
      "Epoch 35/35\n",
      "2680/2680 [==============================] - 1s 553us/step - loss: 0.3475 - acc: 0.8451 - val_loss: 0.4734 - val_acc: 0.7989\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2680 samples, validate on 537 samples\n",
      "Epoch 1/35\n",
      "2680/2680 [==============================] - 7s 3ms/step - loss: 0.6714 - acc: 0.6153 - val_loss: 0.6540 - val_acc: 0.6313\n",
      "Epoch 2/35\n",
      "2680/2680 [==============================] - 1s 533us/step - loss: 0.6154 - acc: 0.6377 - val_loss: 0.6469 - val_acc: 0.5959\n",
      "Epoch 3/35\n",
      "2680/2680 [==============================] - 1s 543us/step - loss: 0.5469 - acc: 0.7116 - val_loss: 0.5941 - val_acc: 0.7281\n",
      "Epoch 4/35\n",
      "2680/2680 [==============================] - 1s 527us/step - loss: 0.5028 - acc: 0.7601 - val_loss: 0.5684 - val_acc: 0.7263\n",
      "Epoch 5/35\n",
      "2680/2680 [==============================] - 2s 571us/step - loss: 0.5102 - acc: 0.7608 - val_loss: 0.6451 - val_acc: 0.6667\n",
      "Epoch 6/35\n",
      "2680/2680 [==============================] - 1s 531us/step - loss: 0.4907 - acc: 0.7653 - val_loss: 0.5530 - val_acc: 0.7114\n",
      "Epoch 7/35\n",
      "2680/2680 [==============================] - 2s 564us/step - loss: 0.4959 - acc: 0.7724 - val_loss: 0.5502 - val_acc: 0.7449\n",
      "Epoch 8/35\n",
      "2680/2680 [==============================] - 2s 575us/step - loss: 0.4667 - acc: 0.7821 - val_loss: 0.5372 - val_acc: 0.7486\n",
      "Epoch 9/35\n",
      "2680/2680 [==============================] - 2s 587us/step - loss: 0.4517 - acc: 0.7907 - val_loss: 0.5225 - val_acc: 0.7579\n",
      "Epoch 10/35\n",
      "2680/2680 [==============================] - 1s 553us/step - loss: 0.4604 - acc: 0.7899 - val_loss: 0.5159 - val_acc: 0.7393\n",
      "Epoch 11/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4467 - acc: 0.7929 - val_loss: 0.5144 - val_acc: 0.7579\n",
      "Epoch 12/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4409 - acc: 0.8030 - val_loss: 0.5079 - val_acc: 0.7654\n",
      "Epoch 13/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4547 - acc: 0.7877 - val_loss: 0.5193 - val_acc: 0.7635\n",
      "Epoch 14/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4506 - acc: 0.7899 - val_loss: 0.4864 - val_acc: 0.7709\n",
      "Epoch 15/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4239 - acc: 0.8075 - val_loss: 0.4990 - val_acc: 0.7654\n",
      "Epoch 16/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4297 - acc: 0.8127 - val_loss: 0.4965 - val_acc: 0.7784\n",
      "Epoch 17/35\n",
      "2680/2680 [==============================] - 2s 627us/step - loss: 0.4259 - acc: 0.8034 - val_loss: 0.4870 - val_acc: 0.7598\n",
      "Epoch 18/35\n",
      "2680/2680 [==============================] - 2s 578us/step - loss: 0.4251 - acc: 0.8138 - val_loss: 0.5023 - val_acc: 0.7765\n",
      "Epoch 19/35\n",
      "2680/2680 [==============================] - 1s 551us/step - loss: 0.4343 - acc: 0.8168 - val_loss: 0.4812 - val_acc: 0.7858\n",
      "Epoch 20/35\n",
      "2680/2680 [==============================] - 2s 577us/step - loss: 0.4134 - acc: 0.8250 - val_loss: 0.4952 - val_acc: 0.7672\n",
      "Epoch 21/35\n",
      "2680/2680 [==============================] - 1s 545us/step - loss: 0.4237 - acc: 0.8187 - val_loss: 0.4966 - val_acc: 0.7784\n",
      "Epoch 22/35\n",
      "2680/2680 [==============================] - 2s 570us/step - loss: 0.4016 - acc: 0.8246 - val_loss: 0.4901 - val_acc: 0.7579\n",
      "Epoch 23/35\n",
      "2680/2680 [==============================] - 2s 580us/step - loss: 0.4117 - acc: 0.8228 - val_loss: 0.4816 - val_acc: 0.7821\n",
      "Epoch 24/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4086 - acc: 0.8194 - val_loss: 0.4734 - val_acc: 0.7672\n",
      "Epoch 25/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4077 - acc: 0.8231 - val_loss: 0.4877 - val_acc: 0.7654\n",
      "Epoch 26/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4040 - acc: 0.8276 - val_loss: 0.4544 - val_acc: 0.7933\n",
      "Epoch 27/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4022 - acc: 0.8369 - val_loss: 0.5054 - val_acc: 0.7765\n",
      "Epoch 28/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4010 - acc: 0.8287 - val_loss: 0.4973 - val_acc: 0.7542\n",
      "Epoch 29/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.3987 - acc: 0.8265 - val_loss: 0.4718 - val_acc: 0.7858\n",
      "Epoch 30/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4070 - acc: 0.8265 - val_loss: 0.4779 - val_acc: 0.7821\n",
      "Epoch 31/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.3973 - acc: 0.8276 - val_loss: 0.4860 - val_acc: 0.7598\n",
      "Epoch 32/35\n",
      "2680/2680 [==============================] - 2s 570us/step - loss: 0.3895 - acc: 0.8343 - val_loss: 0.4810 - val_acc: 0.7821\n",
      "Epoch 33/35\n",
      "2680/2680 [==============================] - 2s 620us/step - loss: 0.3868 - acc: 0.8336 - val_loss: 0.4830 - val_acc: 0.7858\n",
      "Epoch 34/35\n",
      "2680/2680 [==============================] - 2s 587us/step - loss: 0.3976 - acc: 0.8325 - val_loss: 0.4880 - val_acc: 0.7803\n",
      "Epoch 35/35\n",
      "2680/2680 [==============================] - 2s 629us/step - loss: 0.3946 - acc: 0.8272 - val_loss: 0.4747 - val_acc: 0.7579\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2680 samples, validate on 537 samples\n",
      "Epoch 1/35\n",
      "2680/2680 [==============================] - 8s 3ms/step - loss: 0.6721 - acc: 0.6149 - val_loss: 0.6353 - val_acc: 0.6965\n",
      "Epoch 2/35\n",
      "2680/2680 [==============================] - 1s 536us/step - loss: 0.6446 - acc: 0.6172 - val_loss: 0.5783 - val_acc: 0.7058\n",
      "Epoch 3/35\n",
      "2680/2680 [==============================] - 2s 573us/step - loss: 0.5840 - acc: 0.6884 - val_loss: 0.8054 - val_acc: 0.5047\n",
      "Epoch 4/35\n",
      "2680/2680 [==============================] - 1s 552us/step - loss: 0.5714 - acc: 0.7201 - val_loss: 0.5493 - val_acc: 0.6983\n",
      "Epoch 5/35\n",
      "2680/2680 [==============================] - 1s 551us/step - loss: 0.5399 - acc: 0.7299 - val_loss: 0.6385 - val_acc: 0.5810\n",
      "Epoch 6/35\n",
      "2680/2680 [==============================] - 2s 570us/step - loss: 0.5169 - acc: 0.7448 - val_loss: 0.5653 - val_acc: 0.7281\n",
      "Epoch 7/35\n",
      "2680/2680 [==============================] - 1s 544us/step - loss: 0.4856 - acc: 0.7731 - val_loss: 0.5508 - val_acc: 0.7244\n",
      "Epoch 8/35\n",
      "2680/2680 [==============================] - 1s 547us/step - loss: 0.4992 - acc: 0.7556 - val_loss: 0.5344 - val_acc: 0.7039\n",
      "Epoch 9/35\n",
      "2680/2680 [==============================] - 3s 946us/step - loss: 0.4799 - acc: 0.7709 - val_loss: 0.4451 - val_acc: 0.7896\n",
      "Epoch 10/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4542 - acc: 0.7869 - val_loss: 0.4974 - val_acc: 0.7412\n",
      "Epoch 11/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4661 - acc: 0.7847 - val_loss: 0.4401 - val_acc: 0.7896\n",
      "Epoch 12/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4721 - acc: 0.7791 - val_loss: 0.4390 - val_acc: 0.7952\n",
      "Epoch 13/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4568 - acc: 0.7907 - val_loss: 0.4879 - val_acc: 0.7393\n",
      "Epoch 14/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4450 - acc: 0.8007 - val_loss: 0.7734 - val_acc: 0.5512\n",
      "Epoch 15/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4679 - acc: 0.7772 - val_loss: 0.4558 - val_acc: 0.7914\n",
      "Epoch 16/35\n",
      "2680/2680 [==============================] - 2s 778us/step - loss: 0.4440 - acc: 0.7903 - val_loss: 0.4249 - val_acc: 0.7840\n",
      "Epoch 17/35\n",
      "2680/2680 [==============================] - 1s 543us/step - loss: 0.4250 - acc: 0.8082 - val_loss: 0.4254 - val_acc: 0.7914\n",
      "Epoch 18/35\n",
      "2680/2680 [==============================] - 2s 591us/step - loss: 0.4148 - acc: 0.8127 - val_loss: 0.4742 - val_acc: 0.7877\n",
      "Epoch 19/35\n",
      "2680/2680 [==============================] - 2s 564us/step - loss: 0.4167 - acc: 0.8097 - val_loss: 0.4532 - val_acc: 0.7821\n",
      "Epoch 20/35\n",
      "2680/2680 [==============================] - 2s 585us/step - loss: 0.4184 - acc: 0.8142 - val_loss: 0.4544 - val_acc: 0.7821\n",
      "Epoch 21/35\n",
      "2680/2680 [==============================] - 2s 590us/step - loss: 0.4150 - acc: 0.8037 - val_loss: 0.4709 - val_acc: 0.7784\n",
      "Epoch 22/35\n",
      "2680/2680 [==============================] - 2s 911us/step - loss: 0.4198 - acc: 0.8041 - val_loss: 0.4242 - val_acc: 0.7896\n",
      "Epoch 23/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4099 - acc: 0.8194 - val_loss: 0.4545 - val_acc: 0.7765\n",
      "Epoch 24/35\n",
      "2680/2680 [==============================] - 4s 1ms/step - loss: 0.4257 - acc: 0.8086 - val_loss: 0.4165 - val_acc: 0.8082\n",
      "Epoch 25/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4105 - acc: 0.8153 - val_loss: 0.4190 - val_acc: 0.7933\n",
      "Epoch 26/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4067 - acc: 0.8220 - val_loss: 0.4370 - val_acc: 0.7933\n",
      "Epoch 27/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.3931 - acc: 0.8351 - val_loss: 0.4276 - val_acc: 0.8007\n",
      "Epoch 28/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.3984 - acc: 0.8213 - val_loss: 0.4028 - val_acc: 0.8026\n",
      "Epoch 29/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.3855 - acc: 0.8284 - val_loss: 0.4256 - val_acc: 0.7933\n",
      "Epoch 30/35\n",
      "2680/2680 [==============================] - 2s 706us/step - loss: 0.3938 - acc: 0.8287 - val_loss: 0.4073 - val_acc: 0.8101\n",
      "Epoch 31/35\n",
      "2680/2680 [==============================] - 1s 557us/step - loss: 0.3829 - acc: 0.8332 - val_loss: 0.4316 - val_acc: 0.8082\n",
      "Epoch 32/35\n",
      "2680/2680 [==============================] - 1s 546us/step - loss: 0.4065 - acc: 0.8168 - val_loss: 0.4250 - val_acc: 0.7858\n",
      "Epoch 33/35\n",
      "2680/2680 [==============================] - 2s 562us/step - loss: 0.3902 - acc: 0.8280 - val_loss: 0.4290 - val_acc: 0.7970\n",
      "Epoch 34/35\n",
      "2680/2680 [==============================] - 2s 561us/step - loss: 0.3869 - acc: 0.8295 - val_loss: 0.4262 - val_acc: 0.7989\n",
      "Epoch 35/35\n",
      "2680/2680 [==============================] - 3s 976us/step - loss: 0.3788 - acc: 0.8388 - val_loss: 0.3980 - val_acc: 0.8101\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2680 samples, validate on 537 samples\n",
      "Epoch 1/35\n",
      "2680/2680 [==============================] - 9s 3ms/step - loss: 0.6767 - acc: 0.6201 - val_loss: 0.6534 - val_acc: 0.6369\n",
      "Epoch 2/35\n",
      "2680/2680 [==============================] - 2s 669us/step - loss: 0.6620 - acc: 0.6280 - val_loss: 0.6151 - val_acc: 0.6369\n",
      "Epoch 3/35\n",
      "2680/2680 [==============================] - 2s 649us/step - loss: 0.6067 - acc: 0.6556 - val_loss: 0.5530 - val_acc: 0.6443\n",
      "Epoch 4/35\n",
      "2680/2680 [==============================] - 2s 640us/step - loss: 0.5630 - acc: 0.6963 - val_loss: 0.5425 - val_acc: 0.7020\n",
      "Epoch 5/35\n",
      "2680/2680 [==============================] - 2s 637us/step - loss: 0.5494 - acc: 0.7194 - val_loss: 0.4785 - val_acc: 0.7709\n",
      "Epoch 6/35\n",
      "2680/2680 [==============================] - 2s 615us/step - loss: 0.5130 - acc: 0.7470 - val_loss: 0.5036 - val_acc: 0.7169\n",
      "Epoch 7/35\n",
      "2680/2680 [==============================] - 2s 608us/step - loss: 0.4935 - acc: 0.7653 - val_loss: 0.4515 - val_acc: 0.7672\n",
      "Epoch 8/35\n",
      "2680/2680 [==============================] - 2s 638us/step - loss: 0.5018 - acc: 0.7701 - val_loss: 0.4524 - val_acc: 0.7616\n",
      "Epoch 9/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4819 - acc: 0.7690 - val_loss: 0.4293 - val_acc: 0.7784\n",
      "Epoch 10/35\n",
      "2680/2680 [==============================] - 3s 1ms/step - loss: 0.4647 - acc: 0.7724 - val_loss: 0.4462 - val_acc: 0.7654\n",
      "Epoch 11/35\n",
      "2680/2680 [==============================] - 2s 560us/step - loss: 0.4851 - acc: 0.7776 - val_loss: 0.4184 - val_acc: 0.7654\n",
      "Epoch 12/35\n",
      "2680/2680 [==============================] - 2s 565us/step - loss: 0.4625 - acc: 0.7840 - val_loss: 0.5281 - val_acc: 0.7188\n",
      "Epoch 13/35\n",
      "2680/2680 [==============================] - 2s 590us/step - loss: 0.4416 - acc: 0.7937 - val_loss: 0.4420 - val_acc: 0.7691\n",
      "Epoch 14/35\n",
      "2680/2680 [==============================] - 1s 553us/step - loss: 0.4589 - acc: 0.7899 - val_loss: 0.4281 - val_acc: 0.7821\n",
      "Epoch 15/35\n",
      "2680/2680 [==============================] - 1s 542us/step - loss: 0.4490 - acc: 0.7966 - val_loss: 0.4540 - val_acc: 0.7412\n",
      "Epoch 16/35\n",
      "2680/2680 [==============================] - 1s 552us/step - loss: 0.4388 - acc: 0.8011 - val_loss: 0.4452 - val_acc: 0.7616\n",
      "Epoch 17/35\n",
      "2680/2680 [==============================] - 1s 556us/step - loss: 0.4458 - acc: 0.7877 - val_loss: 0.4246 - val_acc: 0.7821\n",
      "Epoch 18/35\n",
      "2680/2680 [==============================] - 2s 562us/step - loss: 0.4210 - acc: 0.8082 - val_loss: 0.4158 - val_acc: 0.7914\n",
      "Epoch 19/35\n",
      "2680/2680 [==============================] - 2s 560us/step - loss: 0.4106 - acc: 0.8164 - val_loss: 0.5523 - val_acc: 0.7467\n",
      "Epoch 20/35\n",
      "2680/2680 [==============================] - 2s 567us/step - loss: 0.4170 - acc: 0.8142 - val_loss: 0.4211 - val_acc: 0.7635\n",
      "Epoch 21/35\n",
      "2680/2680 [==============================] - 1s 550us/step - loss: 0.3999 - acc: 0.8220 - val_loss: 0.4158 - val_acc: 0.8138\n",
      "Epoch 22/35\n",
      "2680/2680 [==============================] - 2s 572us/step - loss: 0.4114 - acc: 0.8190 - val_loss: 0.4339 - val_acc: 0.8101\n",
      "Epoch 23/35\n",
      "2680/2680 [==============================] - 1s 531us/step - loss: 0.4063 - acc: 0.8160 - val_loss: 0.4600 - val_acc: 0.7412\n",
      "Epoch 24/35\n",
      "2680/2680 [==============================] - 1s 558us/step - loss: 0.4005 - acc: 0.8194 - val_loss: 0.4179 - val_acc: 0.7933\n",
      "Epoch 25/35\n",
      "2680/2680 [==============================] - 1s 541us/step - loss: 0.4188 - acc: 0.8104 - val_loss: 0.4006 - val_acc: 0.8045\n",
      "Epoch 26/35\n",
      "2680/2680 [==============================] - 2s 572us/step - loss: 0.3860 - acc: 0.8310 - val_loss: 0.4345 - val_acc: 0.7747\n",
      "Epoch 27/35\n",
      "2680/2680 [==============================] - 2s 573us/step - loss: 0.3951 - acc: 0.8205 - val_loss: 0.4035 - val_acc: 0.8138\n",
      "Epoch 28/35\n",
      "2680/2680 [==============================] - 2s 570us/step - loss: 0.4022 - acc: 0.8179 - val_loss: 0.4224 - val_acc: 0.8082\n",
      "Epoch 29/35\n",
      "2680/2680 [==============================] - 1s 541us/step - loss: 0.3803 - acc: 0.8295 - val_loss: 0.4183 - val_acc: 0.7765\n",
      "Epoch 30/35\n",
      "2680/2680 [==============================] - 2s 592us/step - loss: 0.3755 - acc: 0.8396 - val_loss: 0.4460 - val_acc: 0.8063\n",
      "Epoch 31/35\n",
      "2680/2680 [==============================] - 1s 546us/step - loss: 0.4014 - acc: 0.8187 - val_loss: 0.4432 - val_acc: 0.7914\n",
      "Epoch 32/35\n",
      "2680/2680 [==============================] - 1s 549us/step - loss: 0.3806 - acc: 0.8291 - val_loss: 0.4011 - val_acc: 0.8063\n",
      "Epoch 33/35\n",
      "2680/2680 [==============================] - 2s 570us/step - loss: 0.3902 - acc: 0.8284 - val_loss: 0.4316 - val_acc: 0.7989\n",
      "Epoch 34/35\n",
      "2680/2680 [==============================] - 2s 565us/step - loss: 0.3726 - acc: 0.8440 - val_loss: 0.4052 - val_acc: 0.8138\n",
      "Epoch 35/35\n",
      "2680/2680 [==============================] - 2s 562us/step - loss: 0.3870 - acc: 0.8340 - val_loss: 0.4303 - val_acc: 0.7877\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2685 samples, validate on 532 samples\n",
      "Epoch 1/35\n",
      "2685/2685 [==============================] - 9s 3ms/step - loss: 0.6620 - acc: 0.6439 - val_loss: 0.7185 - val_acc: 0.5188\n",
      "Epoch 2/35\n",
      "2685/2685 [==============================] - 2s 565us/step - loss: 0.6218 - acc: 0.6536 - val_loss: 0.6610 - val_acc: 0.5188\n",
      "Epoch 3/35\n",
      "2685/2685 [==============================] - 2s 566us/step - loss: 0.5624 - acc: 0.6890 - val_loss: 0.5954 - val_acc: 0.6447\n",
      "Epoch 4/35\n",
      "2685/2685 [==============================] - 2s 572us/step - loss: 0.5222 - acc: 0.7385 - val_loss: 0.5842 - val_acc: 0.6391\n",
      "Epoch 5/35\n",
      "2685/2685 [==============================] - 2s 563us/step - loss: 0.5017 - acc: 0.7479 - val_loss: 0.5372 - val_acc: 0.7162\n",
      "Epoch 6/35\n",
      "2685/2685 [==============================] - 2s 575us/step - loss: 0.4904 - acc: 0.7661 - val_loss: 0.5335 - val_acc: 0.7368\n",
      "Epoch 7/35\n",
      "2685/2685 [==============================] - 1s 552us/step - loss: 0.4724 - acc: 0.7717 - val_loss: 0.5080 - val_acc: 0.7331\n",
      "Epoch 8/35\n",
      "2685/2685 [==============================] - 1s 545us/step - loss: 0.4915 - acc: 0.7795 - val_loss: 0.5153 - val_acc: 0.7425\n",
      "Epoch 9/35\n",
      "2685/2685 [==============================] - 2s 559us/step - loss: 0.4548 - acc: 0.7929 - val_loss: 0.4995 - val_acc: 0.7218\n",
      "Epoch 10/35\n",
      "2685/2685 [==============================] - 1s 549us/step - loss: 0.4584 - acc: 0.7978 - val_loss: 0.5399 - val_acc: 0.7199\n",
      "Epoch 11/35\n",
      "2685/2685 [==============================] - 1s 551us/step - loss: 0.4305 - acc: 0.8026 - val_loss: 0.6522 - val_acc: 0.6842\n",
      "Epoch 12/35\n",
      "2685/2685 [==============================] - 1s 558us/step - loss: 0.4679 - acc: 0.7777 - val_loss: 0.5281 - val_acc: 0.7086\n",
      "Epoch 13/35\n",
      "2685/2685 [==============================] - 2s 561us/step - loss: 0.4455 - acc: 0.7896 - val_loss: 0.4818 - val_acc: 0.7632\n",
      "Epoch 14/35\n",
      "2685/2685 [==============================] - 1s 555us/step - loss: 0.4237 - acc: 0.8074 - val_loss: 0.4856 - val_acc: 0.7632\n",
      "Epoch 15/35\n",
      "2685/2685 [==============================] - 2s 571us/step - loss: 0.4209 - acc: 0.8067 - val_loss: 0.5121 - val_acc: 0.7519\n",
      "Epoch 16/35\n",
      "2685/2685 [==============================] - 1s 555us/step - loss: 0.4290 - acc: 0.8123 - val_loss: 0.5267 - val_acc: 0.7500\n",
      "Epoch 17/35\n",
      "2685/2685 [==============================] - 2s 569us/step - loss: 0.4123 - acc: 0.8138 - val_loss: 0.4750 - val_acc: 0.7575\n",
      "Epoch 18/35\n",
      "2685/2685 [==============================] - 2s 593us/step - loss: 0.3964 - acc: 0.8250 - val_loss: 0.4980 - val_acc: 0.7331\n",
      "Epoch 19/35\n",
      "2685/2685 [==============================] - 1s 555us/step - loss: 0.4059 - acc: 0.8209 - val_loss: 0.4669 - val_acc: 0.7632\n",
      "Epoch 20/35\n",
      "2685/2685 [==============================] - 1s 546us/step - loss: 0.4253 - acc: 0.8086 - val_loss: 0.4519 - val_acc: 0.7726\n",
      "Epoch 21/35\n",
      "2685/2685 [==============================] - 2s 583us/step - loss: 0.3908 - acc: 0.8283 - val_loss: 0.5365 - val_acc: 0.7387\n",
      "Epoch 22/35\n",
      "2685/2685 [==============================] - 1s 559us/step - loss: 0.4071 - acc: 0.8264 - val_loss: 0.5248 - val_acc: 0.7594\n",
      "Epoch 23/35\n",
      "2685/2685 [==============================] - 1s 553us/step - loss: 0.4149 - acc: 0.8030 - val_loss: 0.4771 - val_acc: 0.7538\n",
      "Epoch 24/35\n",
      "2685/2685 [==============================] - 1s 541us/step - loss: 0.3945 - acc: 0.8231 - val_loss: 0.4821 - val_acc: 0.7744\n",
      "Epoch 25/35\n",
      "2685/2685 [==============================] - 1s 548us/step - loss: 0.3886 - acc: 0.8350 - val_loss: 0.4417 - val_acc: 0.7744\n",
      "Epoch 26/35\n",
      "2685/2685 [==============================] - 1s 552us/step - loss: 0.3949 - acc: 0.8186 - val_loss: 0.4691 - val_acc: 0.7632\n",
      "Epoch 27/35\n",
      "2685/2685 [==============================] - 1s 553us/step - loss: 0.3861 - acc: 0.8298 - val_loss: 0.4768 - val_acc: 0.7688\n",
      "Epoch 28/35\n",
      "2685/2685 [==============================] - 1s 541us/step - loss: 0.3857 - acc: 0.8291 - val_loss: 0.4463 - val_acc: 0.7801\n",
      "Epoch 29/35\n",
      "2685/2685 [==============================] - 2s 559us/step - loss: 0.3890 - acc: 0.8253 - val_loss: 0.4578 - val_acc: 0.7820\n",
      "Epoch 30/35\n",
      "2685/2685 [==============================] - 2s 567us/step - loss: 0.3943 - acc: 0.8287 - val_loss: 0.4236 - val_acc: 0.7951\n",
      "Epoch 31/35\n",
      "2685/2685 [==============================] - 1s 550us/step - loss: 0.3930 - acc: 0.8231 - val_loss: 0.4661 - val_acc: 0.7406\n",
      "Epoch 32/35\n",
      "2685/2685 [==============================] - 1s 538us/step - loss: 0.3623 - acc: 0.8451 - val_loss: 0.4546 - val_acc: 0.7876\n",
      "Epoch 33/35\n",
      "2685/2685 [==============================] - 1s 540us/step - loss: 0.3897 - acc: 0.8313 - val_loss: 0.4980 - val_acc: 0.7387\n",
      "Epoch 34/35\n",
      "2685/2685 [==============================] - 2s 578us/step - loss: 0.3655 - acc: 0.8439 - val_loss: 0.4679 - val_acc: 0.7538\n",
      "Epoch 35/35\n",
      "2685/2685 [==============================] - 1s 552us/step - loss: 0.3764 - acc: 0.8320 - val_loss: 0.4935 - val_acc: 0.7763\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3217 samples, validate on 537 samples\n",
      "Epoch 1/35\n",
      "3217/3217 [==============================] - 11s 3ms/step - loss: 0.6708 - acc: 0.6298 - val_loss: 0.6382 - val_acc: 0.6704\n",
      "Epoch 2/35\n",
      "3217/3217 [==============================] - 2s 585us/step - loss: 0.6018 - acc: 0.6571 - val_loss: 0.6078 - val_acc: 0.7318\n",
      "Epoch 3/35\n",
      "3217/3217 [==============================] - 2s 561us/step - loss: 0.5511 - acc: 0.7193 - val_loss: 0.5823 - val_acc: 0.7114\n",
      "Epoch 4/35\n",
      "3217/3217 [==============================] - 2s 573us/step - loss: 0.5046 - acc: 0.7532 - val_loss: 0.5495 - val_acc: 0.7616\n",
      "Epoch 5/35\n",
      "3217/3217 [==============================] - 2s 565us/step - loss: 0.4911 - acc: 0.7513 - val_loss: 0.6272 - val_acc: 0.7374\n",
      "Epoch 6/35\n",
      "3217/3217 [==============================] - 2s 559us/step - loss: 0.4957 - acc: 0.7619 - val_loss: 0.5036 - val_acc: 0.7225\n",
      "Epoch 7/35\n",
      "3217/3217 [==============================] - 2s 567us/step - loss: 0.5056 - acc: 0.7554 - val_loss: 0.4931 - val_acc: 0.7691\n",
      "Epoch 8/35\n",
      "3217/3217 [==============================] - 2s 570us/step - loss: 0.4755 - acc: 0.7774 - val_loss: 0.4895 - val_acc: 0.7728\n",
      "Epoch 9/35\n",
      "3217/3217 [==============================] - 2s 555us/step - loss: 0.4796 - acc: 0.7821 - val_loss: 0.4810 - val_acc: 0.7691\n",
      "Epoch 10/35\n",
      "3217/3217 [==============================] - 2s 565us/step - loss: 0.4532 - acc: 0.7886 - val_loss: 0.5615 - val_acc: 0.7449\n",
      "Epoch 11/35\n",
      "3217/3217 [==============================] - 2s 556us/step - loss: 0.4481 - acc: 0.7840 - val_loss: 0.5795 - val_acc: 0.7412\n",
      "Epoch 12/35\n",
      "3217/3217 [==============================] - 2s 561us/step - loss: 0.4351 - acc: 0.8026 - val_loss: 0.4838 - val_acc: 0.7672\n",
      "Epoch 13/35\n",
      "3217/3217 [==============================] - 2s 555us/step - loss: 0.4302 - acc: 0.7927 - val_loss: 0.4900 - val_acc: 0.7561\n",
      "Epoch 14/35\n",
      "3217/3217 [==============================] - 2s 549us/step - loss: 0.4285 - acc: 0.8063 - val_loss: 0.5250 - val_acc: 0.7616\n",
      "Epoch 15/35\n",
      "3217/3217 [==============================] - 2s 573us/step - loss: 0.4269 - acc: 0.8054 - val_loss: 0.4761 - val_acc: 0.7691\n",
      "Epoch 16/35\n",
      "3217/3217 [==============================] - 2s 577us/step - loss: 0.4282 - acc: 0.7967 - val_loss: 0.5119 - val_acc: 0.7281\n",
      "Epoch 17/35\n",
      "3217/3217 [==============================] - 2s 565us/step - loss: 0.4226 - acc: 0.8063 - val_loss: 0.4857 - val_acc: 0.7654\n",
      "Epoch 18/35\n",
      "3217/3217 [==============================] - 2s 576us/step - loss: 0.4100 - acc: 0.8129 - val_loss: 0.4517 - val_acc: 0.7728\n",
      "Epoch 19/35\n",
      "3217/3217 [==============================] - 2s 551us/step - loss: 0.4099 - acc: 0.8082 - val_loss: 0.5154 - val_acc: 0.7784\n",
      "Epoch 20/35\n",
      "3217/3217 [==============================] - 2s 572us/step - loss: 0.4334 - acc: 0.8032 - val_loss: 0.4702 - val_acc: 0.7616\n",
      "Epoch 21/35\n",
      "3217/3217 [==============================] - 2s 587us/step - loss: 0.4084 - acc: 0.8191 - val_loss: 0.4453 - val_acc: 0.7803\n",
      "Epoch 22/35\n",
      "3217/3217 [==============================] - 2s 575us/step - loss: 0.4106 - acc: 0.8210 - val_loss: 0.4707 - val_acc: 0.7709\n",
      "Epoch 23/35\n",
      "3217/3217 [==============================] - 2s 564us/step - loss: 0.4152 - acc: 0.8172 - val_loss: 0.4612 - val_acc: 0.7709\n",
      "Epoch 24/35\n",
      "3217/3217 [==============================] - 2s 554us/step - loss: 0.3899 - acc: 0.8293 - val_loss: 0.4857 - val_acc: 0.7598\n",
      "Epoch 25/35\n",
      "3217/3217 [==============================] - 2s 560us/step - loss: 0.3886 - acc: 0.8275 - val_loss: 0.4742 - val_acc: 0.7654\n",
      "Epoch 26/35\n",
      "3217/3217 [==============================] - 2s 558us/step - loss: 0.4037 - acc: 0.8262 - val_loss: 0.4852 - val_acc: 0.7281\n",
      "Epoch 27/35\n",
      "3217/3217 [==============================] - 2s 557us/step - loss: 0.3902 - acc: 0.8247 - val_loss: 0.4942 - val_acc: 0.7616\n",
      "Epoch 28/35\n",
      "3217/3217 [==============================] - 2s 575us/step - loss: 0.3804 - acc: 0.8340 - val_loss: 0.4533 - val_acc: 0.7616\n",
      "Epoch 29/35\n",
      "3217/3217 [==============================] - 2s 558us/step - loss: 0.3893 - acc: 0.8287 - val_loss: 0.5050 - val_acc: 0.7393\n",
      "Epoch 30/35\n",
      "3217/3217 [==============================] - 2s 578us/step - loss: 0.4053 - acc: 0.8228 - val_loss: 0.5005 - val_acc: 0.7598\n",
      "Epoch 31/35\n",
      "3217/3217 [==============================] - 2s 572us/step - loss: 0.3761 - acc: 0.8399 - val_loss: 0.4691 - val_acc: 0.7523\n",
      "Epoch 32/35\n",
      "3217/3217 [==============================] - 2s 564us/step - loss: 0.3712 - acc: 0.8405 - val_loss: 0.4487 - val_acc: 0.7747\n",
      "Epoch 33/35\n",
      "3217/3217 [==============================] - 2s 560us/step - loss: 0.3735 - acc: 0.8371 - val_loss: 0.4555 - val_acc: 0.7784\n",
      "Epoch 34/35\n",
      "3217/3217 [==============================] - 2s 573us/step - loss: 0.3646 - acc: 0.8449 - val_loss: 0.4638 - val_acc: 0.7858\n",
      "Epoch 35/35\n",
      "3217/3217 [==============================] - 2s 581us/step - loss: 0.3842 - acc: 0.8353 - val_loss: 0.4771 - val_acc: 0.7691\n"
     ]
    }
   ],
   "source": [
    "results_val = {}\n",
    "results_train = {}\n",
    "for i in range(len(training_p38)):\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = gcn.dataframe_to_gcn_input(validation_p38[i])\n",
    "        Y_cold = validation_p38[i].Binary\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = gcn.dataframe_to_gcn_input(training_p38[i])\n",
    "        Y = training_p38[i].Binary\n",
    "        gcn_encoder = gcn.build_encoder()\n",
    "        gcn_model = gcn.build_model(gcn_encoder)\n",
    "        gcn_model.fit([X_atoms_train,X_bonds_train,X_edges_train],Y,\n",
    "                    batch_size = gcn_best['batch_size'],\n",
    "                    epochs = gcn_best['n_epochs'],\n",
    "                    verbose = 1,\n",
    "                    shuffle=True,\n",
    "                    validation_data = ([X_atoms_cold,X_bonds_cold,X_edges_cold],Y_cold))\n",
    "        y_pred_val = gcn_model.predict([X_atoms_cold,X_bonds_cold,X_edges_cold])\n",
    "        y_pred_train = gcn_model.predict([X_atoms_train,X_bonds_train,X_edges_train])\n",
    "        if i < 6:\n",
    "            results_val['Fold %s'%i] = calculate_metrics(np.array(Y_cold), y_pred_val.squeeze())\n",
    "            results_train['Fold %s'%i] = calculate_metrics(np.array(Y),y_pred_train.squeeze())\n",
    "        elif i == 6:\n",
    "            results_val['Test'] = calculate_metrics(np.array(Y_cold), y_pred_val.squeeze())\n",
    "            results_train['Test'] = calculate_metrics(np.array(Y),y_pred_train.squeeze())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>map</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fold 0</th>\n",
       "      <td>0.924027</td>\n",
       "      <td>1553.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>759.0</td>\n",
       "      <td>0.843969</td>\n",
       "      <td>0.839602</td>\n",
       "      <td>0.772912</td>\n",
       "      <td>0.862687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 1</th>\n",
       "      <td>0.944422</td>\n",
       "      <td>1405.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>917.0</td>\n",
       "      <td>0.904470</td>\n",
       "      <td>0.782423</td>\n",
       "      <td>0.899020</td>\n",
       "      <td>0.866418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 2</th>\n",
       "      <td>0.926770</td>\n",
       "      <td>1628.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>0.865787</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.559476</td>\n",
       "      <td>0.814552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 3</th>\n",
       "      <td>0.921972</td>\n",
       "      <td>1466.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>0.870721</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.794547</td>\n",
       "      <td>0.851493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 4</th>\n",
       "      <td>0.923190</td>\n",
       "      <td>1534.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>0.847410</td>\n",
       "      <td>0.831849</td>\n",
       "      <td>0.750754</td>\n",
       "      <td>0.851119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 5</th>\n",
       "      <td>0.924547</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.866469</td>\n",
       "      <td>0.766438</td>\n",
       "      <td>0.836188</td>\n",
       "      <td>0.854376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.932794</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>805.0</td>\n",
       "      <td>0.876039</td>\n",
       "      <td>0.869330</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.842711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         roc_auc      tn     fp     fn     tp       map  precision    recall  \\\n",
       "Fold 0  0.924027  1553.0  145.0  223.0  759.0  0.843969   0.839602  0.772912   \n",
       "Fold 1  0.944422  1405.0  255.0  103.0  917.0  0.904470   0.782423  0.899020   \n",
       "Fold 2  0.926770  1628.0   60.0  437.0  555.0  0.865787   0.902439  0.559476   \n",
       "Fold 3  0.921972  1466.0  187.0  211.0  816.0  0.870721   0.813559  0.794547   \n",
       "Fold 4  0.923190  1534.0  151.0  248.0  747.0  0.847410   0.831849  0.750754   \n",
       "Fold 5  0.924547  1513.0  238.0  153.0  781.0  0.866469   0.766438  0.836188   \n",
       "Test    0.932794  1906.0  121.0  385.0  805.0  0.876039   0.869330  0.676471   \n",
       "\n",
       "        accuracy  \n",
       "Fold 0  0.862687  \n",
       "Fold 1  0.866418  \n",
       "Fold 2  0.814552  \n",
       "Fold 3  0.851493  \n",
       "Fold 4  0.851119  \n",
       "Fold 5  0.854376  \n",
       "Test    0.842711  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results_train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>map</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fold 0</th>\n",
       "      <td>0.816738</td>\n",
       "      <td>290.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.713529</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.533654</td>\n",
       "      <td>0.746741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 1</th>\n",
       "      <td>0.862590</td>\n",
       "      <td>301.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.789743</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.798883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 2</th>\n",
       "      <td>0.860597</td>\n",
       "      <td>320.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.758857</td>\n",
       "      <td>0.820755</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.757914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 3</th>\n",
       "      <td>0.876546</td>\n",
       "      <td>337.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.757467</td>\n",
       "      <td>0.725926</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>0.810056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 4</th>\n",
       "      <td>0.871495</td>\n",
       "      <td>310.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.755338</td>\n",
       "      <td>0.779310</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.787709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold 5</th>\n",
       "      <td>0.867393</td>\n",
       "      <td>192.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.856287</td>\n",
       "      <td>0.724590</td>\n",
       "      <td>0.863281</td>\n",
       "      <td>0.776316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.846375</td>\n",
       "      <td>332.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.690583</td>\n",
       "      <td>0.743119</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.769088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         roc_auc     tn    fp     fn     tp       map  precision    recall  \\\n",
       "Fold 0  0.816738  290.0  39.0   97.0  111.0  0.713529   0.740000  0.533654   \n",
       "Fold 1  0.862590  301.0  66.0   42.0  128.0  0.789743   0.659794  0.752941   \n",
       "Fold 2  0.860597  320.0  19.0  111.0   87.0  0.758857   0.820755  0.439394   \n",
       "Fold 3  0.876546  337.0  37.0   65.0   98.0  0.757467   0.725926  0.601227   \n",
       "Fold 4  0.871495  310.0  32.0   82.0  113.0  0.755338   0.779310  0.579487   \n",
       "Fold 5  0.867393  192.0  84.0   35.0  221.0  0.856287   0.724590  0.863281   \n",
       "Test    0.846375  332.0  28.0   96.0   81.0  0.690583   0.743119  0.457627   \n",
       "\n",
       "        accuracy  \n",
       "Fold 0  0.746741  \n",
       "Fold 1  0.798883  \n",
       "Fold 2  0.757914  \n",
       "Fold 3  0.810056  \n",
       "Fold 4  0.787709  \n",
       "Fold 5  0.776316  \n",
       "Test    0.769088  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results_val).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_model.save('gcn.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
