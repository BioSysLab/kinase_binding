{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.callbacks import History, ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from data_analysis import calculate_metrics, load_weights_and_evaluate\n",
    "from model_builders import GCN_pretraining\n",
    "from hyperparameter_tuning_GCN import objective\n",
    "from functools import partial\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pickle\n",
    "import dill\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model callbacks on training\n",
    "es = EarlyStopping(monitor='loss',patience=8, min_delta=0)\n",
    "rlr = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=4, verbose=1, min_lr=0.0000001)\n",
    "\n",
    "model_params = {\n",
    "        \"num_layers\" : 3,\n",
    "        \"max_atoms\" : 70,\n",
    "        \"num_atom_features\" : 62,\n",
    "        \"num_atom_features_original\" : 62,\n",
    "        \"num_bond_features\" : 6,\n",
    "        \"max_degree\" : 5,\n",
    "        \"conv_width\" : [32,64,96],\n",
    "        \"fp_length\" : [96,96,96],\n",
    "        \"activ_enc\" : \"selu\",\n",
    "        \"activ_dec\" : \"selu\",\n",
    "        \"learning_rates\" : [0.001,0.001,0.001],\n",
    "        \"learning_rates_fp\": [0.005,0.005,0.005],\n",
    "        \"losses_conv\" : {\n",
    "                    \"neighbor_output\": \"mean_squared_error\",\n",
    "                    \"self_output\": \"mean_squared_error\",\n",
    "                    },\n",
    "        \"lossWeights\" : {\"neighbor_output\": 1.0, \"self_output\": 1.0},\n",
    "        \"metrics\" : \"mse\",\n",
    "        \"loss_fp\" : \"mean_squared_error\",\n",
    "        \"enc_layer_names\" : [\"enc_1\", \"enc_2\", \"enc_3\"],\n",
    "        'callbacks' : [es,rlr],\n",
    "        'adam_decay': 0.0005329142291371636,\n",
    "        'beta': 5,\n",
    "        'p': 0.004465204118126482,\n",
    "        'dense_size' : [96,96,48],\n",
    "        'dropout_rate' : [0.1,0.1],\n",
    "        'lr' : 0.001,\n",
    "        'batch_size' : int(64),\n",
    "        'n_epochs' : int(5)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fspace = {\n",
    "    'conv1' : hp.quniform('conv1', 32, 96, 8),\n",
    "    'conv2' : hp.quniform('conv2', 48, 128, 8),\n",
    "    'conv3' : hp.quniform('conv3', 64, 168, 8),\n",
    "    'fp' : hp.quniform('fp', 64, 196, 8),\n",
    "    'dense1' : hp.quniform('dense1',96,256,32),\n",
    "    'dense2' : hp.quniform('dense2',96,256,32),\n",
    "    'dense3' : hp.quniform('dense3',48,128,32),\n",
    "    'dropout_rate' : hp.uniform('dropout_rate',0.1,0.5),\n",
    "    'lr' : hp.uniform('lr',0.0005,0.01),\n",
    "    'n_epochs' : hp.quniform('n_epochs',15,40,5) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1 = 'pi3k'\n",
    "base_path_1 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_1 = base_path_1+f'/data/{target_1}/data.csv'\n",
    "df_p38=pd.read_csv(data_fpath_1).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_1+f'/data/{target_1}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_p38 = dill.load(in_f)\n",
    "\n",
    "with open(base_path_1+f'/data/{target_1}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_p38 = dill.load(in_f)\n",
    "    \n",
    "target_2 = 'akt1'\n",
    "base_path_2 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_2 = base_path_2+f'/data/{target_2}/data.csv'\n",
    "df_akt1 = pd.read_csv(data_fpath_2).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_2+f'/data/{target_2}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_akt1 = dill.load(in_f)\n",
    "with open(base_path_2+f'/data/{target_2}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_akt1 = dill.load(in_f)\n",
    "    \n",
    "target_3 = 'pi3k'\n",
    "base_path_3 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_3 = base_path_3+f'/data/{target_3}/data.csv'\n",
    "df_pi3k = pd.read_csv(data_fpath_3).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_3+f'/data/{target_3}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_pi3k = dill.load(in_f)\n",
    "with open(base_path_3+f'/data/{target_3}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_pi3k = dill.load(in_f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation Splits (our test set)\n",
    "training_p38 = df_p38.loc[train_test_folds_p38[0]]\n",
    "validation_p38 = df_p38.loc[train_test_folds_p38[1]]\n",
    "\n",
    "\n",
    "training_akt1 = df_akt1.loc[train_test_folds_akt1[0]]\n",
    "validation_akt1 = df_akt1.loc[train_test_folds_akt1[1]]\n",
    "               \n",
    "\n",
    "training_pi3k = df_pi3k.loc[train_test_folds_pi3k[0]]\n",
    "validation_pi3k = df_pi3k.loc[train_test_folds_pi3k[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3186 371\n"
     ]
    }
   ],
   "source": [
    "#AVE Bias splits (test) only p38\n",
    "ave_p38_train = pd.read_csv('data/p38/split_aveb/train_all.csv', index_col=0)\n",
    "ave_p38_val = pd.read_csv('data/p38/split_aveb/test.csv', index_col = 0)\n",
    "print(len(ave_p38_train),len(ave_p38_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3190 564\n"
     ]
    }
   ],
   "source": [
    "#Random splits with sklearn (on our test set)\n",
    "df_p38 = df_p38.reset_index(drop=True)\n",
    "X_train_p38, X_val_p38, Y_train_p38, Y_val_p38 = train_test_split(df_p38.rdkit,\n",
    "                                                                  df_p38.Binary,\n",
    "                                                                  test_size = 0.15,\n",
    "                                                                  train_size = 0.85,\n",
    "                                                                  shuffle = True)\n",
    "X_train_p38 = pd.DataFrame(X_train_p38)\n",
    "X_val_p38 = pd.DataFrame(X_val_p38)\n",
    "print(len(X_train_p38),len(X_val_p38))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1819 321\n"
     ]
    }
   ],
   "source": [
    "df_akt1 = df_akt1.reset_index(drop=True)\n",
    "X_train_akt1, X_val_akt1, Y_train_akt1, Y_val_akt1 = train_test_split(df_akt1.rdkit,\n",
    "                                                                     df_akt1.Binary,\n",
    "                                                                     test_size = 0.15,\n",
    "                                                                     train_size = 0.85,\n",
    "                                                                     shuffle = True)\n",
    "X_train_akt1 = pd.DataFrame(X_train_akt1)\n",
    "X_val_akt1 = pd.DataFrame(X_val_akt1)\n",
    "print(len(X_train_akt1),len(X_val_akt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3190 564\n"
     ]
    }
   ],
   "source": [
    "df_pi3k = df_pi3k.reset_index(drop=True)\n",
    "X_train_pi3k, X_val_pi3k, Y_train_pi3k, Y_val_pi3k = train_test_split(df_pi3k.rdkit,\n",
    "                                                                      df_pi3k.Binary,\n",
    "                                                                      test_size = 0.15,\n",
    "                                                                      train_size = 0.85,\n",
    "                                                                      shuffle = True)\n",
    "X_train_pi3k = pd.DataFrame(X_train_pi3k)\n",
    "X_val_pi3k = pd.DataFrame(X_val_pi3k)\n",
    "print(len(X_train_pi3k),len(X_val_pi3k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin_objective = partial(objective, train_sets = training_p38, val_sets = validation_p38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials():\n",
    "\n",
    "    trials_step = 0  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    max_trials = 0  # initial max_trials. put something small to not have to wait\n",
    "\n",
    "    \n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(\"gcn.hyperopt\", \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    best = fmin(fn = fmin_objective, space = fspace, algo=tpe.suggest, max_evals=max_trials, trials=trials)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    \n",
    "    # save the trials object\n",
    "    with open(\"gcn.hyperopt\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "    return(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = run_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = trials.trials[0]['result']['loss']\n",
    "for i in range(1,len(trials.trials)):\n",
    "    if (trials.trials[i]['result']['loss'] <=  best_loss):\n",
    "        best_loss = trials.trials[i]['result']['loss']\n",
    "        index = i\n",
    "best_params = trials.trials[index]['misc']['vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameter_tuning_GCN import GCN_hyper\n",
    "es = EarlyStopping(monitor='loss',patience=8, min_delta=0)\n",
    "rlr = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=4, verbose=1, min_lr=0.0000001)\n",
    "gcn_best = {\n",
    "        \"num_layers\" : 3,\n",
    "        \"max_atoms\" : 70,\n",
    "        \"num_atom_features\" : 62,\n",
    "        \"num_atom_features_original\" : 62,\n",
    "        \"num_bond_features\" : 6,\n",
    "        \"max_degree\" : 5,\n",
    "        \"conv_width\" : [int(best_params['conv1'][0]), int(best_params['conv2'][0]), int(best_params['conv3'][0])],\n",
    "        \"fp_length\" : [int(best_params['fp'][0]), int(best_params['fp'][0]), int(best_params['fp'][0])],\n",
    "        \"activ_enc\" : \"selu\",\n",
    "        \"activ_dec\" : \"selu\",\n",
    "        \"learning_rates\" : [0.001,0.001,0.001],\n",
    "        \"learning_rates_fp\": [0.005,0.005,0.005],\n",
    "        \"losses_conv\" : {\n",
    "                    \"neighbor_output\": \"mean_squared_error\",\n",
    "                    \"self_output\": \"mean_squared_error\",\n",
    "                    },\n",
    "        \"lossWeights\" : {\"neighbor_output\": 1.0, \"self_output\": 1.0},\n",
    "        \"metrics\" : \"mse\",\n",
    "        \"loss_fp\" : \"mean_squared_error\",\n",
    "        \"enc_layer_names\" : [\"enc_1\", \"enc_2\", \"enc_3\"],\n",
    "        'callbacks' : [es,rlr],\n",
    "        'adam_decay': 0.0005329142291371636,\n",
    "        'beta': 5,\n",
    "        'p': 0.004465204118126482,\n",
    "        'dense_size' : [int(best_params['dense1'][0]), int(best_params['dense2'][0]), int(best_params['dense3'][0])],\n",
    "        'dropout_rate' : [best_params['dropout_rate'][0], best_params['dropout_rate'][0]],\n",
    "        'lr' : best_params['lr'][0],\n",
    "        'batch_size' : int(64),\n",
    "        'n_epochs' : int(best_params['n_epochs'][0])\n",
    "        }\n",
    "gcn = GCN_hyper(gcn_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_p38 = [training_p38, ave_p38_train, X_train_p38]\n",
    "val_list_p38 = [validation_p38, ave_p38_val, X_val_p38]\n",
    "\n",
    "train_list_akt1 = [training_akt1, X_train_akt1]\n",
    "val_list_akt1 = [validation_akt1, X_val_akt1]\n",
    "\n",
    "train_list_pi3k = [training_pi3k, X_train_pi3k]\n",
    "val_list_pi3k = [validation_pi3k, X_val_pi3k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_p38 = {}\n",
    "#eval_akt1 = {}\n",
    "#eval_pi3k = {}\n",
    "for i in range(len(train_list_p38)):\n",
    "    if i == 2:\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = gcn.dataframe_to_gcn_input(val_list_p38[i])\n",
    "        Y_cold = Y_val_p38\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = gcn.dataframe_to_gcn_input(train_list_p38[i])\n",
    "        Y = Y_train_p38\n",
    "    else:\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = gcn.dataframe_to_gcn_input(val_list_p38[i])\n",
    "        Y_cold = val_list_p38[i].Binary\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = gcn.dataframe_to_gcn_input(train_list_p38[i])\n",
    "        Y = train_list_p38[i].Binary    \n",
    "    \n",
    "    gcn_encoder = gcn.build_encoder()\n",
    "    gcn_model = gcn.build_model(gcn_encoder)\n",
    "    gcn_model.fit([X_atoms_train,X_bonds_train,X_edges_train],Y,\n",
    "                  batch_size = gcn_best['batch_size'],\n",
    "                  epochs = gcn_best['n_epochs'],\n",
    "                  verbose = 1,\n",
    "                  shuffle=True,\n",
    "                  validation_data = ([X_atoms_cold,X_bonds_cold,X_edges_cold],Y_cold))\n",
    "    y_pred_val = gcn_model.predict([X_atoms_cold,X_bonds_cold,X_edges_cold])\n",
    "    if i == 0:\n",
    "        eval_p38['Test'] = calculate_metrics(np.array(Y_cold), y_pred_val.squeeze())\n",
    "    elif i == 1:\n",
    "        eval_p38['Ave'] = calculate_metrics(np.array(Y_cold), y_pred_val.squeeze())\n",
    "    elif i == 2:\n",
    "        eval_p38['Random'] = calculate_metrics(np.array(Y_cold), y_pred_val.squeeze())\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_p38 = pd.DataFrame(eval_p38).T\n",
    "eval_p38.to_csv('../../../../Desktop/binding/thesis english/Results/1-DeepGCN/p38.csv')\n",
    "eval_p38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_akt1 = {}\n",
    "for i in range(len(train_list_akt1)):\n",
    "    if i == 1:\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = gcn.dataframe_to_gcn_input(val_list_akt1[i])\n",
    "        Y_cold = Y_val_akt1\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = gcn.dataframe_to_gcn_input(train_list_akt1[i])\n",
    "        Y = Y_train_akt1\n",
    "    else:\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = gcn.dataframe_to_gcn_input(val_list_akt1[i])\n",
    "        Y_cold = val_list_akt1[i].Binary\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = gcn.dataframe_to_gcn_input(train_list_akt1[i])\n",
    "        Y = train_list_akt1[i].Binary    \n",
    "    \n",
    "    gcn_encoder = gcn.build_encoder()\n",
    "    gcn_model = gcn.build_model(gcn_encoder)\n",
    "    gcn_model.fit([X_atoms_train,X_bonds_train,X_edges_train],Y,\n",
    "                  batch_size = gcn_best['batch_size'],\n",
    "                  epochs = gcn_best['n_epochs'],\n",
    "                  verbose = 1,\n",
    "                  shuffle=True,\n",
    "                  validation_data = ([X_atoms_cold,X_bonds_cold,X_edges_cold],Y_cold))\n",
    "    y_pred_val = gcn_model.predict([X_atoms_cold,X_bonds_cold,X_edges_cold])\n",
    "    if i == 0:\n",
    "        eval_akt1['Test'] = calculate_metrics(np.array(Y_cold), y_pred_val.squeeze())\n",
    "    elif i == 1:\n",
    "        eval_akt1['Random'] = calculate_metrics(np.array(Y_cold), y_pred_val.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_akt1 = pd.DataFrame(eval_akt1).T\n",
    "eval_akt1.to_csv('../../../../Desktop/binding/thesis english/Results/1-DeepGCN/akt1.csv')\n",
    "eval_akt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pi3k = {}\n",
    "for i in range(len(train_list_pi3k)):\n",
    "    if i == 1:\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = gcn.dataframe_to_gcn_input(val_list_pi3k[i])\n",
    "        Y_cold = Y_val_pi3k\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = gcn.dataframe_to_gcn_input(train_list_pi3k[i])\n",
    "        Y = Y_train_pi3k\n",
    "    else:\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = gcn.dataframe_to_gcn_input(val_list_pi3k[i])\n",
    "        Y_cold = val_list_pi3k[i].Binary\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = gcn.dataframe_to_gcn_input(train_list_pi3k[i])\n",
    "        Y = train_list_pi3k[i].Binary    \n",
    "    \n",
    "    gcn_encoder = gcn.build_encoder()\n",
    "    gcn_model = gcn.build_model(gcn_encoder)\n",
    "    gcn_model.fit([X_atoms_train,X_bonds_train,X_edges_train],Y,\n",
    "                  batch_size = gcn_best['batch_size'],\n",
    "                  epochs = gcn_best['n_epochs'],\n",
    "                  verbose = 1,\n",
    "                  shuffle=True,\n",
    "                  validation_data = ([X_atoms_cold,X_bonds_cold,X_edges_cold],Y_cold))\n",
    "    y_pred_val = gcn_model.predict([X_atoms_cold,X_bonds_cold,X_edges_cold])\n",
    "    if i == 0:\n",
    "        eval_pi3k['Test'] = calculate_metrics(np.array(Y_cold), y_pred_val.squeeze())\n",
    "    elif i == 1:\n",
    "        eval_pi3k['Random'] = calculate_metrics(np.array(Y_cold), y_pred_val.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pi3k = pd.DataFrame(eval_pi3k).T\n",
    "eval_pi3k.to_csv('../../../../Desktop/binding/thesis english/Results/1-DeepGCN/pi3k.csv')\n",
    "eval_pi3k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
