{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.callbacks import History, ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from data_analysis import calculate_metrics\n",
    "from functools import partial\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pickle\n",
    "import dill\n",
    "from hyper_mining import objective_fn\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fspace = {\n",
    "    'conv1' : hp.quniform('conv1', 32, 64, 8),\n",
    "    'conv2' : hp.quniform('conv2', 64, 128, 8),\n",
    "    'conv3' : hp.quniform('conv3', 128, 168, 8),\n",
    "    'fp' : hp.quniform('fp', 96, 196, 8),\n",
    "    'dense1' : hp.quniform('dense1',96,512,32),\n",
    "    'dense2' : hp.quniform('dense2',96,512,32),\n",
    "    'dense3' : hp.quniform('dense3',64,512,32),\n",
    "    'dropout_rate' : hp.uniform('dropout_rate',0.1,0.5),\n",
    "    'lr' : hp.uniform('lr',0.000001,0.01),\n",
    "    'n_epochs' : hp.quniform('n_epochs',15,60,5),\n",
    "    'batch_size' : hp.quniform('batch_size',64,256,16),\n",
    "    'colsample_bylevel' : hp.uniform('colsample_bylevel', 0.1, 1), \n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0.1, 1), \n",
    "    'gamma' : hp.uniform('gamma', 0.1, 1), \n",
    "    'learning_rate' : hp.uniform('learning_rate', 0.1, 1),\n",
    "    'max_delta_step' : hp.quniform('max_delta_step',1,10,1),\n",
    "    'max_depth' : hp.quniform('max_depth',6, 12, 1),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight',10 ,500 ,5),\n",
    "    'reg_alpha' : hp.uniform('reg_alpha',0.1,100),\n",
    "    'reg_lambda' : hp.uniform('reg_lambda',0.1,100),\n",
    "    'subsample' : hp.uniform('subsample',0.1,1.0),\n",
    "    'max_bin' : hp.quniform('max_bin',16,256,16)\n",
    "    #'margin' : hp.uniform('margin',0.2,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1 = 'pi3k'\n",
    "base_path_1 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_1 = base_path_1+f'/data/{target_1}/data.csv'\n",
    "df_p38=pd.read_csv(data_fpath_1).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_1+f'/data/{target_1}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_p38 = dill.load(in_f)\n",
    "\n",
    "with open(base_path_1+f'/data/{target_1}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_p38 = dill.load(in_f)\n",
    "    \n",
    "target_2 = 'akt1'\n",
    "base_path_2 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_2 = base_path_2+f'/data/{target_2}/data.csv'\n",
    "df_akt1 = pd.read_csv(data_fpath_2).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_2+f'/data/{target_2}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_akt1 = dill.load(in_f)\n",
    "with open(base_path_2+f'/data/{target_2}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_akt1 = dill.load(in_f)\n",
    "    \n",
    "target_3 = 'pi3k'\n",
    "base_path_3 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_3 = base_path_3+f'/data/{target_3}/data.csv'\n",
    "df_pi3k = pd.read_csv(data_fpath_3).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_3+f'/data/{target_3}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_pi3k = dill.load(in_f)\n",
    "with open(base_path_3+f'/data/{target_3}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_pi3k = dill.load(in_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation Splits (our test set)\n",
    "training_p38 = df_p38.loc[train_test_folds_p38[0]]\n",
    "validation_p38 = df_p38.loc[train_test_folds_p38[1]]\n",
    "\n",
    "\n",
    "training_akt1 = df_akt1.loc[train_test_folds_akt1[0]]\n",
    "validation_akt1 = df_akt1.loc[train_test_folds_akt1[1]]\n",
    "               \n",
    "\n",
    "training_pi3k = df_pi3k.loc[train_test_folds_pi3k[0]]\n",
    "validation_pi3k = df_pi3k.loc[train_test_folds_pi3k[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3186 371\n"
     ]
    }
   ],
   "source": [
    "#AVE Bias splits (test) only p38\n",
    "ave_p38_train = pd.read_csv('data/p38/split_aveb/train_all.csv', index_col=0)\n",
    "ave_p38_val = pd.read_csv('data/p38/split_aveb/test.csv', index_col = 0)\n",
    "print(len(ave_p38_train),len(ave_p38_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3190 564\n"
     ]
    }
   ],
   "source": [
    "#Random splits with sklearn (on our test set)\n",
    "df_p38 = df_p38.reset_index(drop=True)\n",
    "X_train_p38, X_val_p38, Y_train_p38, Y_val_p38 = train_test_split(df_p38.rdkit,\n",
    "                                                                  df_p38.Binary,\n",
    "                                                                  test_size = 0.15,\n",
    "                                                                  train_size = 0.85,\n",
    "                                                                  shuffle = True)\n",
    "X_train_p38 = pd.DataFrame(X_train_p38)\n",
    "X_val_p38 = pd.DataFrame(X_val_p38)\n",
    "print(len(X_train_p38),len(X_val_p38))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1819 321\n"
     ]
    }
   ],
   "source": [
    "df_akt1 = df_akt1.reset_index(drop=True)\n",
    "X_train_akt1, X_val_akt1, Y_train_akt1, Y_val_akt1 = train_test_split(df_akt1.rdkit,\n",
    "                                                                     df_akt1.Binary,\n",
    "                                                                     test_size = 0.15,\n",
    "                                                                     train_size = 0.85,\n",
    "                                                                     shuffle = True)\n",
    "X_train_akt1 = pd.DataFrame(X_train_akt1)\n",
    "X_val_akt1 = pd.DataFrame(X_val_akt1)\n",
    "print(len(X_train_akt1),len(X_val_akt1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3190 564\n"
     ]
    }
   ],
   "source": [
    "df_pi3k = df_pi3k.reset_index(drop=True)\n",
    "X_train_pi3k, X_val_pi3k, Y_train_pi3k, Y_val_pi3k = train_test_split(df_pi3k.rdkit,\n",
    "                                                                      df_pi3k.Binary,\n",
    "                                                                      test_size = 0.15,\n",
    "                                                                      train_size = 0.85,\n",
    "                                                                      shuffle = True)\n",
    "X_train_pi3k = pd.DataFrame(X_train_pi3k)\n",
    "X_val_pi3k = pd.DataFrame(X_val_pi3k)\n",
    "print(len(X_train_pi3k),len(X_val_pi3k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin_objective = partial(objective_fn, train_sets = training_p38, val_sets = validation_p38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials():\n",
    "\n",
    "    trials_step = 0  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    max_trials = 1  # initial max_trials. put something small to not have to wait\n",
    "\n",
    "    \n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(\"gcn_xgb.hyperopt\", \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    best = fmin(fn = fmin_objective, space = fspace, algo=tpe.suggest, max_evals=max_trials, trials=trials)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    \n",
    "    # save the trials object\n",
    "    with open(\"gcn_xgb.hyperopt\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "    return(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved Trials! Loading...\n",
      "Rerunning from 154 trials to 154 (+0) trials\n",
      "100%|████████████████████████████████████████████████████████████████████████| 154/154 [00:00<?, ?trial/s, best loss=?]\n",
      "Best: {'batch_size': 288.0, 'colsample_bylevel': 0.4371082812232264, 'colsample_bytree': 0.4179415558635843, 'conv1': 56.0, 'conv2': 88.0, 'conv3': 136.0, 'dense1': 384.0, 'dense2': 288.0, 'dense3': 224.0, 'dropout_rate': 0.27225175676555935, 'fp': 152.0, 'gamma': 0.919836526180396, 'learning_rate': 0.41409388868400826, 'lr': 0.0008110012706176706, 'max_bin': 48.0, 'max_delta_step': 2.0, 'max_depth': 7.0, 'min_child_weight': 20.0, 'n_epochs': 25.0, 'reg_alpha': 42.8887552483495, 'reg_lambda': 12.306130216692438, 'subsample': 0.6038298323514097}\n"
     ]
    }
   ],
   "source": [
    "trials = run_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = trials.trials[0]['result']['loss']\n",
    "for i in range(1,len(trials.trials)):\n",
    "    if (trials.trials[i]['result']['loss'] <=  best_loss):\n",
    "        best_loss = trials.trials[i]['result']['loss']\n",
    "        index = i\n",
    "best_params = trials.trials[index]['misc']['vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyper_mining import XGB_predictor,GCN_online_mining_test\n",
    "from data_analysis import calculate_metrics\n",
    "es = EarlyStopping(monitor='loss',patience=8, min_delta=0)\n",
    "rlr = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=4, verbose=1, min_lr=0.0000001)\n",
    "gcn_best = {\n",
    "        \"num_layers\" : 3,\n",
    "        \"max_atoms\" : 70,\n",
    "        \"num_atom_features\" : 62,\n",
    "        \"num_atom_features_original\" : 62,\n",
    "        \"num_bond_features\" : 6,\n",
    "        \"max_degree\" : 5,\n",
    "        \"conv_width\" : [int(best_params['conv1'][0]), int(best_params['conv2'][0]), int(best_params['conv3'][0])],\n",
    "        \"fp_length\" : [int(best_params['fp'][0]), int(best_params['fp'][0]), int(best_params['fp'][0])],\n",
    "        \"activ_enc\" : \"selu\",\n",
    "        \"activ_dec\" : \"selu\",\n",
    "        \"learning_rates\" : [0.001,0.001,0.001],\n",
    "        \"learning_rates_fp\": [0.005,0.005,0.005],\n",
    "        \"losses_conv\" : {\n",
    "                    \"neighbor_output\": \"mean_squared_error\",\n",
    "                    \"self_output\": \"mean_squared_error\",\n",
    "                    },\n",
    "        \"lossWeights\" : {\"neighbor_output\": 1.0, \"self_output\": 1.0},\n",
    "        \"metrics\" : \"mse\",\n",
    "        \"loss_fp\" : \"mean_squared_error\",\n",
    "        \"enc_layer_names\" : [\"enc_1\", \"enc_2\", \"enc_3\"],\n",
    "        'callbacks' : [es,rlr],\n",
    "        'adam_decay': 0.0005329142291371636,\n",
    "        'beta': 5,\n",
    "        'p': 0.004465204118126482,\n",
    "        'dense_size' : [int(best_params['dense1'][0]), int(best_params['dense2'][0]), int(best_params['dense3'][0])],\n",
    "        'dropout_rate' : [best_params['dropout_rate'][0], best_params['dropout_rate'][0]],\n",
    "        'lr' : best_params['lr'][0],\n",
    "        'batch_size' : int(best_params['batch_size'][0]),\n",
    "        'n_epochs' : int(best_params['n_epochs'][0])\n",
    "        #'margin' : best_params['margin'][0]\n",
    "        }\n",
    "xgb_best = {\n",
    "        \"colsample_bylevel\" : best_params['colsample_bylevel'][0],\n",
    "        \"colsample_bytree\" : best_params['colsample_bytree'][0],\n",
    "        \"gamma\" : best_params['gamma'][0],\n",
    "        \"eta\" : best_params['learning_rate'][0],\n",
    "        \"max_delta_step\" : int(best_params['max_delta_step'][0]),\n",
    "        \"max_depth\" : int(best_params['max_depth'][0]),\n",
    "        \"min_child_weight\" : int(best_params['min_child_weight'][0]),\n",
    "        \"alpha\" : best_params['reg_alpha'][0],\n",
    "        \"lambda\" : best_params['reg_lambda'][0],\n",
    "        \"subsample\" : best_params['subsample'][0],\n",
    "        \"max_bin\" : int(best_params['max_bin'][0]),\n",
    "        \"eval_metric\":'auc',\n",
    "        \"objective\":'binary:logistic',\n",
    "        \"booster\":'gbtree'\n",
    "        #\"single_precision_histogram\" : True\n",
    "        }\n",
    "class_XGB = XGB_predictor(xgb_best)\n",
    "class_GCN = GCN_online_mining_test(gcn_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_p38 = [training_p38, ave_p38_train, X_train_p38]\n",
    "val_list_p38 = [validation_p38, ave_p38_val, X_val_p38]\n",
    "\n",
    "train_list_akt1 = [training_akt1, X_train_akt1]\n",
    "val_list_akt1 = [validation_akt1, X_val_akt1]\n",
    "\n",
    "train_list_pi3k = [training_pi3k, X_train_pi3k]\n",
    "val_list_pi3k = [validation_pi3k, X_val_pi3k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3217 samples, validate on 537 samples\n",
      "Epoch 1/25\n",
      "3217/3217 [==============================] - 6s 2ms/step - loss: 0.9970 - val_loss: 0.9983\n",
      "Epoch 2/25\n",
      "3217/3217 [==============================] - 1s 267us/step - loss: 0.9963 - val_loss: 0.9971\n",
      "Epoch 3/25\n",
      "3217/3217 [==============================] - 1s 268us/step - loss: 0.9950 - val_loss: 0.9839\n",
      "Epoch 4/25\n",
      "3217/3217 [==============================] - 1s 263us/step - loss: 0.9896 - val_loss: 0.9757\n",
      "Epoch 5/25\n",
      "3217/3217 [==============================] - 1s 264us/step - loss: 0.9854 - val_loss: 0.9709\n",
      "Epoch 6/25\n",
      "3217/3217 [==============================] - 1s 265us/step - loss: 0.9831 - val_loss: 0.9845\n",
      "Epoch 7/25\n",
      "3217/3217 [==============================] - 1s 264us/step - loss: 0.9814 - val_loss: 0.9779\n",
      "Epoch 8/25\n",
      "3217/3217 [==============================] - 1s 264us/step - loss: 0.9810 - val_loss: 0.9683\n",
      "Epoch 9/25\n",
      "3217/3217 [==============================] - 1s 266us/step - loss: 0.9812 - val_loss: 0.9819\n",
      "Epoch 10/25\n",
      "3217/3217 [==============================] - 1s 264us/step - loss: 0.9754 - val_loss: 0.9766\n",
      "Epoch 11/25\n",
      "3217/3217 [==============================] - 1s 265us/step - loss: 0.9765 - val_loss: 0.9832\n",
      "Epoch 12/25\n",
      "3217/3217 [==============================] - 1s 265us/step - loss: 0.9681 - val_loss: 0.9809\n",
      "Epoch 13/25\n",
      "3217/3217 [==============================] - 1s 266us/step - loss: 0.9784 - val_loss: 0.9579\n",
      "Epoch 14/25\n",
      "3217/3217 [==============================] - 1s 263us/step - loss: 0.9662 - val_loss: 0.9820\n",
      "Epoch 15/25\n",
      "3217/3217 [==============================] - 1s 264us/step - loss: 0.9772 - val_loss: 0.9750\n",
      "Epoch 16/25\n",
      "3217/3217 [==============================] - 1s 266us/step - loss: 0.9772 - val_loss: 0.9744\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 17/25\n",
      "3217/3217 [==============================] - 1s 262us/step - loss: 0.9765 - val_loss: 0.9780\n",
      "Epoch 18/25\n",
      "3217/3217 [==============================] - 1s 260us/step - loss: 0.9667 - val_loss: 0.9715\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 19/25\n",
      "3217/3217 [==============================] - 1s 265us/step - loss: 0.9661 - val_loss: 0.9816\n",
      "Epoch 20/25\n",
      "3217/3217 [==============================] - 1s 265us/step - loss: 0.9612 - val_loss: 0.9733\n",
      "Epoch 21/25\n",
      "3217/3217 [==============================] - 1s 263us/step - loss: 0.9456 - val_loss: 0.9733\n",
      "Epoch 22/25\n",
      "3217/3217 [==============================] - 1s 268us/step - loss: 0.9609 - val_loss: 0.9788\n",
      "Epoch 23/25\n",
      "3217/3217 [==============================] - 1s 272us/step - loss: 0.9704 - val_loss: 0.9801\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 24/25\n",
      "3217/3217 [==============================] - 1s 270us/step - loss: 0.9621 - val_loss: 0.9762\n",
      "Epoch 25/25\n",
      "3217/3217 [==============================] - 1s 267us/step - loss: 0.9577 - val_loss: 0.9699\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 5.06875803694129e-05.\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3186 samples, validate on 371 samples\n",
      "Epoch 1/25\n",
      "3186/3186 [==============================] - 6s 2ms/step - loss: 0.9973 - val_loss: 0.9940\n",
      "Epoch 2/25\n",
      "3186/3186 [==============================] - 1s 263us/step - loss: 0.9966 - val_loss: 0.9877\n",
      "Epoch 3/25\n",
      "3186/3186 [==============================] - 1s 268us/step - loss: 0.9945 - val_loss: 0.9668\n",
      "Epoch 4/25\n",
      "3186/3186 [==============================] - 1s 261us/step - loss: 0.9911 - val_loss: 0.9677\n",
      "Epoch 5/25\n",
      "3186/3186 [==============================] - 1s 266us/step - loss: 0.9873 - val_loss: 0.9646\n",
      "Epoch 6/25\n",
      "3186/3186 [==============================] - 1s 268us/step - loss: 0.9983 - val_loss: 1.0031\n",
      "Epoch 7/25\n",
      "3186/3186 [==============================] - 1s 265us/step - loss: 0.9986 - val_loss: 0.9999\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 8/25\n",
      "3186/3186 [==============================] - 1s 264us/step - loss: 0.9973 - val_loss: 0.9979\n",
      "Epoch 9/25\n",
      "3186/3186 [==============================] - 1s 265us/step - loss: 0.9960 - val_loss: 0.9931\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 10/25\n",
      "3186/3186 [==============================] - 1s 265us/step - loss: 0.9944 - val_loss: 0.9876\n",
      "Epoch 11/25\n",
      "3186/3186 [==============================] - 1s 263us/step - loss: 0.9923 - val_loss: 0.9863\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 12/25\n",
      "3186/3186 [==============================] - 1s 261us/step - loss: 0.9912 - val_loss: 0.9840\n",
      "Epoch 13/25\n",
      "3186/3186 [==============================] - 1s 262us/step - loss: 0.9902 - val_loss: 0.9811\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 5.06875803694129e-05.\n",
      "Epoch 14/25\n",
      "3186/3186 [==============================] - 1s 268us/step - loss: 0.9899 - val_loss: 0.9810\n",
      "Epoch 15/25\n",
      "3186/3186 [==============================] - 1s 268us/step - loss: 0.9890 - val_loss: 0.9767\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.534379018470645e-05.\n",
      "Epoch 16/25\n",
      "3186/3186 [==============================] - 1s 266us/step - loss: 0.9885 - val_loss: 0.9775\n",
      "Epoch 17/25\n",
      "3186/3186 [==============================] - 1s 264us/step - loss: 0.9883 - val_loss: 0.9752\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.2671895092353225e-05.\n",
      "Epoch 18/25\n",
      "3186/3186 [==============================] - 1s 265us/step - loss: 0.9866 - val_loss: 0.9748\n",
      "Epoch 19/25\n",
      "3186/3186 [==============================] - 1s 261us/step - loss: 0.9883 - val_loss: 0.9766\n",
      "Epoch 20/25\n",
      "3186/3186 [==============================] - 1s 265us/step - loss: 0.9884 - val_loss: 0.9743\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.335947546176612e-06.\n",
      "Epoch 21/25\n",
      "3186/3186 [==============================] - 1s 261us/step - loss: 0.9883 - val_loss: 0.9747\n",
      "Epoch 22/25\n",
      "3186/3186 [==============================] - 1s 261us/step - loss: 0.9884 - val_loss: 0.9762\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.167973773088306e-06.\n",
      "Epoch 23/25\n",
      "3186/3186 [==============================] - 1s 266us/step - loss: 0.9877 - val_loss: 0.9764\n",
      "Epoch 24/25\n",
      "3186/3186 [==============================] - 1s 267us/step - loss: 0.9873 - val_loss: 0.9758\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.583986886544153e-06.\n",
      "Epoch 25/25\n",
      "3186/3186 [==============================] - 1s 266us/step - loss: 0.9878 - val_loss: 0.9768\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3190 samples, validate on 564 samples\n",
      "Epoch 1/25\n",
      "3190/3190 [==============================] - 7s 2ms/step - loss: 0.9967 - val_loss: 0.9964\n",
      "Epoch 2/25\n",
      "3190/3190 [==============================] - 1s 270us/step - loss: 0.9957 - val_loss: 0.9938\n",
      "Epoch 3/25\n",
      "3190/3190 [==============================] - 1s 270us/step - loss: 0.9922 - val_loss: 0.9850\n",
      "Epoch 4/25\n",
      "3190/3190 [==============================] - 1s 270us/step - loss: 0.9884 - val_loss: 0.9817\n",
      "Epoch 5/25\n",
      "3190/3190 [==============================] - 1s 269us/step - loss: 0.9860 - val_loss: 0.9826\n",
      "Epoch 6/25\n",
      "3190/3190 [==============================] - 1s 271us/step - loss: 0.9805 - val_loss: 0.9800\n",
      "Epoch 7/25\n",
      "3190/3190 [==============================] - 1s 270us/step - loss: 0.9835 - val_loss: 0.9853\n",
      "Epoch 8/25\n",
      "3190/3190 [==============================] - 1s 272us/step - loss: 0.9825 - val_loss: 0.9826\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 9/25\n",
      "3190/3190 [==============================] - 1s 269us/step - loss: 0.9763 - val_loss: 0.9783\n",
      "Epoch 10/25\n",
      "3190/3190 [==============================] - 1s 272us/step - loss: 0.9590 - val_loss: 0.9843\n",
      "Epoch 11/25\n",
      "3190/3190 [==============================] - 1s 271us/step - loss: 0.9915 - val_loss: 1.0029\n",
      "Epoch 12/25\n",
      "3190/3190 [==============================] - 1s 276us/step - loss: 0.9897 - val_loss: 0.9902\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 13/25\n",
      "3190/3190 [==============================] - 1s 276us/step - loss: 0.9824 - val_loss: 0.9841\n",
      "Epoch 14/25\n",
      "3190/3190 [==============================] - 1s 268us/step - loss: 0.9840 - val_loss: 0.9828\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 15/25\n",
      "3190/3190 [==============================] - 1s 273us/step - loss: 0.9798 - val_loss: 0.9823\n",
      "Epoch 16/25\n",
      "3190/3190 [==============================] - 1s 279us/step - loss: 0.9759 - val_loss: 0.9813\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5.06875803694129e-05.\n",
      "Epoch 17/25\n",
      "3190/3190 [==============================] - 1s 273us/step - loss: 0.9723 - val_loss: 0.9797\n",
      "Epoch 18/25\n",
      "3190/3190 [==============================] - 1s 269us/step - loss: 0.9734 - val_loss: 0.9786\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.534379018470645e-05.\n",
      "Epoch 19/25\n",
      "3190/3190 [==============================] - 1s 274us/step - loss: 0.9695 - val_loss: 0.9818\n",
      "Epoch 20/25\n",
      "3190/3190 [==============================] - 1s 271us/step - loss: 0.9741 - val_loss: 0.9809\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.2671895092353225e-05.\n",
      "Epoch 21/25\n",
      "3190/3190 [==============================] - 1s 270us/step - loss: 0.9736 - val_loss: 0.9801\n",
      "Epoch 22/25\n",
      "3190/3190 [==============================] - 1s 267us/step - loss: 0.9696 - val_loss: 0.9836\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.335947546176612e-06.\n",
      "Epoch 23/25\n",
      "3190/3190 [==============================] - 1s 268us/step - loss: 0.9726 - val_loss: 0.9823\n",
      "Epoch 24/25\n",
      "3190/3190 [==============================] - 1s 270us/step - loss: 0.9673 - val_loss: 0.9786\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.167973773088306e-06.\n",
      "Epoch 25/25\n",
      "3190/3190 [==============================] - 1s 270us/step - loss: 0.9726 - val_loss: 0.9792\n"
     ]
    }
   ],
   "source": [
    "eval_p38 = {}\n",
    "es2 = EarlyStopping(monitor='loss',patience=15, min_delta=0)\n",
    "rlr2 = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=2, verbose=1, min_lr=0.000000001)\n",
    "for i in range(len(train_list_p38)):\n",
    "    if i == 2:\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = class_GCN.dataframe_to_gcn_input(val_list_p38[i])\n",
    "        Y_cold = Y_val_p38\n",
    "        Y_dummy_cold = np.empty((X_atoms_cold.shape[0],gcn_best['dense_size'][2]+1))\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = class_GCN.dataframe_to_gcn_input(train_list_p38[i])\n",
    "        Y = Y_train_p38\n",
    "        Y_dummy_train = np.empty((X_atoms_train.shape[0],gcn_best['dense_size'][2]+1))\n",
    "    else:\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = class_GCN.dataframe_to_gcn_input(val_list_p38[i])\n",
    "        Y_cold = val_list_p38[i].Binary\n",
    "        Y_dummy_cold = np.empty((X_atoms_cold.shape[0],gcn_best['dense_size'][2]+1))\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = class_GCN.dataframe_to_gcn_input(train_list_p38[i])\n",
    "        Y = train_list_p38[i].Binary\n",
    "        Y_dummy_train = np.empty((X_atoms_train.shape[0],gcn_best['dense_size'][2]+1))\n",
    "    \n",
    "    gcn_encoder = class_GCN.build_encoder()\n",
    "    gcn_model = class_GCN.build_model(gcn_encoder)\n",
    "    gcn_mining = class_GCN.build_mining(gcn_model)\n",
    "    gcn_mining.fit([X_atoms_train,X_bonds_train,X_edges_train,Y],\n",
    "                   Y_dummy_train,\n",
    "                   epochs = gcn_best['n_epochs'],\n",
    "                   batch_size = gcn_best['batch_size'],\n",
    "                   shuffle = True,\n",
    "                   validation_data = ([X_atoms_cold,X_bonds_cold,X_edges_cold,Y_cold],Y_dummy_cold),\n",
    "                   callbacks=[es2,rlr2]\n",
    "                  )\n",
    "    #Predict Embeddings\n",
    "    embeddings_cold = gcn_model.predict([X_atoms_cold,X_bonds_cold,X_edges_cold])\n",
    "    embeddings_train = gcn_model.predict([X_atoms_train, X_bonds_train, X_edges_train])\n",
    "        \n",
    "    #Prepare data for XGBoost\n",
    "    dmatrix_train = class_XGB.to_xgb_input(Y,embeddings_train)\n",
    "    dmatrix_cold = class_XGB.to_xgb_input(Y_cold,embeddings_cold)\n",
    "    evalist = [(dmatrix_train,'train'),(dmatrix_cold,'eval')]\n",
    "    xgb_model = class_XGB.build_model(dmatrix_train,evalist,300)\n",
    "    xgb_pred_train = xgb_model.predict(dmatrix_train)\n",
    "    xgb_pred_cold = xgb_model.predict(dmatrix_cold)\n",
    "    \n",
    "    if i == 0:\n",
    "        eval_p38['Test'] = calculate_metrics(np.array(Y_cold),xgb_pred_cold)\n",
    "    elif i == 1:\n",
    "        eval_p38['Ave'] = calculate_metrics(np.array(Y_cold),xgb_pred_cold)\n",
    "    elif i == 2:\n",
    "        eval_p38['Random'] = calculate_metrics(np.array(Y_cold),xgb_pred_cold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>map</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.866008</td>\n",
       "      <td>293.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.726899</td>\n",
       "      <td>0.658163</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.785847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ave</th>\n",
       "      <td>0.729158</td>\n",
       "      <td>197.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.498475</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random</th>\n",
       "      <td>0.845682</td>\n",
       "      <td>280.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.784768</td>\n",
       "      <td>0.708520</td>\n",
       "      <td>0.721461</td>\n",
       "      <td>0.776596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         roc_auc     tn    fp    fn     tp       map  precision    recall  \\\n",
       "Test    0.866008  293.0  67.0  48.0  129.0  0.726899   0.658163  0.728814   \n",
       "Ave     0.729158  197.0  66.0  40.0   68.0  0.498475   0.507463  0.629630   \n",
       "Random  0.845682  280.0  65.0  61.0  158.0  0.784768   0.708520  0.721461   \n",
       "\n",
       "        accuracy  \n",
       "Test    0.785847  \n",
       "Ave     0.714286  \n",
       "Random  0.776596  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_p38 = pd.DataFrame(eval_p38).T\n",
    "eval_p38.to_csv('../../../../Desktop/binding/thesis english/Results/3-One-Shot/Online/p38.csv')\n",
    "eval_p38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1834 samples, validate on 306 samples\n",
      "Epoch 1/25\n",
      "1834/1834 [==============================] - 7s 4ms/step - loss: 0.9970 - val_loss: 0.9961\n",
      "Epoch 2/25\n",
      "1834/1834 [==============================] - 0s 270us/step - loss: 0.9963 - val_loss: 0.9888\n",
      "Epoch 3/25\n",
      "1834/1834 [==============================] - 0s 272us/step - loss: 0.9929 - val_loss: 0.9745\n",
      "Epoch 4/25\n",
      "1834/1834 [==============================] - 0s 270us/step - loss: 0.9883 - val_loss: 0.9754\n",
      "Epoch 5/25\n",
      "1834/1834 [==============================] - 1s 280us/step - loss: 0.9849 - val_loss: 0.9710\n",
      "Epoch 6/25\n",
      "1834/1834 [==============================] - 0s 271us/step - loss: 0.9811 - val_loss: 0.9772\n",
      "Epoch 7/25\n",
      "1834/1834 [==============================] - 0s 272us/step - loss: 0.9821 - val_loss: 0.9698\n",
      "Epoch 8/25\n",
      "1834/1834 [==============================] - 1s 275us/step - loss: 0.9777 - val_loss: 0.9848\n",
      "Epoch 9/25\n",
      "1834/1834 [==============================] - 1s 275us/step - loss: 0.9774 - val_loss: 0.9714\n",
      "Epoch 10/25\n",
      "1834/1834 [==============================] - 1s 273us/step - loss: 0.9777 - val_loss: 0.9644\n",
      "Epoch 11/25\n",
      "1834/1834 [==============================] - 0s 272us/step - loss: 0.9775 - val_loss: 0.9765\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 12/25\n",
      "1834/1834 [==============================] - 1s 275us/step - loss: 0.9734 - val_loss: 0.9684\n",
      "Epoch 13/25\n",
      "1834/1834 [==============================] - 1s 276us/step - loss: 0.9724 - val_loss: 0.9669\n",
      "Epoch 14/25\n",
      "1834/1834 [==============================] - 0s 269us/step - loss: 0.9720 - val_loss: 0.9800\n",
      "Epoch 15/25\n",
      "1834/1834 [==============================] - 0s 269us/step - loss: 0.9720 - val_loss: 0.9730\n",
      "Epoch 16/25\n",
      "1834/1834 [==============================] - 0s 271us/step - loss: 0.9701 - val_loss: 0.9689\n",
      "Epoch 17/25\n",
      "1834/1834 [==============================] - 0s 273us/step - loss: 0.9674 - val_loss: 0.9737\n",
      "Epoch 18/25\n",
      "1834/1834 [==============================] - 0s 262us/step - loss: 0.9677 - val_loss: 0.9717\n",
      "Epoch 19/25\n",
      "1834/1834 [==============================] - 0s 272us/step - loss: 0.9695 - val_loss: 0.9663\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 20/25\n",
      "1834/1834 [==============================] - 0s 268us/step - loss: 0.9605 - val_loss: 0.9753\n",
      "Epoch 21/25\n",
      "1834/1834 [==============================] - 1s 273us/step - loss: 0.9624 - val_loss: 0.9685\n",
      "Epoch 22/25\n",
      "1834/1834 [==============================] - 0s 272us/step - loss: 0.9647 - val_loss: 0.9664\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 23/25\n",
      "1834/1834 [==============================] - 1s 278us/step - loss: 0.9574 - val_loss: 0.9656\n",
      "Epoch 24/25\n",
      "1834/1834 [==============================] - 0s 270us/step - loss: 0.9660 - val_loss: 0.9631\n",
      "Epoch 25/25\n",
      "1834/1834 [==============================] - 1s 276us/step - loss: 0.9517 - val_loss: 0.9627\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1819 samples, validate on 321 samples\n",
      "Epoch 1/25\n",
      "1819/1819 [==============================] - 8s 5ms/step - loss: 0.9969 - val_loss: 0.9970\n",
      "Epoch 2/25\n",
      "1819/1819 [==============================] - 0s 265us/step - loss: 0.9960 - val_loss: 0.9928\n",
      "Epoch 3/25\n",
      "1819/1819 [==============================] - 0s 271us/step - loss: 0.9926 - val_loss: 0.9769\n",
      "Epoch 4/25\n",
      "1819/1819 [==============================] - 0s 272us/step - loss: 0.9878 - val_loss: 0.9694\n",
      "Epoch 5/25\n",
      "1819/1819 [==============================] - 0s 271us/step - loss: 0.9848 - val_loss: 0.9719\n",
      "Epoch 6/25\n",
      "1819/1819 [==============================] - 0s 273us/step - loss: 0.9817 - val_loss: 0.9724\n",
      "Epoch 7/25\n",
      "1819/1819 [==============================] - 0s 273us/step - loss: 0.9823 - val_loss: 0.9696\n",
      "Epoch 8/25\n",
      "1819/1819 [==============================] - 0s 274us/step - loss: 0.9788 - val_loss: 0.9610\n",
      "Epoch 9/25\n",
      "1819/1819 [==============================] - 0s 274us/step - loss: 0.9783 - val_loss: 0.9729\n",
      "Epoch 10/25\n",
      "1819/1819 [==============================] - 0s 272us/step - loss: 0.9750 - val_loss: 0.9681\n",
      "Epoch 11/25\n",
      "1819/1819 [==============================] - 0s 274us/step - loss: 0.9729 - val_loss: 0.9652\n",
      "Epoch 12/25\n",
      "1819/1819 [==============================] - 0s 274us/step - loss: 0.9712 - val_loss: 0.9630\n",
      "Epoch 13/25\n",
      "1819/1819 [==============================] - 1s 275us/step - loss: 0.9730 - val_loss: 0.9695\n",
      "Epoch 14/25\n",
      "1819/1819 [==============================] - 1s 278us/step - loss: 0.9734 - val_loss: 0.9710\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 15/25\n",
      "1819/1819 [==============================] - 1s 286us/step - loss: 0.9723 - val_loss: 0.9655\n",
      "Epoch 16/25\n",
      "1819/1819 [==============================] - 0s 274us/step - loss: 0.9656 - val_loss: 0.9493\n",
      "Epoch 17/25\n",
      "1819/1819 [==============================] - 0s 274us/step - loss: 0.9565 - val_loss: 0.9534\n",
      "Epoch 18/25\n",
      "1819/1819 [==============================] - 0s 273us/step - loss: 0.9557 - val_loss: 0.9368\n",
      "Epoch 19/25\n",
      "1819/1819 [==============================] - 0s 274us/step - loss: 0.9461 - val_loss: 0.9462\n",
      "Epoch 20/25\n",
      "1819/1819 [==============================] - 0s 273us/step - loss: 0.9669 - val_loss: 0.9654\n",
      "Epoch 21/25\n",
      "1819/1819 [==============================] - 1s 279us/step - loss: 0.9667 - val_loss: 0.9652\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 22/25\n",
      "1819/1819 [==============================] - 0s 270us/step - loss: 0.9545 - val_loss: 0.9566\n",
      "Epoch 23/25\n",
      "1819/1819 [==============================] - 1s 276us/step - loss: 0.9540 - val_loss: 0.9475\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 24/25\n",
      "1819/1819 [==============================] - 0s 275us/step - loss: 0.9541 - val_loss: 0.9575\n",
      "Epoch 25/25\n",
      "1819/1819 [==============================] - 1s 280us/step - loss: 0.9467 - val_loss: 0.9600\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 5.06875803694129e-05.\n"
     ]
    }
   ],
   "source": [
    "eval_akt1 = {}\n",
    "es2 = EarlyStopping(monitor='loss',patience=15, min_delta=0)\n",
    "rlr2 = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=2, verbose=1, min_lr=0.000000001)\n",
    "for i in range(len(train_list_akt1)):\n",
    "    if i == 1:\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = class_GCN.dataframe_to_gcn_input(val_list_akt1[i])\n",
    "        Y_cold = Y_val_akt1\n",
    "        Y_dummy_cold = np.empty((X_atoms_cold.shape[0],gcn_best['dense_size'][2]+1))\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = class_GCN.dataframe_to_gcn_input(train_list_akt1[i])\n",
    "        Y = Y_train_akt1\n",
    "        Y_dummy_train = np.empty((X_atoms_train.shape[0],gcn_best['dense_size'][2]+1))\n",
    "    else:\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = class_GCN.dataframe_to_gcn_input(val_list_akt1[i])\n",
    "        Y_cold = val_list_akt1[i].Binary\n",
    "        Y_dummy_cold = np.empty((X_atoms_cold.shape[0],gcn_best['dense_size'][2]+1))\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = class_GCN.dataframe_to_gcn_input(train_list_akt1[i])\n",
    "        Y = train_list_akt1[i].Binary\n",
    "        Y_dummy_train = np.empty((X_atoms_train.shape[0],gcn_best['dense_size'][2]+1))\n",
    "    \n",
    "    gcn_encoder = class_GCN.build_encoder()\n",
    "    gcn_model = class_GCN.build_model(gcn_encoder)\n",
    "    gcn_mining = class_GCN.build_mining(gcn_model)\n",
    "    gcn_mining.fit([X_atoms_train,X_bonds_train,X_edges_train,Y],\n",
    "                   Y_dummy_train,\n",
    "                   epochs = gcn_best['n_epochs'],\n",
    "                   batch_size = gcn_best['batch_size'],\n",
    "                   shuffle = True,\n",
    "                   validation_data = ([X_atoms_cold,X_bonds_cold,X_edges_cold,Y_cold],Y_dummy_cold),\n",
    "                   callbacks=[es2,rlr2]\n",
    "                  )\n",
    "    #Predict Embeddings\n",
    "    embeddings_cold = gcn_model.predict([X_atoms_cold,X_bonds_cold,X_edges_cold])\n",
    "    embeddings_train = gcn_model.predict([X_atoms_train, X_bonds_train, X_edges_train])\n",
    "        \n",
    "    #Prepare data for XGBoost\n",
    "    dmatrix_train = class_XGB.to_xgb_input(Y,embeddings_train)\n",
    "    dmatrix_cold = class_XGB.to_xgb_input(Y_cold,embeddings_cold)\n",
    "    evalist = [(dmatrix_train,'train'),(dmatrix_cold,'eval')]\n",
    "    xgb_model = class_XGB.build_model(dmatrix_train,evalist,300)\n",
    "    xgb_pred_train = xgb_model.predict(dmatrix_train)\n",
    "    xgb_pred_cold = xgb_model.predict(dmatrix_cold)\n",
    "    \n",
    "    if i == 0:\n",
    "        eval_akt1['Test'] = calculate_metrics(np.array(Y_cold),xgb_pred_cold)\n",
    "    elif i == 1:\n",
    "        eval_akt1['Random'] = calculate_metrics(np.array(Y_cold),xgb_pred_cold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>map</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.874115</td>\n",
       "      <td>168.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.795934</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.663793</td>\n",
       "      <td>0.800654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random</th>\n",
       "      <td>0.881458</td>\n",
       "      <td>149.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.818932</td>\n",
       "      <td>0.715278</td>\n",
       "      <td>0.786260</td>\n",
       "      <td>0.785047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         roc_auc     tn    fp    fn     tp       map  precision    recall  \\\n",
       "Test    0.874115  168.0  22.0  39.0   77.0  0.795934   0.777778  0.663793   \n",
       "Random  0.881458  149.0  41.0  28.0  103.0  0.818932   0.715278  0.786260   \n",
       "\n",
       "        accuracy  \n",
       "Test    0.800654  \n",
       "Random  0.785047  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_akt1 = pd.DataFrame(eval_akt1).T\n",
    "eval_akt1.to_csv('../../../../Desktop/binding/thesis english/Results/3-One-Shot/Online/akt1.csv')\n",
    "eval_akt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3217 samples, validate on 537 samples\n",
      "Epoch 1/25\n",
      "3217/3217 [==============================] - 10s 3ms/step - loss: 0.9968 - val_loss: 0.9981\n",
      "Epoch 2/25\n",
      "3217/3217 [==============================] - 1s 276us/step - loss: 0.9961 - val_loss: 0.9949\n",
      "Epoch 3/25\n",
      "3217/3217 [==============================] - 1s 275us/step - loss: 0.9930 - val_loss: 0.9845\n",
      "Epoch 4/25\n",
      "3217/3217 [==============================] - 1s 274us/step - loss: 0.9881 - val_loss: 0.9798\n",
      "Epoch 5/25\n",
      "3217/3217 [==============================] - 1s 274us/step - loss: 0.9839 - val_loss: 0.9815\n",
      "Epoch 6/25\n",
      "3217/3217 [==============================] - 1s 279us/step - loss: 0.9827 - val_loss: 0.9973\n",
      "Epoch 7/25\n",
      "3217/3217 [==============================] - 1s 274us/step - loss: 0.9793 - val_loss: 0.9778\n",
      "Epoch 8/25\n",
      "3217/3217 [==============================] - 1s 272us/step - loss: 0.9756 - val_loss: 0.9742\n",
      "Epoch 9/25\n",
      "3217/3217 [==============================] - 1s 269us/step - loss: 0.9762 - val_loss: 0.9741\n",
      "Epoch 10/25\n",
      "3217/3217 [==============================] - 1s 279us/step - loss: 0.9827 - val_loss: 0.9802\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 11/25\n",
      "3217/3217 [==============================] - 1s 290us/step - loss: 0.9800 - val_loss: 0.9815\n",
      "Epoch 12/25\n",
      "3217/3217 [==============================] - 1s 280us/step - loss: 0.9688 - val_loss: 0.9782\n",
      "Epoch 13/25\n",
      "3217/3217 [==============================] - 1s 272us/step - loss: 0.9718 - val_loss: 0.9679\n",
      "Epoch 14/25\n",
      "3217/3217 [==============================] - 1s 277us/step - loss: 0.9683 - val_loss: 0.9792\n",
      "Epoch 15/25\n",
      "3217/3217 [==============================] - 1s 278us/step - loss: 0.9612 - val_loss: 0.9766\n",
      "Epoch 16/25\n",
      "3217/3217 [==============================] - 1s 276us/step - loss: 0.9761 - val_loss: 0.9809\n",
      "Epoch 17/25\n",
      "3217/3217 [==============================] - 1s 277us/step - loss: 0.9573 - val_loss: 0.9656\n",
      "Epoch 18/25\n",
      "3217/3217 [==============================] - 1s 281us/step - loss: 0.9647 - val_loss: 0.9799\n",
      "Epoch 19/25\n",
      "3217/3217 [==============================] - 1s 273us/step - loss: 0.9767 - val_loss: 0.9806\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 20/25\n",
      "3217/3217 [==============================] - 1s 273us/step - loss: 0.9692 - val_loss: 0.9797\n",
      "Epoch 21/25\n",
      "3217/3217 [==============================] - 1s 274us/step - loss: 0.9616 - val_loss: 0.9803\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 22/25\n",
      "3217/3217 [==============================] - 1s 272us/step - loss: 0.9700 - val_loss: 0.9808\n",
      "Epoch 23/25\n",
      "3217/3217 [==============================] - 1s 274us/step - loss: 0.9612 - val_loss: 0.9749\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 5.06875803694129e-05.\n",
      "Epoch 24/25\n",
      "3217/3217 [==============================] - 1s 281us/step - loss: 0.9436 - val_loss: 0.9761\n",
      "Epoch 25/25\n",
      "3217/3217 [==============================] - 1s 277us/step - loss: 0.9484 - val_loss: 0.9689\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3190 samples, validate on 564 samples\n",
      "Epoch 1/25\n",
      "3190/3190 [==============================] - 11s 3ms/step - loss: 0.9968 - val_loss: 0.9976\n",
      "Epoch 2/25\n",
      "3190/3190 [==============================] - 1s 287us/step - loss: 0.9962 - val_loss: 0.9936\n",
      "Epoch 3/25\n",
      "3190/3190 [==============================] - 1s 284us/step - loss: 0.9946 - val_loss: 0.9921\n",
      "Epoch 4/25\n",
      "3190/3190 [==============================] - 1s 284us/step - loss: 0.9916 - val_loss: 0.9894\n",
      "Epoch 5/25\n",
      "3190/3190 [==============================] - 1s 287us/step - loss: 0.9880 - val_loss: 0.9982\n",
      "Epoch 6/25\n",
      "3190/3190 [==============================] - 1s 284us/step - loss: 0.9878 - val_loss: 0.9800\n",
      "Epoch 7/25\n",
      "3190/3190 [==============================] - 1s 285us/step - loss: 0.9827 - val_loss: 0.9762\n",
      "Epoch 8/25\n",
      "3190/3190 [==============================] - 1s 285us/step - loss: 0.9820 - val_loss: 0.9896\n",
      "Epoch 9/25\n",
      "3190/3190 [==============================] - 1s 281us/step - loss: 0.9815 - val_loss: 0.9833\n",
      "Epoch 10/25\n",
      "3190/3190 [==============================] - 1s 288us/step - loss: 0.9826 - val_loss: 0.9833\n",
      "Epoch 11/25\n",
      "3190/3190 [==============================] - 1s 287us/step - loss: 0.9841 - val_loss: 0.9818\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 12/25\n",
      "3190/3190 [==============================] - 1s 303us/step - loss: 0.9736 - val_loss: 0.9828\n",
      "Epoch 13/25\n",
      "3190/3190 [==============================] - 1s 284us/step - loss: 0.9799 - val_loss: 0.9804\n",
      "Epoch 14/25\n",
      "3190/3190 [==============================] - 1s 286us/step - loss: 0.9795 - val_loss: 0.9798\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 15/25\n",
      "3190/3190 [==============================] - 1s 285us/step - loss: 0.9623 - val_loss: 0.9785\n",
      "Epoch 16/25\n",
      "3190/3190 [==============================] - 1s 287us/step - loss: 0.9779 - val_loss: 0.9817\n",
      "Epoch 17/25\n",
      "3190/3190 [==============================] - 1s 291us/step - loss: 0.9716 - val_loss: 0.9777\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 18/25\n",
      "3190/3190 [==============================] - 1s 291us/step - loss: 0.9758 - val_loss: 0.9815\n",
      "Epoch 19/25\n",
      "3190/3190 [==============================] - 1s 291us/step - loss: 0.9695 - val_loss: 0.9801\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 5.06875803694129e-05.\n",
      "Epoch 20/25\n",
      "3190/3190 [==============================] - 1s 291us/step - loss: 0.9607 - val_loss: 0.9805\n",
      "Epoch 21/25\n",
      "3190/3190 [==============================] - 1s 288us/step - loss: 0.9632 - val_loss: 0.9789\n",
      "Epoch 22/25\n",
      "3190/3190 [==============================] - 1s 285us/step - loss: 0.9600 - val_loss: 0.9795\n",
      "Epoch 23/25\n",
      "3190/3190 [==============================] - 1s 288us/step - loss: 0.9651 - val_loss: 0.9792\n",
      "Epoch 24/25\n",
      "3190/3190 [==============================] - 1s 285us/step - loss: 0.9640 - val_loss: 0.9778\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 2.534379018470645e-05.\n",
      "Epoch 25/25\n",
      "3190/3190 [==============================] - 1s 286us/step - loss: 0.9659 - val_loss: 0.9782\n"
     ]
    }
   ],
   "source": [
    "eval_pi3k = {}\n",
    "es2 = EarlyStopping(monitor='loss',patience=15, min_delta=0)\n",
    "rlr2 = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=2, verbose=1, min_lr=0.000000001)\n",
    "for i in range(len(train_list_pi3k)):\n",
    "    if i == 1:\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = class_GCN.dataframe_to_gcn_input(val_list_pi3k[i])\n",
    "        Y_cold = Y_val_pi3k\n",
    "        Y_dummy_cold = np.empty((X_atoms_cold.shape[0],gcn_best['dense_size'][2]+1))\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = class_GCN.dataframe_to_gcn_input(train_list_pi3k[i])\n",
    "        Y = Y_train_pi3k\n",
    "        Y_dummy_train = np.empty((X_atoms_train.shape[0],gcn_best['dense_size'][2]+1))\n",
    "    else:\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = class_GCN.dataframe_to_gcn_input(val_list_pi3k[i])\n",
    "        Y_cold = val_list_pi3k[i].Binary\n",
    "        Y_dummy_cold = np.empty((X_atoms_cold.shape[0],gcn_best['dense_size'][2]+1))\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = class_GCN.dataframe_to_gcn_input(train_list_pi3k[i])\n",
    "        Y = train_list_pi3k[i].Binary\n",
    "        Y_dummy_train = np.empty((X_atoms_train.shape[0],gcn_best['dense_size'][2]+1))\n",
    "    \n",
    "    gcn_encoder = class_GCN.build_encoder()\n",
    "    gcn_model = class_GCN.build_model(gcn_encoder)\n",
    "    gcn_mining = class_GCN.build_mining(gcn_model)\n",
    "    gcn_mining.fit([X_atoms_train,X_bonds_train,X_edges_train,Y],\n",
    "                   Y_dummy_train,\n",
    "                   epochs = gcn_best['n_epochs'],\n",
    "                   batch_size = gcn_best['batch_size'],\n",
    "                   shuffle = True,\n",
    "                   validation_data = ([X_atoms_cold,X_bonds_cold,X_edges_cold,Y_cold],Y_dummy_cold),\n",
    "                   callbacks=[es2,rlr2]\n",
    "                  )\n",
    "    #Predict Embeddings\n",
    "    embeddings_cold = gcn_model.predict([X_atoms_cold,X_bonds_cold,X_edges_cold])\n",
    "    embeddings_train = gcn_model.predict([X_atoms_train, X_bonds_train, X_edges_train])\n",
    "        \n",
    "    #Prepare data for XGBoost\n",
    "    dmatrix_train = class_XGB.to_xgb_input(Y,embeddings_train)\n",
    "    dmatrix_cold = class_XGB.to_xgb_input(Y_cold,embeddings_cold)\n",
    "    evalist = [(dmatrix_train,'train'),(dmatrix_cold,'eval')]\n",
    "    xgb_model = class_XGB.build_model(dmatrix_train,evalist,300)\n",
    "    xgb_pred_train = xgb_model.predict(dmatrix_train)\n",
    "    xgb_pred_cold = xgb_model.predict(dmatrix_cold)\n",
    "    \n",
    "    if i == 0:\n",
    "        eval_pi3k['Test'] = calculate_metrics(np.array(Y_cold),xgb_pred_cold)\n",
    "    elif i == 1:\n",
    "        eval_pi3k['Random'] = calculate_metrics(np.array(Y_cold),xgb_pred_cold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>map</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.872332</td>\n",
       "      <td>310.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.732184</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.655367</td>\n",
       "      <td>0.793296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random</th>\n",
       "      <td>0.876480</td>\n",
       "      <td>291.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.840004</td>\n",
       "      <td>0.792079</td>\n",
       "      <td>0.692641</td>\n",
       "      <td>0.799645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         roc_auc     tn    fp    fn     tp       map  precision    recall  \\\n",
       "Test    0.872332  310.0  50.0  61.0  116.0  0.732184   0.698795  0.655367   \n",
       "Random  0.876480  291.0  42.0  71.0  160.0  0.840004   0.792079  0.692641   \n",
       "\n",
       "        accuracy  \n",
       "Test    0.793296  \n",
       "Random  0.799645  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_pi3k = pd.DataFrame(eval_pi3k).T\n",
    "eval_pi3k.to_csv('../../../../Desktop/binding/thesis english/Results/3-One-Shot/Online/pi3k.csv')\n",
    "eval_pi3k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
