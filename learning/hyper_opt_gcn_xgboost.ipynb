{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.callbacks import History, ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from data_analysis import calculate_metrics\n",
    "from functools import partial\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pickle\n",
    "import dill\n",
    "from hyper_mining import objective_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fspace = {\n",
    "    'conv1' : hp.quniform('conv1', 32, 64, 8),\n",
    "    'conv2' : hp.quniform('conv2', 64, 128, 8),\n",
    "    'conv3' : hp.quniform('conv3', 128, 168, 8),\n",
    "    'fp' : hp.quniform('fp', 96, 196, 8),\n",
    "    'dense1' : hp.quniform('dense1',96,512,32),\n",
    "    'dense2' : hp.quniform('dense2',96,512,32),\n",
    "    'dense3' : hp.quniform('dense3',64,512,32),\n",
    "    'dropout_rate' : hp.uniform('dropout_rate',0.1,0.5),\n",
    "    'lr' : hp.uniform('lr',0.000001,0.01),\n",
    "    'n_epochs' : hp.quniform('n_epochs',15,60,5),\n",
    "    'batch_size' : hp.quniform('batch_size',64,256,16),\n",
    "    'colsample_bylevel' : hp.uniform('colsample_bylevel', 0.1, 1), \n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0.1, 1), \n",
    "    'gamma' : hp.uniform('gamma', 0.1, 1), \n",
    "    'learning_rate' : hp.uniform('learning_rate', 0.1, 1),\n",
    "    'max_delta_step' : hp.quniform('max_delta_step',1,10,1),\n",
    "    'max_depth' : hp.quniform('max_depth',6, 12, 1),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight',10 ,500 ,5),\n",
    "    'reg_alpha' : hp.uniform('reg_alpha',0.1,100),\n",
    "    'reg_lambda' : hp.uniform('reg_lambda',0.1,100),\n",
    "    'subsample' : hp.uniform('subsample',0.1,1.0),\n",
    "    'max_bin' : hp.quniform('max_bin',16,256,16)\n",
    "    #'margin' : hp.uniform('margin',0.2,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'p38'\n",
    "base_path = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath = base_path+f'/data/{target}/data.csv'\n",
    "df=pd.read_csv(data_fpath).set_index('biolab_index')\n",
    "\n",
    "with open(base_path+f'/data/{target}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds = dill.load(in_f)\n",
    "with open(base_path+f'/data/{target}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds = dill.load(in_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[train_test_folds[0]]\n",
    "val = df.loc[train_test_folds[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rdkit</th>\n",
       "      <th>standard_value</th>\n",
       "      <th>Binary</th>\n",
       "      <th>assay_type</th>\n",
       "      <th>not_sure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6694856396823213521</th>\n",
       "      <td>CSc1nc(-c2ccc(F)cc2)c(-c2ccnc(NC(=O)c3ccccc3)c...</td>\n",
       "      <td>6.472370</td>\n",
       "      <td>0</td>\n",
       "      <td>binding</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-2189404736969118545</th>\n",
       "      <td>ClC1=C(Cl)[C@]2(Cl)[C@@H]3[C@@H]4C[C@H]([C@@H]...</td>\n",
       "      <td>4.364587</td>\n",
       "      <td>0</td>\n",
       "      <td>binding</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-4240361026298800667</th>\n",
       "      <td>CCn1c(SC)nc(-c2ccc(F)cc2)c1-c1ccnc(NC(=O)/C=C/...</td>\n",
       "      <td>7.420216</td>\n",
       "      <td>1</td>\n",
       "      <td>binding</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704799582349985732</th>\n",
       "      <td>C[N+](C)(C)CCO.Cn1c(-c2cn(-c3cccc(C(=O)[O-])c3...</td>\n",
       "      <td>7.040959</td>\n",
       "      <td>1</td>\n",
       "      <td>binding</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624810325886590497</th>\n",
       "      <td>Cn1c(-c2cn(-c3cccc(C(=O)NCCCO)c3)nn2)nc(-c2ccc...</td>\n",
       "      <td>6.537602</td>\n",
       "      <td>0</td>\n",
       "      <td>binding</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8379113105631449385</th>\n",
       "      <td>COc1ccccc1-c1c(N)c(C(=O)c2ccc(F)cc2F)cc[n+]1[O-]</td>\n",
       "      <td>7.638272</td>\n",
       "      <td>1</td>\n",
       "      <td>binding</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-4924377680869854252</th>\n",
       "      <td>Cn1nc(C(C)(C)C)cc1NC(=O)Nc1ccc(Cc2ccc(NC(=O)OC...</td>\n",
       "      <td>7.060481</td>\n",
       "      <td>1</td>\n",
       "      <td>binding</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7108923260425567357</th>\n",
       "      <td>Cn1c(=O)c(Cc2ccc(F)cc2F)cc2cnnc(-c3ccc(F)cc3F)c21</td>\n",
       "      <td>8.214670</td>\n",
       "      <td>1</td>\n",
       "      <td>binding</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347631336707966323</th>\n",
       "      <td>OCCNc1cc(-c2[nH]c3cccnc3c2-c2ccc(F)cc2)ccn1</td>\n",
       "      <td>8.522879</td>\n",
       "      <td>1</td>\n",
       "      <td>binding</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5662305738265717508</th>\n",
       "      <td>Cc1cc(F)ccc1Oc1ccc2c(-c3ccccc3C)nncc2n1</td>\n",
       "      <td>6.774691</td>\n",
       "      <td>0</td>\n",
       "      <td>binding</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  rdkit  \\\n",
       " 6694856396823213521  CSc1nc(-c2ccc(F)cc2)c(-c2ccnc(NC(=O)c3ccccc3)c...   \n",
       "-2189404736969118545  ClC1=C(Cl)[C@]2(Cl)[C@@H]3[C@@H]4C[C@H]([C@@H]...   \n",
       "-4240361026298800667  CCn1c(SC)nc(-c2ccc(F)cc2)c1-c1ccnc(NC(=O)/C=C/...   \n",
       " 1704799582349985732  C[N+](C)(C)CCO.Cn1c(-c2cn(-c3cccc(C(=O)[O-])c3...   \n",
       " 1624810325886590497  Cn1c(-c2cn(-c3cccc(C(=O)NCCCO)c3)nn2)nc(-c2ccc...   \n",
       "...                                                                 ...   \n",
       " 8379113105631449385   COc1ccccc1-c1c(N)c(C(=O)c2ccc(F)cc2F)cc[n+]1[O-]   \n",
       "-4924377680869854252  Cn1nc(C(C)(C)C)cc1NC(=O)Nc1ccc(Cc2ccc(NC(=O)OC...   \n",
       " 7108923260425567357  Cn1c(=O)c(Cc2ccc(F)cc2F)cc2cnnc(-c3ccc(F)cc3F)c21   \n",
       " 347631336707966323         OCCNc1cc(-c2[nH]c3cccnc3c2-c2ccc(F)cc2)ccn1   \n",
       " 5662305738265717508            Cc1cc(F)ccc1Oc1ccc2c(-c3ccccc3C)nncc2n1   \n",
       "\n",
       "                      standard_value  Binary assay_type  not_sure  \n",
       " 6694856396823213521        6.472370       0    binding         1  \n",
       "-2189404736969118545        4.364587       0    binding         0  \n",
       "-4240361026298800667        7.420216       1    binding         1  \n",
       " 1704799582349985732        7.040959       1    binding         1  \n",
       " 1624810325886590497        6.537602       0    binding         1  \n",
       "...                              ...     ...        ...       ...  \n",
       " 8379113105631449385        7.638272       1    binding         0  \n",
       "-4924377680869854252        7.060481       1    binding         1  \n",
       " 7108923260425567357        8.214670       1    binding         0  \n",
       " 347631336707966323         8.522879       1    binding         0  \n",
       " 5662305738265717508        6.774691       0    binding         1  \n",
       "\n",
       "[509 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list = [df.loc[train_val_folds[0][0]],\n",
    "                 df.loc[train_val_folds[1][0]],\n",
    "                 df.loc[train_val_folds[2][0]],\n",
    "                 df.loc[train_val_folds[3][0]],\n",
    "                 df.loc[train_val_folds[4][0]],\n",
    "                 df.loc[train_val_folds[5][0]],\n",
    "                 ]\n",
    "validation_list = [df.loc[train_val_folds[0][1]],\n",
    "                   df.loc[train_val_folds[1][1]],\n",
    "                   df.loc[train_val_folds[2][1]],\n",
    "                   df.loc[train_val_folds[3][1]],\n",
    "                   df.loc[train_val_folds[4][1]],\n",
    "                   df.loc[train_val_folds[5][1]],\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin_objective = partial(objective_fn, train_sets = training_list, val_sets = validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials():\n",
    "\n",
    "    trials_step = 0  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    max_trials = 1  # initial max_trials. put something small to not have to wait\n",
    "\n",
    "    \n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(\"gcn_xgb.hyperopt\", \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    best = fmin(fn = fmin_objective, space = fspace, algo=tpe.suggest, max_evals=max_trials, trials=trials)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    \n",
    "    # save the trials object\n",
    "    with open(\"gcn_xgb.hyperopt\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "    return(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved Trials! Loading...\n",
      "Rerunning from 154 trials to 154 (+0) trials\n",
      "100%|████████████████████████████████████████████████████████████████████████| 154/154 [00:00<?, ?trial/s, best loss=?]\n",
      "Best: {'batch_size': 288.0, 'colsample_bylevel': 0.4371082812232264, 'colsample_bytree': 0.4179415558635843, 'conv1': 56.0, 'conv2': 88.0, 'conv3': 136.0, 'dense1': 384.0, 'dense2': 288.0, 'dense3': 224.0, 'dropout_rate': 0.27225175676555935, 'fp': 152.0, 'gamma': 0.919836526180396, 'learning_rate': 0.41409388868400826, 'lr': 0.0008110012706176706, 'max_bin': 48.0, 'max_delta_step': 2.0, 'max_depth': 7.0, 'min_child_weight': 20.0, 'n_epochs': 25.0, 'reg_alpha': 42.8887552483495, 'reg_lambda': 12.306130216692438, 'subsample': 0.6038298323514097}\n"
     ]
    }
   ],
   "source": [
    "trials = run_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = trials.trials[0]['result']['loss']\n",
    "for i in range(1,len(trials.trials)):\n",
    "    if (trials.trials[i]['result']['loss'] <=  best_loss):\n",
    "        best_loss = trials.trials[i]['result']['loss']\n",
    "        index = i\n",
    "best_params = trials.trials[index]['misc']['vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': [288.0],\n",
       " 'colsample_bylevel': [0.4371082812232264],\n",
       " 'colsample_bytree': [0.4179415558635843],\n",
       " 'conv1': [56.0],\n",
       " 'conv2': [88.0],\n",
       " 'conv3': [136.0],\n",
       " 'dense1': [384.0],\n",
       " 'dense2': [288.0],\n",
       " 'dense3': [224.0],\n",
       " 'dropout_rate': [0.27225175676555935],\n",
       " 'fp': [152.0],\n",
       " 'gamma': [0.919836526180396],\n",
       " 'learning_rate': [0.41409388868400826],\n",
       " 'lr': [0.0008110012706176706],\n",
       " 'max_bin': [48.0],\n",
       " 'max_delta_step': [2.0],\n",
       " 'max_depth': [7.0],\n",
       " 'min_child_weight': [20.0],\n",
       " 'n_epochs': [25.0],\n",
       " 'reg_alpha': [42.8887552483495],\n",
       " 'reg_lambda': [12.306130216692438],\n",
       " 'subsample': [0.6038298323514097]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyper_mining import XGB_predictor,GCN_online_mining_test\n",
    "from data_analysis import calculate_metrics\n",
    "es = EarlyStopping(monitor='loss',patience=8, min_delta=0)\n",
    "rlr = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=4, verbose=1, min_lr=0.0000001)\n",
    "gcn_best = {\n",
    "        \"num_layers\" : 3,\n",
    "        \"max_atoms\" : 70,\n",
    "        \"num_atom_features\" : 62,\n",
    "        \"num_atom_features_original\" : 62,\n",
    "        \"num_bond_features\" : 6,\n",
    "        \"max_degree\" : 5,\n",
    "        \"conv_width\" : [int(best_params['conv1'][0]), int(best_params['conv2'][0]), int(best_params['conv3'][0])],\n",
    "        \"fp_length\" : [int(best_params['fp'][0]), int(best_params['fp'][0]), int(best_params['fp'][0])],\n",
    "        \"activ_enc\" : \"selu\",\n",
    "        \"activ_dec\" : \"selu\",\n",
    "        \"learning_rates\" : [0.001,0.001,0.001],\n",
    "        \"learning_rates_fp\": [0.005,0.005,0.005],\n",
    "        \"losses_conv\" : {\n",
    "                    \"neighbor_output\": \"mean_squared_error\",\n",
    "                    \"self_output\": \"mean_squared_error\",\n",
    "                    },\n",
    "        \"lossWeights\" : {\"neighbor_output\": 1.0, \"self_output\": 1.0},\n",
    "        \"metrics\" : \"mse\",\n",
    "        \"loss_fp\" : \"mean_squared_error\",\n",
    "        \"enc_layer_names\" : [\"enc_1\", \"enc_2\", \"enc_3\"],\n",
    "        'callbacks' : [es,rlr],\n",
    "        'adam_decay': 0.0005329142291371636,\n",
    "        'beta': 5,\n",
    "        'p': 0.004465204118126482,\n",
    "        'dense_size' : [int(best_params['dense1'][0]), int(best_params['dense2'][0]), int(best_params['dense3'][0])],\n",
    "        'dropout_rate' : [best_params['dropout_rate'][0], best_params['dropout_rate'][0]],\n",
    "        'lr' : best_params['lr'][0],\n",
    "        'batch_size' : int(best_params['batch_size'][0]),\n",
    "        'n_epochs' : int(best_params['n_epochs'][0])\n",
    "        #'margin' : best_params['margin'][0]\n",
    "        }\n",
    "xgb_best = {\n",
    "        \"colsample_bylevel\" : best_params['colsample_bylevel'][0],\n",
    "        \"colsample_bytree\" : best_params['colsample_bytree'][0],\n",
    "        \"gamma\" : best_params['gamma'][0],\n",
    "        \"eta\" : best_params['learning_rate'][0],\n",
    "        \"max_delta_step\" : int(best_params['max_delta_step'][0]),\n",
    "        \"max_depth\" : int(best_params['max_depth'][0]),\n",
    "        \"min_child_weight\" : int(best_params['min_child_weight'][0]),\n",
    "        \"alpha\" : best_params['reg_alpha'][0],\n",
    "        \"lambda\" : best_params['reg_lambda'][0],\n",
    "        \"subsample\" : best_params['subsample'][0],\n",
    "        \"max_bin\" : int(best_params['max_bin'][0]),\n",
    "        \"eval_metric\":'auc',\n",
    "        \"objective\":'binary:logistic',\n",
    "        \"booster\":'gbtree'\n",
    "        #\"single_precision_histogram\" : True\n",
    "        }\n",
    "class_XGB = XGB_predictor(xgb_best)\n",
    "class_GCN = GCN_online_mining_test(gcn_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyper = {\n",
    "        \"colsample_bylevel\" : 0.5612301667238877,\n",
    "        \"colsample_bytree\" : 0.788688363076523,\n",
    "        \"gamma\" : 0.35376030016117566,\n",
    "        \"eta\" : 0.4023692255888918,\n",
    "        \"max_delta_step\" : int(3),\n",
    "        \"max_depth\" : int(8),\n",
    "        \"min_child_weight\" : int(70),\n",
    "        \"alpha\" : 0.15030685758880047,\n",
    "        \"lambda\" : 15.311721955443915,\n",
    "        \"subsample\" : 0.8303923929525608,\n",
    "        \"max_bin\" : int(208),\n",
    "        \"eval_metric\":'auc',\n",
    "        \"objective\":'binary:logistic',\n",
    "        \"booster\":'gbtree',\n",
    "        \"single_precision_histogram\" : True\n",
    "}\n",
    "class_XGB_2 = XGB_predictor(xgb_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/25\n",
      "2541/2541 [==============================] - 7s 3ms/step - loss: 0.9973 - val_loss: 0.9967\n",
      "Epoch 2/25\n",
      "2541/2541 [==============================] - 1s 265us/step - loss: 0.9967 - val_loss: 0.9881\n",
      "Epoch 3/25\n",
      "2541/2541 [==============================] - 1s 265us/step - loss: 0.9941 - val_loss: 0.9804\n",
      "Epoch 4/25\n",
      "2541/2541 [==============================] - 1s 266us/step - loss: 0.9897 - val_loss: 0.9772\n",
      "Epoch 5/25\n",
      "2541/2541 [==============================] - 1s 268us/step - loss: 0.9861 - val_loss: 0.9742\n",
      "Epoch 6/25\n",
      "2541/2541 [==============================] - 1s 264us/step - loss: 0.9849 - val_loss: 0.9741\n",
      "Epoch 7/25\n",
      "2541/2541 [==============================] - 1s 264us/step - loss: 0.9815 - val_loss: 0.9704\n",
      "Epoch 8/25\n",
      "2541/2541 [==============================] - 1s 262us/step - loss: 0.9760 - val_loss: 0.9727\n",
      "Epoch 9/25\n",
      "2541/2541 [==============================] - 1s 262us/step - loss: 0.9767 - val_loss: 0.9698\n",
      "Epoch 10/25\n",
      "2541/2541 [==============================] - 1s 263us/step - loss: 0.9750 - val_loss: 0.9629\n",
      "Epoch 11/25\n",
      "2541/2541 [==============================] - 1s 264us/step - loss: 0.9756 - val_loss: 0.9778\n",
      "Epoch 12/25\n",
      "2541/2541 [==============================] - 1s 262us/step - loss: 0.9700 - val_loss: 0.9747\n",
      "Epoch 13/25\n",
      "2541/2541 [==============================] - 1s 263us/step - loss: 0.9711 - val_loss: 0.9683\n",
      "Epoch 14/25\n",
      "2541/2541 [==============================] - 1s 264us/step - loss: 0.9705 - val_loss: 0.9758\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 15/25\n",
      "2541/2541 [==============================] - 1s 264us/step - loss: 0.9677 - val_loss: 0.9721\n",
      "Epoch 16/25\n",
      "2541/2541 [==============================] - 1s 265us/step - loss: 0.9519 - val_loss: 0.9715\n",
      "Epoch 17/25\n",
      "2541/2541 [==============================] - 1s 262us/step - loss: 0.9625 - val_loss: 0.9705\n",
      "Epoch 18/25\n",
      "2541/2541 [==============================] - 1s 263us/step - loss: 0.9449 - val_loss: 0.9747\n",
      "Epoch 19/25\n",
      "2541/2541 [==============================] - 1s 264us/step - loss: 0.9574 - val_loss: 0.9486\n",
      "Epoch 20/25\n",
      "2541/2541 [==============================] - 1s 262us/step - loss: 0.9460 - val_loss: 0.9675\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 21/25\n",
      "2541/2541 [==============================] - 1s 263us/step - loss: 0.9517 - val_loss: 0.9740\n",
      "Epoch 22/25\n",
      "2541/2541 [==============================] - 1s 265us/step - loss: 0.9562 - val_loss: 0.9697\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 23/25\n",
      "2541/2541 [==============================] - 1s 262us/step - loss: 0.9267 - val_loss: 0.9640\n",
      "Epoch 24/25\n",
      "2541/2541 [==============================] - 1s 264us/step - loss: 0.9175 - val_loss: 0.9730\n",
      "Epoch 25/25\n",
      "2541/2541 [==============================] - 1s 263us/step - loss: 0.9549 - val_loss: 0.9736\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/25\n",
      "2541/2541 [==============================] - 6s 2ms/step - loss: 0.9974 - val_loss: 0.9980\n",
      "Epoch 2/25\n",
      "2541/2541 [==============================] - 1s 266us/step - loss: 0.9966 - val_loss: 0.9970\n",
      "Epoch 3/25\n",
      "2541/2541 [==============================] - 1s 265us/step - loss: 0.9954 - val_loss: 0.9871\n",
      "Epoch 4/25\n",
      "2541/2541 [==============================] - 1s 265us/step - loss: 0.9910 - val_loss: 0.9790\n",
      "Epoch 5/25\n",
      "2541/2541 [==============================] - 1s 266us/step - loss: 0.9873 - val_loss: 0.9822\n",
      "Epoch 6/25\n",
      "2541/2541 [==============================] - 1s 265us/step - loss: 0.9851 - val_loss: 0.9749\n",
      "Epoch 7/25\n",
      "2541/2541 [==============================] - ETA: 0s - loss: 0.983 - 1s 264us/step - loss: 0.9822 - val_loss: 0.9753\n",
      "Epoch 8/25\n",
      "2541/2541 [==============================] - 1s 264us/step - loss: 0.9806 - val_loss: 0.9809\n",
      "Epoch 9/25\n",
      "2541/2541 [==============================] - 1s 265us/step - loss: 0.9787 - val_loss: 0.9775\n",
      "Epoch 10/25\n",
      "2541/2541 [==============================] - 1s 266us/step - loss: 0.9783 - val_loss: 0.9722\n",
      "Epoch 11/25\n",
      "2541/2541 [==============================] - 1s 267us/step - loss: 0.9764 - val_loss: 1.0050\n",
      "Epoch 12/25\n",
      "2541/2541 [==============================] - 1s 266us/step - loss: 0.9753 - val_loss: 0.9659\n",
      "Epoch 13/25\n",
      "2541/2541 [==============================] - 1s 266us/step - loss: 0.9764 - val_loss: 0.9575\n",
      "Epoch 14/25\n",
      "2541/2541 [==============================] - 1s 265us/step - loss: 0.9717 - val_loss: 0.9791\n",
      "Epoch 15/25\n",
      "2541/2541 [==============================] - 1s 265us/step - loss: 0.9758 - val_loss: 0.9783\n",
      "Epoch 16/25\n",
      "2541/2541 [==============================] - 1s 264us/step - loss: 0.9740 - val_loss: 0.9753\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 17/25\n",
      "2541/2541 [==============================] - 1s 267us/step - loss: 0.9688 - val_loss: 0.9776\n",
      "Epoch 18/25\n",
      "2541/2541 [==============================] - 1s 265us/step - loss: 0.9676 - val_loss: 0.9734\n",
      "Epoch 19/25\n",
      "2541/2541 [==============================] - 1s 264us/step - loss: 0.9672 - val_loss: 0.9632\n",
      "Epoch 20/25\n",
      "2541/2541 [==============================] - 1s 266us/step - loss: 0.9561 - val_loss: 0.9695\n",
      "Epoch 21/25\n",
      "2541/2541 [==============================] - 1s 266us/step - loss: 0.9594 - val_loss: 0.9582\n",
      "Epoch 22/25\n",
      "2541/2541 [==============================] - 1s 265us/step - loss: 0.9521 - val_loss: 0.9658\n",
      "Epoch 23/25\n",
      "2541/2541 [==============================] - 1s 267us/step - loss: 0.9444 - val_loss: 0.9728\n",
      "Epoch 24/25\n",
      "2541/2541 [==============================] - 1s 264us/step - loss: 0.9547 - val_loss: 0.9693\n",
      "Epoch 25/25\n",
      "2541/2541 [==============================] - 1s 265us/step - loss: 0.9464 - val_loss: 0.9072\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/25\n",
      "2541/2541 [==============================] - 7s 3ms/step - loss: 0.9971 - val_loss: 0.9978\n",
      "Epoch 2/25\n",
      "2541/2541 [==============================] - 1s 270us/step - loss: 0.9962 - val_loss: 0.9945\n",
      "Epoch 3/25\n",
      "2541/2541 [==============================] - 1s 274us/step - loss: 0.9939 - val_loss: 0.9856\n",
      "Epoch 4/25\n",
      "2541/2541 [==============================] - 1s 273us/step - loss: 0.9883 - val_loss: 0.9809\n",
      "Epoch 5/25\n",
      "2541/2541 [==============================] - 1s 276us/step - loss: 0.9856 - val_loss: 0.9816\n",
      "Epoch 6/25\n",
      "2541/2541 [==============================] - 1s 275us/step - loss: 0.9830 - val_loss: 0.9795\n",
      "Epoch 7/25\n",
      "2541/2541 [==============================] - 1s 275us/step - loss: 0.9800 - val_loss: 0.9807\n",
      "Epoch 8/25\n",
      "2541/2541 [==============================] - 1s 273us/step - loss: 0.9782 - val_loss: 0.9786\n",
      "Epoch 9/25\n",
      "2541/2541 [==============================] - 1s 277us/step - loss: 0.9755 - val_loss: 0.9800\n",
      "Epoch 10/25\n",
      "2541/2541 [==============================] - 1s 274us/step - loss: 0.9755 - val_loss: 0.9794\n",
      "Epoch 11/25\n",
      "2541/2541 [==============================] - 1s 274us/step - loss: 0.9739 - val_loss: 0.9782\n",
      "Epoch 12/25\n",
      "2541/2541 [==============================] - 1s 273us/step - loss: 0.9712 - val_loss: 0.9822\n",
      "Epoch 13/25\n",
      "2541/2541 [==============================] - 1s 274us/step - loss: 0.9697 - val_loss: 0.9764\n",
      "Epoch 14/25\n",
      "2541/2541 [==============================] - 1s 273us/step - loss: 0.9719 - val_loss: 0.9662\n",
      "Epoch 15/25\n",
      "2541/2541 [==============================] - 1s 274us/step - loss: 0.9651 - val_loss: 0.9756\n",
      "Epoch 16/25\n",
      "2541/2541 [==============================] - 1s 277us/step - loss: 0.9674 - val_loss: 0.9839\n",
      "Epoch 17/25\n",
      "2541/2541 [==============================] - 1s 299us/step - loss: 0.9803 - val_loss: 0.9756\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 18/25\n",
      "2541/2541 [==============================] - 1s 265us/step - loss: 0.9683 - val_loss: 0.9689\n",
      "Epoch 19/25\n",
      "2541/2541 [==============================] - 1s 267us/step - loss: 0.9463 - val_loss: 0.9668\n",
      "Epoch 20/25\n",
      "2541/2541 [==============================] - 1s 267us/step - loss: 0.9549 - val_loss: 0.9814\n",
      "Epoch 21/25\n",
      "2541/2541 [==============================] - 1s 273us/step - loss: 0.9702 - val_loss: 0.9791\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 22/25\n",
      "2541/2541 [==============================] - 2s 882us/step - loss: 0.9658 - val_loss: 0.9774\n",
      "Epoch 23/25\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.9424 - val_loss: 0.9463\n",
      "Epoch 24/25\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.9204 - val_loss: 0.9513\n",
      "Epoch 25/25\n",
      "2541/2541 [==============================] - 2s 693us/step - loss: 0.9342 - val_loss: 0.9748\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/25\n",
      "2541/2541 [==============================] - 8s 3ms/step - loss: 0.9972 - val_loss: 0.9982\n",
      "Epoch 2/25\n",
      "2541/2541 [==============================] - 1s 271us/step - loss: 0.9965 - val_loss: 0.9959\n",
      "Epoch 3/25\n",
      "2541/2541 [==============================] - 1s 273us/step - loss: 0.9949 - val_loss: 0.9801\n",
      "Epoch 4/25\n",
      "2541/2541 [==============================] - 1s 269us/step - loss: 0.9895 - val_loss: 0.9831\n",
      "Epoch 5/25\n",
      "2541/2541 [==============================] - 1s 269us/step - loss: 0.9858 - val_loss: 0.9814\n",
      "Epoch 6/25\n",
      "2541/2541 [==============================] - 1s 270us/step - loss: 0.9831 - val_loss: 0.9772\n",
      "Epoch 7/25\n",
      "2541/2541 [==============================] - 1s 269us/step - loss: 0.9793 - val_loss: 0.9770\n",
      "Epoch 8/25\n",
      "2541/2541 [==============================] - 1s 269us/step - loss: 0.9781 - val_loss: 0.9596\n",
      "Epoch 9/25\n",
      "2541/2541 [==============================] - 1s 270us/step - loss: 0.9740 - val_loss: 0.9759\n",
      "Epoch 10/25\n",
      "2541/2541 [==============================] - 1s 270us/step - loss: 0.9759 - val_loss: 0.9784\n",
      "Epoch 11/25\n",
      "2541/2541 [==============================] - 1s 270us/step - loss: 0.9710 - val_loss: 0.9809\n",
      "Epoch 12/25\n",
      "2541/2541 [==============================] - 1s 270us/step - loss: 0.9678 - val_loss: 0.9789\n",
      "Epoch 13/25\n",
      "2541/2541 [==============================] - 1s 275us/step - loss: 0.9718 - val_loss: 0.9603\n",
      "Epoch 14/25\n",
      "2541/2541 [==============================] - 1s 277us/step - loss: 0.9700 - val_loss: 0.9740\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 15/25\n",
      "2541/2541 [==============================] - 1s 269us/step - loss: 0.9596 - val_loss: 0.9754\n",
      "Epoch 16/25\n",
      "2541/2541 [==============================] - 1s 276us/step - loss: 0.9451 - val_loss: 0.9474\n",
      "Epoch 17/25\n",
      "2541/2541 [==============================] - 1s 272us/step - loss: 0.9669 - val_loss: 0.9777\n",
      "Epoch 18/25\n",
      "2541/2541 [==============================] - 1s 272us/step - loss: 0.9530 - val_loss: 0.9504\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 19/25\n",
      "2541/2541 [==============================] - 1s 279us/step - loss: 0.9474 - val_loss: 0.9757\n",
      "Epoch 20/25\n",
      "2541/2541 [==============================] - 1s 269us/step - loss: 0.9446 - val_loss: 0.9646\n",
      "Epoch 21/25\n",
      "2541/2541 [==============================] - 1s 277us/step - loss: 0.9368 - val_loss: 0.9729\n",
      "Epoch 22/25\n",
      "2541/2541 [==============================] - 1s 275us/step - loss: 0.9408 - val_loss: 0.9633\n",
      "Epoch 23/25\n",
      "2541/2541 [==============================] - 1s 271us/step - loss: 0.9250 - val_loss: 0.9533\n",
      "Epoch 24/25\n",
      "2541/2541 [==============================] - 1s 271us/step - loss: 0.9265 - val_loss: 0.9707\n",
      "Epoch 25/25\n",
      "2541/2541 [==============================] - 1s 285us/step - loss: 0.9219 - val_loss: 1.0085\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/25\n",
      "2541/2541 [==============================] - 9s 3ms/step - loss: 0.9975 - val_loss: 0.9984\n",
      "Epoch 2/25\n",
      "2541/2541 [==============================] - 1s 269us/step - loss: 0.9969 - val_loss: 0.9922\n",
      "Epoch 3/25\n",
      "2541/2541 [==============================] - 1s 279us/step - loss: 0.9945 - val_loss: 0.9819\n",
      "Epoch 4/25\n",
      "2541/2541 [==============================] - ETA: 0s - loss: 0.989 - 1s 270us/step - loss: 0.9889 - val_loss: 0.9808\n",
      "Epoch 5/25\n",
      "2541/2541 [==============================] - 1s 271us/step - loss: 0.9862 - val_loss: 0.9739\n",
      "Epoch 6/25\n",
      "2541/2541 [==============================] - 1s 275us/step - loss: 0.9830 - val_loss: 0.9788\n",
      "Epoch 7/25\n",
      "2541/2541 [==============================] - 1s 272us/step - loss: 0.9823 - val_loss: 0.9475\n",
      "Epoch 8/25\n",
      "2541/2541 [==============================] - 1s 271us/step - loss: 0.9769 - val_loss: 0.9792\n",
      "Epoch 9/25\n",
      "2541/2541 [==============================] - 1s 270us/step - loss: 0.9805 - val_loss: 0.9760\n",
      "Epoch 10/25\n",
      "2541/2541 [==============================] - 1s 269us/step - loss: 0.9798 - val_loss: 0.9763\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 11/25\n",
      "2541/2541 [==============================] - 1s 281us/step - loss: 0.9732 - val_loss: 0.9770\n",
      "Epoch 12/25\n",
      "2541/2541 [==============================] - 1s 269us/step - loss: 0.9704 - val_loss: 0.9727\n",
      "Epoch 13/25\n",
      "2541/2541 [==============================] - 1s 276us/step - loss: 0.9681 - val_loss: 0.9713\n",
      "Epoch 14/25\n",
      "2541/2541 [==============================] - 1s 285us/step - loss: 0.9679 - val_loss: 0.9700\n",
      "Epoch 15/25\n",
      "2541/2541 [==============================] - 1s 270us/step - loss: 0.9571 - val_loss: 0.9734\n",
      "Epoch 16/25\n",
      "2541/2541 [==============================] - 1s 269us/step - loss: 0.9662 - val_loss: 0.9269\n",
      "Epoch 17/25\n",
      "2541/2541 [==============================] - 1s 271us/step - loss: 0.9593 - val_loss: 0.9709\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 18/25\n",
      "2541/2541 [==============================] - 1s 271us/step - loss: 0.9479 - val_loss: 0.9744\n",
      "Epoch 19/25\n",
      "2541/2541 [==============================] - 1s 270us/step - loss: 0.9462 - val_loss: 0.9673\n",
      "Epoch 20/25\n",
      "2541/2541 [==============================] - 1s 278us/step - loss: 0.9347 - val_loss: 0.9629\n",
      "Epoch 21/25\n",
      "2541/2541 [==============================] - 1s 282us/step - loss: 0.9286 - val_loss: 0.9604\n",
      "Epoch 22/25\n",
      "2541/2541 [==============================] - 1s 275us/step - loss: 0.9319 - val_loss: 0.9597\n",
      "Epoch 23/25\n",
      "2541/2541 [==============================] - 1s 271us/step - loss: 0.9383 - val_loss: 0.9536\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 24/25\n",
      "2541/2541 [==============================] - 2s 595us/step - loss: 0.9385 - val_loss: 0.9421\n",
      "Epoch 25/25\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.9087 - val_loss: 0.9461\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2545 samples, validate on 505 samples\n",
      "Epoch 1/25\n",
      "2545/2545 [==============================] - 10s 4ms/step - loss: 0.9976 - val_loss: 0.9976\n",
      "Epoch 2/25\n",
      "2545/2545 [==============================] - 1s 270us/step - loss: 0.9971 - val_loss: 0.9961\n",
      "Epoch 3/25\n",
      "2545/2545 [==============================] - 1s 273us/step - loss: 0.9950 - val_loss: 0.9835\n",
      "Epoch 4/25\n",
      "2545/2545 [==============================] - 1s 270us/step - loss: 0.9904 - val_loss: 0.9776\n",
      "Epoch 5/25\n",
      "2545/2545 [==============================] - 1s 271us/step - loss: 0.9862 - val_loss: 0.9769\n",
      "Epoch 6/25\n",
      "2545/2545 [==============================] - 1s 270us/step - loss: 0.9836 - val_loss: 0.9777\n",
      "Epoch 7/25\n",
      "2545/2545 [==============================] - 1s 270us/step - loss: 0.9808 - val_loss: 0.9805\n",
      "Epoch 8/25\n",
      "2545/2545 [==============================] - 1s 272us/step - loss: 0.9774 - val_loss: 0.9774\n",
      "Epoch 9/25\n",
      "2545/2545 [==============================] - 1s 283us/step - loss: 0.9774 - val_loss: 0.9777\n",
      "Epoch 10/25\n",
      "2545/2545 [==============================] - 1s 277us/step - loss: 0.9754 - val_loss: 0.9748\n",
      "Epoch 11/25\n",
      "2545/2545 [==============================] - 1s 280us/step - loss: 0.9732 - val_loss: 0.9734\n",
      "Epoch 12/25\n",
      "2545/2545 [==============================] - 1s 275us/step - loss: 0.9705 - val_loss: 0.9733\n",
      "Epoch 13/25\n",
      "2545/2545 [==============================] - 1s 279us/step - loss: 0.9721 - val_loss: 0.9727\n",
      "Epoch 14/25\n",
      "2545/2545 [==============================] - 1s 281us/step - loss: 0.9697 - val_loss: 0.9702\n",
      "Epoch 15/25\n",
      "2545/2545 [==============================] - 1s 281us/step - loss: 0.9599 - val_loss: 0.9657\n",
      "Epoch 16/25\n",
      "2545/2545 [==============================] - 1s 276us/step - loss: 0.9612 - val_loss: 0.9669\n",
      "Epoch 17/25\n",
      "2545/2545 [==============================] - 1s 274us/step - loss: 0.9682 - val_loss: 0.9708\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 18/25\n",
      "2545/2545 [==============================] - 1s 285us/step - loss: 0.9642 - val_loss: 0.9673\n",
      "Epoch 19/25\n",
      "2545/2545 [==============================] - 1s 276us/step - loss: 0.9412 - val_loss: 0.9750\n",
      "Epoch 20/25\n",
      "2545/2545 [==============================] - 1s 273us/step - loss: 0.9617 - val_loss: 0.9712\n",
      "Epoch 21/25\n",
      "2545/2545 [==============================] - 1s 272us/step - loss: 0.9633 - val_loss: 0.9591\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 22/25\n",
      "2545/2545 [==============================] - 1s 273us/step - loss: 0.9546 - val_loss: 0.9660\n",
      "Epoch 23/25\n",
      "2545/2545 [==============================] - 1s 273us/step - loss: 0.9492 - val_loss: 0.9542\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 24/25\n",
      "2545/2545 [==============================] - 1s 273us/step - loss: 0.9331 - val_loss: 0.9583\n",
      "Epoch 25/25\n",
      "2545/2545 [==============================] - 1s 271us/step - loss: 0.9195 - val_loss: 0.9666\n"
     ]
    }
   ],
   "source": [
    "#K.clear_session()\n",
    "training_metrics = {}\n",
    "validation_metrics = {}\n",
    "es2 = EarlyStopping(monitor='loss',patience=15, min_delta=0)\n",
    "rlr2 = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=2, verbose=1, min_lr=0.000000001)\n",
    "for i in range(len(training_list)):\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = class_GCN.dataframe_to_gcn_input(validation_list[i])\n",
    "        Y_cold = validation_list[i].Binary \n",
    "        Y_dummy_cold = np.empty((X_atoms_cold.shape[0],gcn_best['dense_size'][2]+1))\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = class_GCN.dataframe_to_gcn_input(training_list[i])\n",
    "        Y = training_list[i].Binary\n",
    "        Y_dummy_train = np.empty((X_atoms_train.shape[0],gcn_best['dense_size'][2]+1))\n",
    "        \n",
    "        gcn_encoder = class_GCN.build_encoder()\n",
    "        gcn_model = class_GCN.build_model(gcn_encoder)\n",
    "        gcn_mining = class_GCN.build_mining(gcn_model)\n",
    "        \n",
    "        gcn_mining.fit([X_atoms_train,X_bonds_train,X_edges_train,Y],\n",
    "                       Y_dummy_train,\n",
    "                       epochs = gcn_best['n_epochs'],\n",
    "                       batch_size = gcn_best['batch_size'],\n",
    "                       shuffle = True,\n",
    "                       validation_data = ([X_atoms_cold,X_bonds_cold,X_edges_cold,Y_cold],Y_dummy_cold),\n",
    "                       callbacks=[es2,rlr2]\n",
    "                      )\n",
    "        #Predict Embeddings\n",
    "        embeddings_cold = gcn_model.predict([X_atoms_cold,X_bonds_cold,X_edges_cold])\n",
    "        embeddings_train = gcn_model.predict([X_atoms_train, X_bonds_train, X_edges_train])\n",
    "        \n",
    "        #Prepare data for XGBoost\n",
    "        dmatrix_train = class_XGB.to_xgb_input(Y,embeddings_train)\n",
    "        dmatrix_cold = class_XGB.to_xgb_input(Y_cold,embeddings_cold)\n",
    "        \n",
    "        evalist = [(dmatrix_train,'train'),(dmatrix_cold,'eval')]\n",
    "        xgb_model = class_XGB.build_model(dmatrix_train,evalist,300)\n",
    "        \n",
    "        xgb_pred_cold = xgb_model.predict(dmatrix_cold)\n",
    "        validation_metrics['Val_%s'%i] = calculate_metrics(np.array(Y_cold),xgb_pred_cold)\n",
    "        \n",
    "        xgb_pred_train = xgb_model.predict(dmatrix_train)\n",
    "        training_metrics['Train_%s'%i] = calculate_metrics(np.array(Y),xgb_pred_train)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3050 samples, validate on 509 samples\n",
      "Epoch 1/25\n",
      "3050/3050 [==============================] - 9s 3ms/step - loss: 0.9971 - val_loss: 0.9970\n",
      "Epoch 2/25\n",
      "3050/3050 [==============================] - 1s 266us/step - loss: 0.9957 - val_loss: 0.9881\n",
      "Epoch 3/25\n",
      "3050/3050 [==============================] - 1s 267us/step - loss: 0.9912 - val_loss: 0.9847\n",
      "Epoch 4/25\n",
      "3050/3050 [==============================] - 1s 267us/step - loss: 0.9873 - val_loss: 0.9864\n",
      "Epoch 5/25\n",
      "3050/3050 [==============================] - 1s 270us/step - loss: 0.9838 - val_loss: 0.9846\n",
      "Epoch 6/25\n",
      "3050/3050 [==============================] - 1s 266us/step - loss: 0.9811 - val_loss: 0.9841\n",
      "Epoch 7/25\n",
      "3050/3050 [==============================] - 1s 270us/step - loss: 0.9790 - val_loss: 0.9835\n",
      "Epoch 8/25\n",
      "3050/3050 [==============================] - 1s 273us/step - loss: 0.9761 - val_loss: 0.9834\n",
      "Epoch 9/25\n",
      "3050/3050 [==============================] - 1s 267us/step - loss: 0.9761 - val_loss: 0.9816\n",
      "Epoch 10/25\n",
      "3050/3050 [==============================] - 1s 272us/step - loss: 0.9732 - val_loss: 0.9780\n",
      "Epoch 11/25\n",
      "3050/3050 [==============================] - 1s 269us/step - loss: 0.9735 - val_loss: 0.9794\n",
      "Epoch 12/25\n",
      "3050/3050 [==============================] - 1s 266us/step - loss: 0.9704 - val_loss: 0.9822\n",
      "Epoch 13/25\n",
      "3050/3050 [==============================] - 1s 273us/step - loss: 0.9735 - val_loss: 0.9833\n",
      "Epoch 14/25\n",
      "3050/3050 [==============================] - 1s 273us/step - loss: 0.9650 - val_loss: 0.9778\n",
      "Epoch 15/25\n",
      "3050/3050 [==============================] - 1s 268us/step - loss: 0.9697 - val_loss: 0.9796\n",
      "Epoch 16/25\n",
      "3050/3050 [==============================] - 1s 268us/step - loss: 0.9578 - val_loss: 0.9755\n",
      "Epoch 17/25\n",
      "3050/3050 [==============================] - 1s 268us/step - loss: 0.9667 - val_loss: 0.9824\n",
      "Epoch 18/25\n",
      "3050/3050 [==============================] - 1s 268us/step - loss: 0.9484 - val_loss: 0.9799\n",
      "Epoch 19/25\n",
      "3050/3050 [==============================] - 1s 268us/step - loss: 0.9702 - val_loss: 0.9763\n",
      "Epoch 20/25\n",
      "3050/3050 [==============================] - 1s 270us/step - loss: 0.9570 - val_loss: 0.9680\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 21/25\n",
      "3050/3050 [==============================] - 1s 268us/step - loss: 0.9543 - val_loss: 0.9679\n",
      "Epoch 22/25\n",
      "3050/3050 [==============================] - 1s 267us/step - loss: 0.9584 - val_loss: 0.9771\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 23/25\n",
      "3050/3050 [==============================] - 1s 267us/step - loss: 0.9485 - val_loss: 0.9792\n",
      "Epoch 24/25\n",
      "3050/3050 [==============================] - 1s 270us/step - loss: 0.9289 - val_loss: 0.9777\n",
      "Epoch 25/25\n",
      "3050/3050 [==============================] - 1s 266us/step - loss: 0.9230 - val_loss: 0.9725\n"
     ]
    }
   ],
   "source": [
    "es2 = EarlyStopping(monitor='loss',patience=15, min_delta=0)\n",
    "rlr2 = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=2, verbose=1, min_lr=0.000000001)\n",
    "X_atoms_cold,X_bonds_cold,X_edges_cold = class_GCN.dataframe_to_gcn_input(val)\n",
    "Y_cold = val.Binary \n",
    "Y_dummy_cold = np.empty((X_atoms_cold.shape[0],gcn_best['dense_size'][2]+1))\n",
    "X_atoms_train, X_bonds_train, X_edges_train = class_GCN.dataframe_to_gcn_input(train)\n",
    "Y = train.Binary\n",
    "Y_dummy_train = np.empty((X_atoms_train.shape[0],gcn_best['dense_size'][2]+1))\n",
    "        \n",
    "gcn_encoder = class_GCN.build_encoder()\n",
    "gcn_model = class_GCN.build_model(gcn_encoder)\n",
    "gcn_mining = class_GCN.build_mining(gcn_model)\n",
    "gcn_mining.fit([X_atoms_train,X_bonds_train,X_edges_train,Y],\n",
    "                       Y_dummy_train,\n",
    "                       epochs = gcn_best['n_epochs'],\n",
    "                       batch_size = gcn_best['batch_size'],\n",
    "                       shuffle = True,\n",
    "                       validation_data = ([X_atoms_cold,X_bonds_cold,X_edges_cold,Y_cold],Y_dummy_cold),\n",
    "                       callbacks=[es2,rlr2]\n",
    "                      )\n",
    "        #Predict Embeddings\n",
    "embeddings_cold = gcn_model.predict([X_atoms_cold,X_bonds_cold,X_edges_cold])\n",
    "embeddings_train = gcn_model.predict([X_atoms_train, X_bonds_train, X_edges_train])\n",
    "        \n",
    "        #Prepare data for XGBoost\n",
    "dmatrix_train = class_XGB.to_xgb_input(Y,embeddings_train)\n",
    "dmatrix_cold = class_XGB.to_xgb_input(Y_cold,embeddings_cold)\n",
    "        \n",
    "evalist = [(dmatrix_train,'train'),(dmatrix_cold,'eval')]\n",
    "xgb_model = class_XGB.build_model(dmatrix_train,evalist,300)\n",
    "        \n",
    "xgb_pred_cold = xgb_model.predict(dmatrix_cold)\n",
    "validation_metrics = calculate_metrics(np.array(Y_cold),xgb_pred_cold)\n",
    "        \n",
    "xgb_pred_train = xgb_model.predict(dmatrix_train)\n",
    "training_metrics = calculate_metrics(np.array(Y),xgb_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.8126307994462152,\n",
       " 'tn': 235,\n",
       " 'fp': 71,\n",
       " 'fn': 63,\n",
       " 'tp': 140,\n",
       " 'map': 0.7295864861386149,\n",
       " 'precision': 0.6635071090047393,\n",
       " 'recall': 0.6896551724137931,\n",
       " 'accuracy': 0.7367387033398821}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.9511664222545811,\n",
       " 'tn': 1335,\n",
       " 'fp': 231,\n",
       " 'fn': 116,\n",
       " 'tp': 1368,\n",
       " 'map': 0.939258856153558,\n",
       " 'precision': 0.8555347091932458,\n",
       " 'recall': 0.921832884097035,\n",
       " 'accuracy': 0.8862295081967213}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
