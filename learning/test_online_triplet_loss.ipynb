{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.callbacks import History, ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from data_analysis import calculate_metrics, load_weights_and_evaluate\n",
    "from model_builders import GCN_pretraining\n",
    "from hyperparameter_tuning_GCN import objective\n",
    "from functools import partial\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pickle\n",
    "import dill\n",
    "from hyper_mining import objective_fn\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, concatenate,Flatten\n",
    "from keras.models import Model, load_model\n",
    "from hyper_mining import XGB_predictor\n",
    "from data_analysis import calculate_metrics\n",
    "from distance_and_mask_fn import pairwise_distance,masked_maximum,masked_minimum\n",
    "import seaborn as sns\n",
    "from NGF.preprocessing import tensorise_smiles\n",
    "from custom_layers.model_creator import encode_smiles, stage_creator\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_online_mining(object):\n",
    "    \n",
    "    def __init__(self,  model_params):\n",
    "        self.model_params = model_params\n",
    "    \n",
    "    def build_encoder(self):\n",
    "        model_enc_1 = stage_creator(self.model_params, 1, conv=True)[0]\n",
    "        model_enc_2 = stage_creator(self.model_params, 2, conv=True)[0]\n",
    "        model_enc_3 = stage_creator(self.model_params, 3, conv=True)[0]\n",
    "        \n",
    "        model_enc_fp_1 = stage_creator(self.model_params, 1, conv=False)[1]\n",
    "        model_enc_fp_2 = stage_creator(self.model_params, 2, conv=False)[1]\n",
    "        model_enc_fp_3 = stage_creator(self.model_params, 3, conv=False)[1]\n",
    "        \n",
    "        atoms, bonds, edges = encode_smiles(self.model_params[\"max_atoms\"],\n",
    "                                            self.model_params[\"num_atom_features\"],\n",
    "                                            self.model_params[\"max_degree\"],\n",
    "                                            self.model_params[\"num_bond_features\"])\n",
    "        \n",
    "        graph_conv_1 = model_enc_1([atoms, bonds, edges])\n",
    "        graph_conv_2 = model_enc_2([graph_conv_1, bonds, edges])\n",
    "        graph_conv_3 = model_enc_3([graph_conv_2, bonds, edges])\n",
    "        \n",
    "        fingerprint_1 = model_enc_fp_1([graph_conv_1, bonds, edges])\n",
    "        fingerprint_1 = Lambda(lambda x: K.sum(x, axis=1), output_shape=lambda s: (s[0], s[2]))(fingerprint_1)\n",
    "        \n",
    "        fingerprint_2 = model_enc_fp_2([graph_conv_2, bonds, edges])\n",
    "        fingerprint_2 = Lambda(lambda x: K.sum(x, axis=1), output_shape=lambda s: (s[0], s[2]))(fingerprint_2)\n",
    "        \n",
    "        fingerprint_3 = model_enc_fp_3([graph_conv_3, bonds, edges])\n",
    "        fingerprint_3 = Lambda(lambda x: K.sum(x, axis=1), output_shape=lambda s: (s[0], s[2]))(fingerprint_3)\n",
    "        \n",
    "        final_fingerprint = keras.layers.add([fingerprint_1, fingerprint_2, fingerprint_3])\n",
    "        return Model([atoms, bonds, edges], [final_fingerprint])\n",
    "    \n",
    "    def build_model(self, encoder, verbose=False):\n",
    "        atoms = Input(name='atom_inputs',shape=(self.model_params['max_atoms'],\n",
    "                                                self.model_params['num_atom_features']), dtype='float32')\n",
    "        bonds = Input(name='bond_inputs', shape=(self.model_params['max_atoms'], \n",
    "                                                 self.model_params['max_degree'],\n",
    "                                                 self.model_params['num_bond_features']),dtype='float32')\n",
    "        edges = Input(name='edge_inputs', shape=(self.model_params['max_atoms'], \n",
    "                                                 self.model_params['max_degree']),dtype='int32')\n",
    "        encode_drug = encoder([atoms, bonds, edges])\n",
    "        \n",
    "        # Fully connected\n",
    "        FC1 = Dense(self.model_params[\"dense_size\"][0], \n",
    "                    activation='relu',kernel_initializer='random_normal')(encode_drug)\n",
    "        FC2 = Dropout(self.model_params[\"dropout_rate\"][0])(FC1)\n",
    "        FC2 = Dense(self.model_params[\"dense_size\"][1], \n",
    "                    activation='relu',kernel_initializer='random_normal')(FC2)\n",
    "        FC2 = Dropout(self.model_params[\"dropout_rate\"][1])(FC2)\n",
    "        FC2 = Dense(self.model_params[\"dense_size\"][2], \n",
    "                    activation = None,kernel_initializer='random_normal')(FC2)\n",
    "        \n",
    "        \n",
    "        embeddings = Lambda(lambda x: K.l2_normalize(x,axis=1),name = 'Embeddings')(FC2)\n",
    "        \n",
    "        gcn_model = Model(inputs=[atoms, bonds, edges], outputs = embeddings)\n",
    "        \n",
    "        if verbose:\n",
    "            print('encoder')\n",
    "            encoder.summary()\n",
    "            print('GCN_model')\n",
    "        return gcn_model\n",
    "    \n",
    "    \n",
    "\n",
    "    def build_mining(self,gcn_model):\n",
    "        atoms = Input(name='atom_inputs',shape=(self.model_params['max_atoms'],\n",
    "                                                self.model_params['num_atom_features']), dtype='float32')\n",
    "        bonds = Input(name='bond_inputs', shape=(self.model_params['max_atoms'], \n",
    "                                                 self.model_params['max_degree'],\n",
    "                                                 self.model_params['num_bond_features']),dtype='float32')\n",
    "        edges = Input(name='edge_inputs', shape=(self.model_params['max_atoms'], \n",
    "                                                 self.model_params['max_degree']),dtype='int32')\n",
    "        labels = Input(name = 'labels_inputs',shape = (1,),dtype = 'float32')\n",
    "        encoded = gcn_model([atoms,bonds,edges])\n",
    "        labels_plus_embeddings = concatenate([labels, encoded])\n",
    "        mining_net = Model(inputs = [atoms,bonds,edges,labels],outputs = labels_plus_embeddings)\n",
    "        adam = keras.optimizers.Adam(lr = self.model_params[\"lr\"], \n",
    "                                     beta_1=0.9, \n",
    "                                     beta_2=0.999, \n",
    "                                     decay=0.0, \n",
    "                                     amsgrad=False)\n",
    "        mining_net.compile(optimizer=adam , loss = triplet_loss_adapted_from_tf)\n",
    "        return mining_net\n",
    "    \n",
    "    def dataframe_to_gcn_input(self,input_data):\n",
    "        x_atoms_cold, x_bonds_cold, x_edges_cold = tensorise_smiles(input_data['rdkit'],\n",
    "                                                                    max_degree=self.model_params['max_degree'],max_atoms=self.model_params['max_atoms'])\n",
    "        return [x_atoms_cold, x_bonds_cold, x_edges_cold]\n",
    "    \n",
    "def triplet_loss_adapted_from_tf(y_true, y_pred):\n",
    "        del y_true\n",
    "        margin = gcn_params['margin']\n",
    "        labels = y_pred[:, :1]\n",
    "        labels = tf.cast(labels, dtype='int32')\n",
    "        embeddings = y_pred[:, 1:]\n",
    "\n",
    "        ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "        # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "        # lshape=array_ops.shape(labels)\n",
    "        # assert lshape.shape == 1\n",
    "        # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "        # Build pairwise squared distance matrix.\n",
    "        pdist_matrix = pairwise_distance(embeddings, squared=False)\n",
    "        # Build pairwise binary adjacency matrix.\n",
    "        adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "        # Invert so we can select negatives only.\n",
    "        adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "        # global batch_size  \n",
    "        batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "        # Compute the mask.\n",
    "        pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "        mask = math_ops.logical_and(\n",
    "            array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "            math_ops.greater(\n",
    "                pdist_matrix_tile, array_ops.reshape(\n",
    "                    array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "        mask_final = array_ops.reshape(\n",
    "            math_ops.greater(\n",
    "                math_ops.reduce_sum(\n",
    "                    math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "                0.0), [batch_size, batch_size])\n",
    "        mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "        adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "        mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "        # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "        negatives_outside = array_ops.reshape(\n",
    "            masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "        negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "        # negatives_inside: largest D_an.\n",
    "        negatives_inside = array_ops.tile(\n",
    "            masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "        semi_hard_negatives = array_ops.where(\n",
    "            mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "        loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "        mask_positives = math_ops.cast(\n",
    "            adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "            array_ops.ones([batch_size]))\n",
    "\n",
    "        # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "        #   in semihard, they take all positive pairs except the diagonal.\n",
    "        num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "        semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.maximum(\n",
    "                    math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "            num_positives,\n",
    "            name='triplet_semihard_loss')\n",
    "    \n",
    "        ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "        return semi_hard_triplet_loss_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss',patience=8, min_delta=0)\n",
    "rlr = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=4, verbose=1, min_lr=0.0000001)\n",
    "gcn_params = {\n",
    "        \"num_layers\" : 3,\n",
    "        \"max_atoms\" : 70,\n",
    "        \"num_atom_features\" : 62,\n",
    "        \"num_atom_features_original\" : 62,\n",
    "        \"num_bond_features\" : 6,\n",
    "        \"max_degree\" : 5,\n",
    "        \"conv_width\" : [int(88), int(96), int(160)],\n",
    "        \"fp_length\" : [int(160), int(160), int(160)],\n",
    "        \"activ_enc\" : \"selu\",\n",
    "        \"activ_dec\" : \"selu\",\n",
    "        \"learning_rates\" : [0.001,0.001,0.001],\n",
    "        \"learning_rates_fp\": [0.005,0.005,0.005],\n",
    "        \"losses_conv\" : {\n",
    "                    \"neighbor_output\": \"mean_squared_error\",\n",
    "                    \"self_output\": \"mean_squared_error\",\n",
    "                    },\n",
    "        \"lossWeights\" : {\"neighbor_output\": 1.0, \"self_output\": 1.0},\n",
    "        \"metrics\" : \"mse\",\n",
    "        \"loss_fp\" : \"mean_squared_error\",\n",
    "        \"enc_layer_names\" : [\"enc_1\", \"enc_2\", \"enc_3\"],\n",
    "        'callbacks' : [es,rlr],\n",
    "        'adam_decay': 0.0005329142291371636,\n",
    "        'beta': 5,\n",
    "        'p': 0.004465204118126482,\n",
    "        'dense_size' : [int(96), int(416), int(320)],\n",
    "        'dropout_rate' : [0.45547284530546817, 0.45547284530546817],\n",
    "        'lr' : 0.0019499018534396453,\n",
    "        'batch_size' : int(96),\n",
    "        'n_epochs' : int(60),\n",
    "        'margin' : 0.6011401246738063\n",
    "        }\n",
    "xgb_params = {\n",
    "        \"colsample_bylevel\" : 0.19610957495328543,\n",
    "        \"colsample_bytree\" : 0.9504328963958085,\n",
    "        \"gamma\" : 0.66484093460,\n",
    "        \"eta\" : 0.9073217403165388,\n",
    "        \"max_delta_step\" : int(1),\n",
    "        \"max_depth\" : int(8),\n",
    "        \"min_child_weight\" : int(355),\n",
    "        \"alpha\" : 56.190739151936484,\n",
    "        \"lambda\" : 52.450354051682496,\n",
    "        \"subsample\" : 0.8316785844601918,\n",
    "        \"max_bin\" : int(208),\n",
    "        \"eval_metric\":'auc',\n",
    "        \"objective\":'binary:logistic',\n",
    "        \"booster\":'gbtree',\n",
    "        \"single_precision_histogram\" : True\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_XGB = XGB_predictor(xgb_params)\n",
    "class_GCN = GCN_online_mining(gcn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0 = pd.read_csv(\"data/p38/split_aveb/fold_0/train_0.csv\",index_col=0)\n",
    "train_1 = pd.read_csv(\"data/p38/split_aveb/fold_1/train_1.csv\",index_col=0)\n",
    "train_2 = pd.read_csv(\"data/p38/split_aveb/fold_2/train_2.csv\",index_col=0)\n",
    "train_3 = pd.read_csv(\"data/p38/split_aveb/fold_3/train_3.csv\",index_col=0)\n",
    "train_4 = pd.read_csv(\"data/p38/split_aveb/fold_4/train_4.csv\",index_col=0)\n",
    "train_5 = pd.read_csv(\"data/p38/split_aveb/fold_5/train_5.csv\",index_col=0)\n",
    "train_6 = pd.read_csv(\"data/p38/split_aveb/fold_6/train_6.csv\",index_col=0)\n",
    "\n",
    "train_set = [train_0,train_1,train_2,train_3,train_4,train_5,train_6]\n",
    "del train_0,train_1,train_2,train_3,train_4,train_5,train_6\n",
    "\n",
    "val_0 = pd.read_csv(\"data/p38/split_aveb/fold_0/val_0.csv\",index_col=0)\n",
    "val_1 = pd.read_csv(\"data/p38/split_aveb/fold_1/val_1.csv\",index_col=0)\n",
    "val_2 = pd.read_csv(\"data/p38/split_aveb/fold_2/val_2.csv\",index_col=0)\n",
    "val_3 = pd.read_csv(\"data/p38/split_aveb/fold_3/val_3.csv\",index_col=0)\n",
    "val_4 = pd.read_csv(\"data/p38/split_aveb/fold_4/val_4.csv\",index_col=0)\n",
    "val_5 = pd.read_csv(\"data/p38/split_aveb/fold_5/val_5.csv\",index_col=0)\n",
    "val_6 = pd.read_csv(\"data/p38/split_aveb/fold_6/val_6.csv\",index_col=0)\n",
    "\n",
    "val_set = [val_0,val_1,val_2,val_3,val_4,val_5,val_6]\n",
    "del val_0,val_1,val_2,val_3,val_4,val_5,val_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del val_set , train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi Hard Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'p38'\n",
    "base_path = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath = base_path+f'/data/{target}/data.csv'\n",
    "df=pd.read_csv(data_fpath).set_index('biolab_index')\n",
    "\n",
    "with open(base_path+f'/data/{target}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds = dill.load(in_f)\n",
    "with open(base_path+f'/data/{target}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds = dill.load(in_f)\n",
    "    \n",
    "training_list = [df.loc[train_val_folds[0][0]],\n",
    "                 df.loc[train_val_folds[1][0]],\n",
    "                 df.loc[train_val_folds[2][0]],\n",
    "                 df.loc[train_val_folds[3][0]],\n",
    "                 df.loc[train_val_folds[4][0]],\n",
    "                 df.loc[train_val_folds[5][0]],\n",
    "                 ]\n",
    "validation_list = [df.loc[train_val_folds[0][1]],\n",
    "                   df.loc[train_val_folds[1][1]],\n",
    "                   df.loc[train_val_folds[2][1]],\n",
    "                   df.loc[train_val_folds[3][1]],\n",
    "                   df.loc[train_val_folds[4][1]],\n",
    "                   df.loc[train_val_folds[5][1]],\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/60\n",
      "2541/2541 [==============================] - 6s 2ms/step - loss: 0.5895 - val_loss: 0.5970\n",
      "Epoch 2/60\n",
      "2541/2541 [==============================] - 1s 380us/step - loss: 0.5750 - val_loss: 0.5968\n",
      "Epoch 3/60\n",
      "2541/2541 [==============================] - 1s 364us/step - loss: 0.5693 - val_loss: 0.5979\n",
      "Epoch 4/60\n",
      "2541/2541 [==============================] - 1s 375us/step - loss: 0.5690 - val_loss: 0.5821\n",
      "Epoch 5/60\n",
      "2541/2541 [==============================] - 1s 361us/step - loss: 0.5665 - val_loss: 0.5990\n",
      "Epoch 6/60\n",
      "2541/2541 [==============================] - 1s 377us/step - loss: 0.5656 - val_loss: 0.5999\n",
      "Epoch 7/60\n",
      "2541/2541 [==============================] - 1s 359us/step - loss: 0.5644 - val_loss: 0.5994\n",
      "Epoch 8/60\n",
      "2541/2541 [==============================] - 1s 376us/step - loss: 0.5632 - val_loss: 0.5987\n",
      "Epoch 9/60\n",
      "2541/2541 [==============================] - 1s 360us/step - loss: 0.5693 - val_loss: 0.5993\n",
      "Epoch 10/60\n",
      "2541/2541 [==============================] - 1s 377us/step - loss: 0.5671 - val_loss: 0.5990\n",
      "Epoch 11/60\n",
      "2541/2541 [==============================] - 1s 361us/step - loss: 0.5657 - val_loss: 0.5987\n",
      "Epoch 12/60\n",
      "2541/2541 [==============================] - 1s 372us/step - loss: 0.5659 - val_loss: 0.5968\n",
      "Epoch 13/60\n",
      "2541/2541 [==============================] - 1s 367us/step - loss: 0.5662 - val_loss: 0.5939\n",
      "Epoch 14/60\n",
      "2541/2541 [==============================] - 2s 620us/step - loss: 0.5646 - val_loss: 0.5799\n",
      "Epoch 15/60\n",
      "2541/2541 [==============================] - 5s 2ms/step - loss: 0.5637 - val_loss: 0.5949\n",
      "Epoch 16/60\n",
      "2541/2541 [==============================] - 5s 2ms/step - loss: 0.5618 - val_loss: 0.5959\n",
      "Epoch 17/60\n",
      "2541/2541 [==============================] - 5s 2ms/step - loss: 0.5620 - val_loss: 0.5967\n",
      "Epoch 18/60\n",
      "2541/2541 [==============================] - 5s 2ms/step - loss: 0.5608 - val_loss: 0.5955\n",
      "Epoch 19/60\n",
      "2541/2541 [==============================] - 2s 856us/step - loss: 0.5615 - val_loss: 0.5971\n",
      "Epoch 20/60\n",
      "2541/2541 [==============================] - 1s 371us/step - loss: 0.5627 - val_loss: 0.5959\n",
      "Epoch 21/60\n",
      "2541/2541 [==============================] - 1s 376us/step - loss: 0.5622 - val_loss: 0.5933\n",
      "Epoch 22/60\n",
      "2541/2541 [==============================] - 1s 365us/step - loss: 0.5620 - val_loss: 0.5933\n",
      "Epoch 23/60\n",
      "2541/2541 [==============================] - 1s 377us/step - loss: 0.5612 - val_loss: 0.5971\n",
      "Epoch 24/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5610 - val_loss: 0.5938\n",
      "Epoch 25/60\n",
      "2541/2541 [==============================] - 4s 2ms/step - loss: 0.5593 - val_loss: 0.5951\n",
      "Epoch 26/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5609 - val_loss: 0.5957\n",
      "Epoch 27/60\n",
      "2541/2541 [==============================] - 2s 878us/step - loss: 0.5613 - val_loss: 0.5903\n",
      "Epoch 28/60\n",
      "2541/2541 [==============================] - 1s 323us/step - loss: 0.5588 - val_loss: 0.5844\n",
      "Epoch 29/60\n",
      "2541/2541 [==============================] - 1s 323us/step - loss: 0.5605 - val_loss: 0.5717\n",
      "Epoch 30/60\n",
      "2541/2541 [==============================] - 1s 325us/step - loss: 0.5602 - val_loss: 0.5563\n",
      "Epoch 31/60\n",
      "2541/2541 [==============================] - 1s 341us/step - loss: 0.5585 - val_loss: 0.5785\n",
      "Epoch 32/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5603 - val_loss: 0.5344\n",
      "Epoch 33/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5587 - val_loss: 0.5557\n",
      "Epoch 34/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5621 - val_loss: 0.5615\n",
      "Epoch 35/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5625 - val_loss: 0.6190\n",
      "Epoch 36/60\n",
      "2541/2541 [==============================] - 1s 429us/step - loss: 0.5705 - val_loss: 0.6086\n",
      "Epoch 37/60\n",
      "2541/2541 [==============================] - 1s 321us/step - loss: 0.5623 - val_loss: 0.5993\n",
      "Epoch 38/60\n",
      "2541/2541 [==============================] - 1s 321us/step - loss: 0.5613 - val_loss: 0.6435\n",
      "Epoch 39/60\n",
      "2541/2541 [==============================] - 1s 325us/step - loss: 0.5593 - val_loss: 0.5459\n",
      "Epoch 40/60\n",
      "2541/2541 [==============================] - 1s 532us/step - loss: 0.5512 - val_loss: 0.5398\n",
      "Epoch 41/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5517 - val_loss: 0.5584\n",
      "Epoch 42/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5505 - val_loss: 0.6384\n",
      "Epoch 43/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5603 - val_loss: 0.5578\n",
      "Epoch 44/60\n",
      "2541/2541 [==============================] - 1s 555us/step - loss: 0.5523 - val_loss: 0.5516\n",
      "Epoch 45/60\n",
      "2541/2541 [==============================] - 1s 324us/step - loss: 0.5506 - val_loss: 0.5529\n",
      "Epoch 46/60\n",
      "2541/2541 [==============================] - 1s 323us/step - loss: 0.5537 - val_loss: 0.5464\n",
      "Epoch 47/60\n",
      "2541/2541 [==============================] - 1s 323us/step - loss: 0.5513 - val_loss: 0.5439\n",
      "Epoch 48/60\n",
      "2541/2541 [==============================] - 1s 418us/step - loss: 0.5508 - val_loss: 0.5605\n",
      "Epoch 49/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5468 - val_loss: 0.5436\n",
      "Epoch 50/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5477 - val_loss: 0.5347\n",
      "Epoch 51/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5397 - val_loss: 0.5190\n",
      "Epoch 52/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5347 - val_loss: 0.5529\n",
      "Epoch 53/60\n",
      "2541/2541 [==============================] - 1s 528us/step - loss: 0.5410 - val_loss: 0.5262\n",
      "Epoch 54/60\n",
      "2541/2541 [==============================] - 1s 321us/step - loss: 0.5395 - val_loss: 0.5505\n",
      "Epoch 55/60\n",
      "2541/2541 [==============================] - 1s 322us/step - loss: 0.5441 - val_loss: 0.5726\n",
      "Epoch 56/60\n",
      "2541/2541 [==============================] - 1s 323us/step - loss: 0.5412 - val_loss: 0.5044\n",
      "Epoch 57/60\n",
      "2541/2541 [==============================] - 1s 323us/step - loss: 0.5454 - val_loss: 0.5082\n",
      "Epoch 58/60\n",
      "2541/2541 [==============================] - 1s 326us/step - loss: 0.5428 - val_loss: 0.5556\n",
      "Epoch 59/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5381 - val_loss: 0.5436\n",
      "Epoch 60/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5424 - val_loss: 0.5454\n",
      "[13:33:39] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\learner.cc:328: \n",
      "Parameters: { single_precision_histogram } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/60\n",
      "2541/2541 [==============================] - 6s 2ms/step - loss: 0.5862 - val_loss: 0.5988\n",
      "Epoch 2/60\n",
      "2541/2541 [==============================] - 1s 332us/step - loss: 0.5741 - val_loss: 0.6005\n",
      "Epoch 3/60\n",
      "2541/2541 [==============================] - 1s 329us/step - loss: 0.5715 - val_loss: 0.6006\n",
      "Epoch 4/60\n",
      "2541/2541 [==============================] - 1s 328us/step - loss: 0.5686 - val_loss: 0.6003\n",
      "Epoch 5/60\n",
      "2541/2541 [==============================] - 1s 331us/step - loss: 0.5675 - val_loss: 0.5990\n",
      "Epoch 6/60\n",
      "2541/2541 [==============================] - 1s 329us/step - loss: 0.5667 - val_loss: 0.5982\n",
      "Epoch 7/60\n",
      "2541/2541 [==============================] - 1s 328us/step - loss: 0.5671 - val_loss: 0.5969\n",
      "Epoch 8/60\n",
      "2541/2541 [==============================] - 1s 330us/step - loss: 0.5671 - val_loss: 0.6005\n",
      "Epoch 9/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5654 - val_loss: 0.6000\n",
      "Epoch 10/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5653 - val_loss: 0.6002\n",
      "Epoch 11/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5629 - val_loss: 0.5997\n",
      "Epoch 12/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5638 - val_loss: 0.6001\n",
      "Epoch 13/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5650 - val_loss: 0.6008\n",
      "Epoch 14/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5624 - val_loss: 0.6005\n",
      "Epoch 15/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5647 - val_loss: 0.6008\n",
      "Epoch 16/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5625 - val_loss: 0.6006\n",
      "Epoch 17/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5621 - val_loss: 0.6006\n",
      "Epoch 18/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5626 - val_loss: 0.6007\n",
      "Epoch 19/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5625 - val_loss: 0.6008\n",
      "Epoch 20/60\n",
      "2541/2541 [==============================] - 1s 335us/step - loss: 0.5625 - val_loss: 0.6008\n",
      "Epoch 21/60\n",
      "2541/2541 [==============================] - 1s 372us/step - loss: 0.5639 - val_loss: 0.6007\n",
      "Epoch 22/60\n",
      "2541/2541 [==============================] - 1s 357us/step - loss: 0.5637 - val_loss: 0.6005\n",
      "Epoch 23/60\n",
      "2541/2541 [==============================] - 1s 355us/step - loss: 0.5621 - val_loss: 0.6005\n",
      "Epoch 24/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5617 - val_loss: 0.6006\n",
      "Epoch 25/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5608 - val_loss: 0.6007\n",
      "Epoch 26/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5613 - val_loss: 0.6008\n",
      "Epoch 27/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5610 - val_loss: 0.6006\n",
      "Epoch 28/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5622 - val_loss: 0.6004\n",
      "Epoch 29/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5608 - val_loss: 0.6005\n",
      "Epoch 30/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5630 - val_loss: 0.6006\n",
      "Epoch 31/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5639 - val_loss: 0.6008\n",
      "Epoch 32/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5617 - val_loss: 0.6006\n",
      "Epoch 33/60\n",
      "2541/2541 [==============================] - 1s 341us/step - loss: 0.5633 - val_loss: 0.6004\n",
      "Epoch 34/60\n",
      "2541/2541 [==============================] - 1s 334us/step - loss: 0.5598 - val_loss: 0.6003\n",
      "Epoch 35/60\n",
      "2541/2541 [==============================] - 1s 341us/step - loss: 0.5603 - val_loss: 0.6003\n",
      "Epoch 36/60\n",
      "2541/2541 [==============================] - 1s 347us/step - loss: 0.5632 - val_loss: 0.6006\n",
      "Epoch 37/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5596 - val_loss: 0.6006\n",
      "Epoch 38/60\n",
      "2541/2541 [==============================] - 5s 2ms/step - loss: 0.5601 - val_loss: 0.6006\n",
      "Epoch 39/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5621 - val_loss: 0.6005\n",
      "Epoch 40/60\n",
      "2541/2541 [==============================] - 4s 2ms/step - loss: 0.5604 - val_loss: 0.6004\n",
      "Epoch 41/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5598 - val_loss: 0.6003\n",
      "Epoch 42/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5598 - val_loss: 0.6005\n",
      "Epoch 43/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5614 - val_loss: 0.6005\n",
      "Epoch 44/60\n",
      "2541/2541 [==============================] - 2s 914us/step - loss: 0.5630 - val_loss: 0.6006\n",
      "Epoch 45/60\n",
      "2541/2541 [==============================] - 1s 337us/step - loss: 0.5616 - val_loss: 0.6006\n",
      "Epoch 46/60\n",
      "2541/2541 [==============================] - 1s 338us/step - loss: 0.5623 - val_loss: 0.6003\n",
      "Epoch 47/60\n",
      "2541/2541 [==============================] - 1s 334us/step - loss: 0.5602 - val_loss: 0.6005\n",
      "Epoch 48/60\n",
      "2541/2541 [==============================] - 1s 348us/step - loss: 0.5605 - val_loss: 0.5979\n",
      "Epoch 49/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5598 - val_loss: 0.6004\n",
      "Epoch 50/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5609 - val_loss: 0.6002\n",
      "Epoch 51/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5591 - val_loss: 0.5997\n",
      "Epoch 52/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5597 - val_loss: 0.5996\n",
      "Epoch 53/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5602 - val_loss: 0.6010\n",
      "Epoch 54/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5623 - val_loss: 0.6009\n",
      "Epoch 55/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5606 - val_loss: 0.6009\n",
      "Epoch 56/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5602 - val_loss: 0.6009\n",
      "Epoch 57/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5631 - val_loss: 0.6009\n",
      "Epoch 58/60\n",
      "2541/2541 [==============================] - 1s 337us/step - loss: 0.5618 - val_loss: 0.6009\n",
      "Epoch 59/60\n",
      "2541/2541 [==============================] - 1s 367us/step - loss: 0.5621 - val_loss: 0.6008\n",
      "Epoch 60/60\n",
      "2541/2541 [==============================] - 1s 383us/step - loss: 0.5607 - val_loss: 0.6009\n",
      "[13:36:29] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\learner.cc:328: \n",
      "Parameters: { single_precision_histogram } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/60\n",
      "2541/2541 [==============================] - 7s 3ms/step - loss: 0.5934 - val_loss: 0.5934\n",
      "Epoch 2/60\n",
      "2541/2541 [==============================] - 1s 339us/step - loss: 0.5839 - val_loss: 0.5594\n",
      "Epoch 3/60\n",
      "2541/2541 [==============================] - 1s 338us/step - loss: 0.5673 - val_loss: 0.5479\n",
      "Epoch 4/60\n",
      "2541/2541 [==============================] - 1s 338us/step - loss: 0.5659 - val_loss: 0.5568\n",
      "Epoch 5/60\n",
      "2541/2541 [==============================] - 1s 338us/step - loss: 0.5866 - val_loss: 0.6007\n",
      "Epoch 6/60\n",
      "2541/2541 [==============================] - 1s 354us/step - loss: 0.5810 - val_loss: 0.6005\n",
      "Epoch 7/60\n",
      "2541/2541 [==============================] - 2s 933us/step - loss: 0.5708 - val_loss: 0.6007\n",
      "Epoch 8/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5675 - val_loss: 0.5682\n",
      "Epoch 9/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5674 - val_loss: 0.5954\n",
      "Epoch 10/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5669 - val_loss: 0.5893\n",
      "Epoch 11/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5631 - val_loss: 0.5933\n",
      "Epoch 12/60\n",
      "2541/2541 [==============================] - 1s 343us/step - loss: 0.5589 - val_loss: 0.5652\n",
      "Epoch 13/60\n",
      "2541/2541 [==============================] - 1s 338us/step - loss: 0.5554 - val_loss: 0.5623\n",
      "Epoch 14/60\n",
      "2541/2541 [==============================] - 1s 335us/step - loss: 0.5513 - val_loss: 0.5527\n",
      "Epoch 15/60\n",
      "2541/2541 [==============================] - 1s 337us/step - loss: 0.5519 - val_loss: 0.5535\n",
      "Epoch 16/60\n",
      "2541/2541 [==============================] - 1s 358us/step - loss: 0.5408 - val_loss: 0.5559\n",
      "Epoch 17/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5431 - val_loss: 0.5760\n",
      "Epoch 18/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5491 - val_loss: 0.5697\n",
      "Epoch 19/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5445 - val_loss: 0.5604\n",
      "Epoch 20/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5365 - val_loss: 0.5526\n",
      "Epoch 21/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5325 - val_loss: 0.5583\n",
      "Epoch 22/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5516 - val_loss: 0.5461\n",
      "Epoch 23/60\n",
      "2541/2541 [==============================] - 2s 736us/step - loss: 0.5470 - val_loss: 0.5487\n",
      "Epoch 24/60\n",
      "2541/2541 [==============================] - 1s 337us/step - loss: 0.5426 - val_loss: 0.5459\n",
      "Epoch 25/60\n",
      "2541/2541 [==============================] - 1s 336us/step - loss: 0.5287 - val_loss: 0.5697\n",
      "Epoch 26/60\n",
      "2541/2541 [==============================] - 1s 336us/step - loss: 0.5251 - val_loss: 0.5428\n",
      "Epoch 27/60\n",
      "2541/2541 [==============================] - 1s 488us/step - loss: 0.5251 - val_loss: 0.5338\n",
      "Epoch 28/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5294 - val_loss: 0.5690\n",
      "Epoch 29/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5343 - val_loss: 0.5306\n",
      "Epoch 30/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5354 - val_loss: 0.5371\n",
      "Epoch 31/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5297 - val_loss: 0.5362\n",
      "Epoch 32/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5393 - val_loss: 0.5578\n",
      "Epoch 33/60\n",
      "2541/2541 [==============================] - 1s 548us/step - loss: 0.5425 - val_loss: 0.5507\n",
      "Epoch 34/60\n",
      "2541/2541 [==============================] - 1s 339us/step - loss: 0.5385 - val_loss: 0.6092\n",
      "Epoch 35/60\n",
      "2541/2541 [==============================] - 1s 340us/step - loss: 0.5181 - val_loss: 0.5448\n",
      "Epoch 36/60\n",
      "2541/2541 [==============================] - 1s 337us/step - loss: 0.5316 - val_loss: 0.5644\n",
      "Epoch 37/60\n",
      "2541/2541 [==============================] - 1s 339us/step - loss: 0.5406 - val_loss: 0.5614\n",
      "Epoch 38/60\n",
      "2541/2541 [==============================] - 1s 547us/step - loss: 0.5214 - val_loss: 0.5592\n",
      "Epoch 39/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5227 - val_loss: 0.5459\n",
      "Epoch 40/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5211 - val_loss: 0.5700\n",
      "Epoch 41/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5045 - val_loss: 0.5493\n",
      "Epoch 42/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5296 - val_loss: 0.5544\n",
      "Epoch 43/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5186 - val_loss: 0.5484\n",
      "Epoch 44/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5141 - val_loss: 0.5393\n",
      "Epoch 45/60\n",
      "2541/2541 [==============================] - 1s 337us/step - loss: 0.5110 - val_loss: 0.5352\n",
      "Epoch 46/60\n",
      "2541/2541 [==============================] - 1s 335us/step - loss: 0.4999 - val_loss: 0.5991\n",
      "Epoch 47/60\n",
      "2541/2541 [==============================] - 1s 340us/step - loss: 0.5111 - val_loss: 0.5475\n",
      "Epoch 48/60\n",
      "2541/2541 [==============================] - 1s 337us/step - loss: 0.5198 - val_loss: 0.5469\n",
      "Epoch 49/60\n",
      "2541/2541 [==============================] - 2s 908us/step - loss: 0.5052 - val_loss: 0.5334\n",
      "Epoch 50/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.4933 - val_loss: 0.5307\n",
      "Epoch 51/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.4847 - val_loss: 0.5305\n",
      "Epoch 52/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.4940 - val_loss: 0.5377\n",
      "Epoch 53/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.4896 - val_loss: 0.5263\n",
      "Epoch 54/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.4862 - val_loss: 0.5268\n",
      "Epoch 55/60\n",
      "2541/2541 [==============================] - 3s 997us/step - loss: 0.4715 - val_loss: 0.5171\n",
      "Epoch 56/60\n",
      "2541/2541 [==============================] - 1s 337us/step - loss: 0.4804 - val_loss: 0.5108\n",
      "Epoch 57/60\n",
      "2541/2541 [==============================] - 1s 336us/step - loss: 0.4776 - val_loss: 0.5177\n",
      "Epoch 58/60\n",
      "2541/2541 [==============================] - 1s 337us/step - loss: 0.4715 - val_loss: 0.5164\n",
      "Epoch 59/60\n",
      "2541/2541 [==============================] - 1s 337us/step - loss: 0.4880 - val_loss: 0.5219\n",
      "Epoch 60/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.4842 - val_loss: 0.5100\n",
      "[13:39:01] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\learner.cc:328: \n",
      "Parameters: { single_precision_histogram } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/60\n",
      "2541/2541 [==============================] - 8s 3ms/step - loss: 0.5901 - val_loss: 0.5912\n",
      "Epoch 2/60\n",
      "2541/2541 [==============================] - 1s 343us/step - loss: 0.5756 - val_loss: 0.5953\n",
      "Epoch 3/60\n",
      "2541/2541 [==============================] - 1s 343us/step - loss: 0.5684 - val_loss: 0.5718\n",
      "Epoch 4/60\n",
      "2541/2541 [==============================] - 1s 360us/step - loss: 0.5604 - val_loss: 0.5462\n",
      "Epoch 5/60\n",
      "2541/2541 [==============================] - 1s 340us/step - loss: 0.5838 - val_loss: 0.5999\n",
      "Epoch 6/60\n",
      "2541/2541 [==============================] - 1s 343us/step - loss: 0.5680 - val_loss: 0.5961\n",
      "Epoch 7/60\n",
      "2541/2541 [==============================] - 1s 342us/step - loss: 0.5618 - val_loss: 0.5734\n",
      "Epoch 8/60\n",
      "2541/2541 [==============================] - 2s 769us/step - loss: 0.5606 - val_loss: 0.5482\n",
      "Epoch 9/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5545 - val_loss: 0.5483\n",
      "Epoch 10/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5602 - val_loss: 0.5525\n",
      "Epoch 11/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5528 - val_loss: 0.5388\n",
      "Epoch 12/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5532 - val_loss: 0.5623\n",
      "Epoch 13/60\n",
      "2541/2541 [==============================] - 2s 716us/step - loss: 0.5525 - val_loss: 0.5565\n",
      "Epoch 14/60\n",
      "2541/2541 [==============================] - 1s 343us/step - loss: 0.5467 - val_loss: 0.5368\n",
      "Epoch 15/60\n",
      "2541/2541 [==============================] - 1s 342us/step - loss: 0.5502 - val_loss: 0.5400\n",
      "Epoch 16/60\n",
      "2541/2541 [==============================] - 1s 341us/step - loss: 0.5503 - val_loss: 0.5489\n",
      "Epoch 17/60\n",
      "2541/2541 [==============================] - 1s 343us/step - loss: 0.5427 - val_loss: 0.5529\n",
      "Epoch 18/60\n",
      "2541/2541 [==============================] - 1s 454us/step - loss: 0.5484 - val_loss: 0.5395\n",
      "Epoch 19/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5393 - val_loss: 0.5474\n",
      "Epoch 20/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5457 - val_loss: 0.5577\n",
      "Epoch 21/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5508 - val_loss: 0.5521\n",
      "Epoch 22/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5398 - val_loss: 0.5455\n",
      "Epoch 23/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5378 - val_loss: 0.5493\n",
      "Epoch 24/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5382 - val_loss: 0.5296\n",
      "Epoch 25/60\n",
      "2541/2541 [==============================] - 1s 398us/step - loss: 0.5409 - val_loss: 0.5475\n",
      "Epoch 26/60\n",
      "2541/2541 [==============================] - 1s 341us/step - loss: 0.5426 - val_loss: 0.5386\n",
      "Epoch 27/60\n",
      "2541/2541 [==============================] - 1s 341us/step - loss: 0.5307 - val_loss: 0.5268\n",
      "Epoch 28/60\n",
      "2541/2541 [==============================] - 1s 344us/step - loss: 0.5307 - val_loss: 0.5319\n",
      "Epoch 29/60\n",
      "2541/2541 [==============================] - 2s 956us/step - loss: 0.5396 - val_loss: 0.5274\n",
      "Epoch 30/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5350 - val_loss: 0.5916\n",
      "Epoch 31/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5364 - val_loss: 0.5337\n",
      "Epoch 32/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5312 - val_loss: 0.5300\n",
      "Epoch 33/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5195 - val_loss: 0.5254\n",
      "Epoch 34/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5249 - val_loss: 0.5290\n",
      "Epoch 35/60\n",
      "2541/2541 [==============================] - 4s 2ms/step - loss: 0.5299 - val_loss: 0.5403\n",
      "Epoch 36/60\n",
      "2541/2541 [==============================] - 1s 341us/step - loss: 0.5430 - val_loss: 0.5368\n",
      "Epoch 37/60\n",
      "2541/2541 [==============================] - 1s 344us/step - loss: 0.5221 - val_loss: 0.5232\n",
      "Epoch 38/60\n",
      "2541/2541 [==============================] - 1s 350us/step - loss: 0.5172 - val_loss: 0.5366\n",
      "Epoch 39/60\n",
      "2541/2541 [==============================] - 1s 360us/step - loss: 0.5214 - val_loss: 0.5286\n",
      "Epoch 40/60\n",
      "2541/2541 [==============================] - 1s 351us/step - loss: 0.5203 - val_loss: 0.5359\n",
      "Epoch 41/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5202 - val_loss: 0.5233\n",
      "Epoch 42/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5048 - val_loss: 0.5184\n",
      "Epoch 43/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5056 - val_loss: 0.5451\n",
      "Epoch 44/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5091 - val_loss: 0.5230\n",
      "Epoch 45/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5058 - val_loss: 0.5353\n",
      "Epoch 46/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5053 - val_loss: 0.5316\n",
      "Epoch 47/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5154 - val_loss: 0.5302\n",
      "Epoch 48/60\n",
      "2541/2541 [==============================] - 2s 843us/step - loss: 0.5118 - val_loss: 0.5484\n",
      "Epoch 49/60\n",
      "2541/2541 [==============================] - 1s 342us/step - loss: 0.5001 - val_loss: 0.5300\n",
      "Epoch 50/60\n",
      "2541/2541 [==============================] - 1s 342us/step - loss: 0.5093 - val_loss: 0.5113\n",
      "Epoch 51/60\n",
      "2541/2541 [==============================] - 1s 341us/step - loss: 0.4988 - val_loss: 0.5190\n",
      "Epoch 52/60\n",
      "2541/2541 [==============================] - 1s 342us/step - loss: 0.4940 - val_loss: 0.5289\n",
      "Epoch 53/60\n",
      "2541/2541 [==============================] - 1s 347us/step - loss: 0.5029 - val_loss: 0.5439\n",
      "Epoch 54/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5223 - val_loss: 0.5370\n",
      "Epoch 55/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5047 - val_loss: 0.5211\n",
      "Epoch 56/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5115 - val_loss: 0.5633\n",
      "Epoch 57/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.4882 - val_loss: 0.5328\n",
      "Epoch 58/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.4866 - val_loss: 0.5287\n",
      "Epoch 59/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.4858 - val_loss: 0.5320\n",
      "Epoch 60/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.4798 - val_loss: 0.5282\n",
      "[13:41:43] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\learner.cc:328: \n",
      "Parameters: { single_precision_histogram } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/60\n",
      "2541/2541 [==============================] - 9s 3ms/step - loss: 0.5927 - val_loss: 0.5983\n",
      "Epoch 2/60\n",
      "2541/2541 [==============================] - 1s 370us/step - loss: 0.5874 - val_loss: 0.5875\n",
      "Epoch 3/60\n",
      "2541/2541 [==============================] - 1s 348us/step - loss: 0.5754 - val_loss: 0.5481\n",
      "Epoch 4/60\n",
      "2541/2541 [==============================] - 1s 347us/step - loss: 0.5676 - val_loss: 0.5704\n",
      "Epoch 5/60\n",
      "2541/2541 [==============================] - 1s 348us/step - loss: 0.5651 - val_loss: 0.5521\n",
      "Epoch 6/60\n",
      "2541/2541 [==============================] - 1s 347us/step - loss: 0.5600 - val_loss: 0.5483\n",
      "Epoch 7/60\n",
      "2541/2541 [==============================] - 1s 348us/step - loss: 0.5599 - val_loss: 0.5501\n",
      "Epoch 8/60\n",
      "2541/2541 [==============================] - 1s 347us/step - loss: 0.5566 - val_loss: 0.5501\n",
      "Epoch 9/60\n",
      "2541/2541 [==============================] - 2s 739us/step - loss: 0.5575 - val_loss: 0.5554\n",
      "Epoch 10/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5600 - val_loss: 0.6097\n",
      "Epoch 11/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5636 - val_loss: 0.5834\n",
      "Epoch 12/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5604 - val_loss: 0.5448\n",
      "Epoch 13/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5529 - val_loss: 0.5280\n",
      "Epoch 14/60\n",
      "2541/2541 [==============================] - 2s 728us/step - loss: 0.5557 - val_loss: 0.5432\n",
      "Epoch 15/60\n",
      "2541/2541 [==============================] - 1s 349us/step - loss: 0.5495 - val_loss: 0.5508\n",
      "Epoch 16/60\n",
      "2541/2541 [==============================] - 1s 347us/step - loss: 0.5528 - val_loss: 0.5406\n",
      "Epoch 17/60\n",
      "2541/2541 [==============================] - 1s 348us/step - loss: 0.5488 - val_loss: 0.5408\n",
      "Epoch 18/60\n",
      "2541/2541 [==============================] - 2s 650us/step - loss: 0.5450 - val_loss: 0.5548\n",
      "Epoch 19/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5551 - val_loss: 0.5422\n",
      "Epoch 20/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5490 - val_loss: 0.5394\n",
      "Epoch 21/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5568 - val_loss: 0.5365\n",
      "Epoch 22/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5433 - val_loss: 0.5397\n",
      "Epoch 23/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5397 - val_loss: 0.5211\n",
      "Epoch 24/60\n",
      "2541/2541 [==============================] - 2s 649us/step - loss: 0.5427 - val_loss: 0.5122\n",
      "Epoch 25/60\n",
      "2541/2541 [==============================] - 1s 348us/step - loss: 0.5441 - val_loss: 0.5211\n",
      "Epoch 26/60\n",
      "2541/2541 [==============================] - 1s 349us/step - loss: 0.5467 - val_loss: 0.5298\n",
      "Epoch 27/60\n",
      "2541/2541 [==============================] - 1s 348us/step - loss: 0.5427 - val_loss: 0.5230\n",
      "Epoch 28/60\n",
      "2541/2541 [==============================] - 1s 370us/step - loss: 0.5439 - val_loss: 0.5211\n",
      "Epoch 29/60\n",
      "2541/2541 [==============================] - 2s 693us/step - loss: 0.5318 - val_loss: 0.5331\n",
      "Epoch 30/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5345 - val_loss: 0.5317\n",
      "Epoch 31/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5531 - val_loss: 0.5385\n",
      "Epoch 32/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5480 - val_loss: 0.5256\n",
      "Epoch 33/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5299 - val_loss: 0.5298\n",
      "Epoch 34/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5294 - val_loss: 0.5212\n",
      "Epoch 35/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5252 - val_loss: 0.5353\n",
      "Epoch 36/60\n",
      "2541/2541 [==============================] - 2s 724us/step - loss: 0.5281 - val_loss: 0.5226\n",
      "Epoch 37/60\n",
      "2541/2541 [==============================] - 1s 348us/step - loss: 0.5209 - val_loss: 0.5263\n",
      "Epoch 38/60\n",
      "2541/2541 [==============================] - 1s 349us/step - loss: 0.5247 - val_loss: 0.5140\n",
      "Epoch 39/60\n",
      "2541/2541 [==============================] - 1s 347us/step - loss: 0.5136 - val_loss: 0.5229\n",
      "Epoch 40/60\n",
      "2541/2541 [==============================] - 2s 650us/step - loss: 0.5083 - val_loss: 0.5264\n",
      "Epoch 41/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5136 - val_loss: 0.5248\n",
      "Epoch 42/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.4973 - val_loss: 0.5243\n",
      "Epoch 43/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5146 - val_loss: 0.5351\n",
      "Epoch 44/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5157 - val_loss: 0.5298\n",
      "Epoch 45/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5030 - val_loss: 0.5306\n",
      "Epoch 46/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5058 - val_loss: 0.5146\n",
      "Epoch 47/60\n",
      "2541/2541 [==============================] - 1s 471us/step - loss: 0.5090 - val_loss: 0.5149\n",
      "Epoch 48/60\n",
      "2541/2541 [==============================] - ETA: 0s - loss: 0.507 - 1s 350us/step - loss: 0.5067 - val_loss: 0.5432\n",
      "Epoch 49/60\n",
      "2541/2541 [==============================] - 1s 347us/step - loss: 0.5069 - val_loss: 0.5212\n",
      "Epoch 50/60\n",
      "2541/2541 [==============================] - 1s 348us/step - loss: 0.4902 - val_loss: 0.5220\n",
      "Epoch 51/60\n",
      "2541/2541 [==============================] - 1s 348us/step - loss: 0.5044 - val_loss: 0.5065\n",
      "Epoch 52/60\n",
      "2541/2541 [==============================] - 2s 783us/step - loss: 0.4836 - val_loss: 0.4893\n",
      "Epoch 53/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.4859 - val_loss: 0.4982\n",
      "Epoch 54/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.4839 - val_loss: 0.5037\n",
      "Epoch 55/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.4940 - val_loss: 0.5299\n",
      "Epoch 56/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.4843 - val_loss: 0.4783\n",
      "Epoch 57/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.4902 - val_loss: 0.4684\n",
      "Epoch 58/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.4909 - val_loss: 0.4720\n",
      "Epoch 59/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.4861 - val_loss: 0.4917\n",
      "Epoch 60/60\n",
      "2541/2541 [==============================] - 1s 453us/step - loss: 0.4713 - val_loss: 0.5030\n",
      "[13:44:25] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\learner.cc:328: \n",
      "Parameters: { single_precision_histogram } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2545 samples, validate on 505 samples\n",
      "Epoch 1/60\n",
      "2545/2545 [==============================] - 10s 4ms/step - loss: 0.5903 - val_loss: 0.5935\n",
      "Epoch 2/60\n",
      "2545/2545 [==============================] - 1s 356us/step - loss: 0.5756 - val_loss: 0.5842\n",
      "Epoch 3/60\n",
      "2545/2545 [==============================] - 1s 358us/step - loss: 0.5674 - val_loss: 0.5230\n",
      "Epoch 4/60\n",
      "2545/2545 [==============================] - 1s 357us/step - loss: 0.5666 - val_loss: 0.5360\n",
      "Epoch 5/60\n",
      "2545/2545 [==============================] - 1s 356us/step - loss: 0.5669 - val_loss: 0.5803\n",
      "Epoch 6/60\n",
      "2545/2545 [==============================] - 1s 357us/step - loss: 0.5675 - val_loss: 0.5726\n",
      "Epoch 7/60\n",
      "2545/2545 [==============================] - 1s 353us/step - loss: 0.5688 - val_loss: 0.5926\n",
      "Epoch 8/60\n",
      "2545/2545 [==============================] - 1s 356us/step - loss: 0.5629 - val_loss: 0.5993\n",
      "Epoch 9/60\n",
      "2545/2545 [==============================] - 1s 355us/step - loss: 0.5641 - val_loss: 0.5987\n",
      "Epoch 10/60\n",
      "2545/2545 [==============================] - 2s 683us/step - loss: 0.5654 - val_loss: 0.5998\n",
      "Epoch 11/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5665 - val_loss: 0.6003\n",
      "Epoch 12/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5654 - val_loss: 0.6005\n",
      "Epoch 13/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5665 - val_loss: 0.6002\n",
      "Epoch 14/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5639 - val_loss: 0.6000\n",
      "Epoch 15/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5641 - val_loss: 0.6000\n",
      "Epoch 16/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5637 - val_loss: 0.5995\n",
      "Epoch 17/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5600 - val_loss: 0.6001\n",
      "Epoch 18/60\n",
      "2545/2545 [==============================] - 1s 502us/step - loss: 0.5641 - val_loss: 0.6001\n",
      "Epoch 19/60\n",
      "2545/2545 [==============================] - 1s 357us/step - loss: 0.5626 - val_loss: 0.6002\n",
      "Epoch 20/60\n",
      "2545/2545 [==============================] - 1s 356us/step - loss: 0.5635 - val_loss: 0.6002\n",
      "Epoch 21/60\n",
      "2545/2545 [==============================] - 1s 357us/step - loss: 0.5634 - val_loss: 0.6001\n",
      "Epoch 22/60\n",
      "2545/2545 [==============================] - 1s 356us/step - loss: 0.5638 - val_loss: 0.6004\n",
      "Epoch 23/60\n",
      "2545/2545 [==============================] - 1s 376us/step - loss: 0.5637 - val_loss: 0.6000\n",
      "Epoch 24/60\n",
      "2545/2545 [==============================] - 2s 884us/step - loss: 0.5601 - val_loss: 0.5992\n",
      "Epoch 25/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5637 - val_loss: 0.6004\n",
      "Epoch 26/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5593 - val_loss: 0.6007\n",
      "Epoch 27/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5614 - val_loss: 0.6003\n",
      "Epoch 28/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5601 - val_loss: 0.6008\n",
      "Epoch 29/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5617 - val_loss: 0.6008\n",
      "Epoch 30/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5640 - val_loss: 0.6008\n",
      "Epoch 31/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5612 - val_loss: 0.6008\n",
      "Epoch 32/60\n",
      "2545/2545 [==============================] - 2s 898us/step - loss: 0.5613 - val_loss: 0.6007\n",
      "Epoch 33/60\n",
      "2545/2545 [==============================] - 1s 377us/step - loss: 0.5620 - val_loss: 0.6006\n",
      "Epoch 34/60\n",
      "2545/2545 [==============================] - 1s 355us/step - loss: 0.5607 - val_loss: 0.6007\n",
      "Epoch 35/60\n",
      "2545/2545 [==============================] - 1s 356us/step - loss: 0.5606 - val_loss: 0.6007\n",
      "Epoch 36/60\n",
      "2545/2545 [==============================] - 1s 357us/step - loss: 0.5628 - val_loss: 0.6006\n",
      "Epoch 37/60\n",
      "2545/2545 [==============================] - 1s 589us/step - loss: 0.5619 - val_loss: 0.6005\n",
      "Epoch 38/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5615 - val_loss: 0.6005\n",
      "Epoch 39/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5607 - val_loss: 0.6006\n",
      "Epoch 40/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5612 - val_loss: 0.6006\n",
      "Epoch 41/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5618 - val_loss: 0.6004\n",
      "Epoch 42/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5637 - val_loss: 0.6004\n",
      "Epoch 43/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5600 - val_loss: 0.6004\n",
      "Epoch 44/60\n",
      "2545/2545 [==============================] - 3s 1ms/step - loss: 0.5598 - val_loss: 0.6005\n",
      "Epoch 45/60\n",
      "2545/2545 [==============================] - 1s 354us/step - loss: 0.5605 - val_loss: 0.6006\n",
      "Epoch 46/60\n",
      "2545/2545 [==============================] - 1s 377us/step - loss: 0.5599 - val_loss: 0.6006\n",
      "Epoch 47/60\n",
      "2545/2545 [==============================] - 1s 355us/step - loss: 0.5620 - val_loss: 0.5994\n",
      "Epoch 48/60\n",
      "2545/2545 [==============================] - 1s 494us/step - loss: 0.5600 - val_loss: 0.5977\n",
      "Epoch 49/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5611 - val_loss: 0.5983\n",
      "Epoch 50/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5611 - val_loss: 0.6001\n",
      "Epoch 51/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5621 - val_loss: 0.6000\n",
      "Epoch 52/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5623 - val_loss: 0.6001\n",
      "Epoch 53/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5599 - val_loss: 0.6003\n",
      "Epoch 54/60\n",
      "2545/2545 [==============================] - 3s 1ms/step - loss: 0.5594 - val_loss: 0.6002\n",
      "Epoch 55/60\n",
      "2545/2545 [==============================] - 1s 353us/step - loss: 0.5614 - val_loss: 0.6003\n",
      "Epoch 56/60\n",
      "2545/2545 [==============================] - 1s 353us/step - loss: 0.5606 - val_loss: 0.6002\n",
      "Epoch 57/60\n",
      "2545/2545 [==============================] - 1s 356us/step - loss: 0.5608 - val_loss: 0.6000\n",
      "Epoch 58/60\n",
      "2545/2545 [==============================] - 1s 356us/step - loss: 0.5610 - val_loss: 0.6000\n",
      "Epoch 59/60\n",
      "2545/2545 [==============================] - 3s 1ms/step - loss: 0.5598 - val_loss: 0.6002\n",
      "Epoch 60/60\n",
      "2545/2545 [==============================] - 4s 1ms/step - loss: 0.5594 - val_loss: 0.6001\n",
      "[13:47:08] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\learner.cc:328: \n",
      "Parameters: { single_precision_histogram } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "training_metrics = {}\n",
    "validation_metrics = {}\n",
    "for i in range(len(training_list)):\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = class_GCN.dataframe_to_gcn_input(validation_list[i])\n",
    "        Y_cold = validation_list[i].Binary \n",
    "        Y_dummy_cold = np.empty((X_atoms_cold.shape[0],gcn_params['dense_size'][2]+1))\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = class_GCN.dataframe_to_gcn_input(training_list[i])\n",
    "        Y = training_list[i].Binary\n",
    "        Y_dummy_train = np.empty((X_atoms_train.shape[0],gcn_params['dense_size'][2]+1))\n",
    "        \n",
    "        gcn_encoder = class_GCN.build_encoder()\n",
    "        gcn_model = class_GCN.build_model(gcn_encoder)\n",
    "        gcn_mining = class_GCN.build_mining(gcn_model)\n",
    "        \n",
    "        gcn_mining.fit([X_atoms_train,X_bonds_train,X_edges_train,Y],\n",
    "                       Y_dummy_train,\n",
    "                       epochs = gcn_params['n_epochs'],\n",
    "                       batch_size = gcn_params['batch_size'],\n",
    "                       shuffle = True,\n",
    "                       validation_data = ([X_atoms_cold,X_bonds_cold,X_edges_cold,Y_cold],Y_dummy_cold)\n",
    "                      )\n",
    "        #Predict Embeddings\n",
    "        embeddings_cold = gcn_model.predict([X_atoms_cold,X_bonds_cold,X_edges_cold])\n",
    "        embeddings_train = gcn_model.predict([X_atoms_train, X_bonds_train, X_edges_train])\n",
    "        \n",
    "        #Prepare data for XGBoost\n",
    "        dmatrix_train = class_XGB.to_xgb_input(Y,embeddings_train)\n",
    "        dmatrix_cold = class_XGB.to_xgb_input(Y_cold,embeddings_cold)\n",
    "        \n",
    "        evalist = [(dmatrix_train,'train'),(dmatrix_cold,'eval')]\n",
    "        xgb_model = class_XGB.build_model(dmatrix_train,evalist,300)\n",
    "        \n",
    "        xgb_pred_cold = xgb_model.predict(dmatrix_cold)\n",
    "        validation_metrics['Val_%s'%i] = calculate_metrics(np.array(Y_cold),xgb_pred_cold)\n",
    "        \n",
    "        xgb_pred_train = xgb_model.predict(dmatrix_train)\n",
    "        training_metrics['Train_%s'%i] = calculate_metrics(np.array(Y),xgb_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train_0': {'roc_auc': 0.5,\n",
       "  'tn': 1292,\n",
       "  'fp': 0,\n",
       "  'fn': 1249,\n",
       "  'tp': 0,\n",
       "  'map': 0.491538764266037,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.508461235733963},\n",
       " 'Train_1': {'roc_auc': 0.5,\n",
       "  'tn': 1271,\n",
       "  'fp': 0,\n",
       "  'fn': 1270,\n",
       "  'tp': 0,\n",
       "  'map': 0.49980322707595437,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.5001967729240456},\n",
       " 'Train_2': {'roc_auc': 0.5,\n",
       "  'tn': 1303,\n",
       "  'fp': 0,\n",
       "  'fn': 1238,\n",
       "  'tp': 0,\n",
       "  'map': 0.48720975993703264,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.5127902400629674},\n",
       " 'Train_3': {'roc_auc': 0.5,\n",
       "  'tn': 1304,\n",
       "  'fp': 0,\n",
       "  'fn': 1237,\n",
       "  'tp': 0,\n",
       "  'map': 0.4868162140889414,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.5131837859110586},\n",
       " 'Train_4': {'roc_auc': 0.5,\n",
       "  'tn': 1338,\n",
       "  'fp': 0,\n",
       "  'fn': 1203,\n",
       "  'tp': 0,\n",
       "  'map': 0.4734356552538371,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.526564344746163},\n",
       " 'Train_5': {'roc_auc': 0.5,\n",
       "  'tn': 1322,\n",
       "  'fp': 0,\n",
       "  'fn': 1223,\n",
       "  'tp': 0,\n",
       "  'map': 0.4805500982318271,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.5194499017681729}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
