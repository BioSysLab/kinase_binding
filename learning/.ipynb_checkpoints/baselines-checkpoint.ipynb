{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import time\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "# import scikitplot as skplt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scikitplot as skplt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "from keras.layers import Dense, Dropout, Input, Lambda\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, accuracy_score, auc, auc, average_precision_score, average_precision_score, \\\n",
    "    confusion_matrix, confusion_matrix, pairwise_distances, precision_score, precision_score, recall_score, \\\n",
    "    recall_score, roc_auc_score, roc_auc_score, roc_curve, roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from tqdm import tqdm\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, Descriptors\n",
    "from rdkit.Chem import AllChem\n",
    "from data_analysis import calculate_metrics, get_rdkit_features\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ECFP4(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    mols = [Chem.MolFromSmiles(rdk) for rdk in df.rdkit]\n",
    "    ECFP = [AllChem.GetMorganFingerprintAsBitVect(x,2,nBits=2048) for x in mols]\n",
    "    a = np.array(ECFP)\n",
    "    ECFP_mols = a.astype(np.float32)\n",
    "    Y = np.array(df.Binary)\n",
    "    return(ECFP_mols, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_simple_nn(num_units=35, activation='relu', drop_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(Dense(num_units, activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines_map = {\n",
    "    'knn_clf': KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'svc': SVC(gamma='auto'),\n",
    "    'xgboost':\n",
    "        xgb.XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            booster='gbtree',\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            min_child_weight=12,\n",
    "            n_estimators=100,\n",
    "            subsample=0.95\n",
    "        ),\n",
    "    'simple-NN': get_keras_simple_nn()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change inputs with ECFP fingerprints and not panos features.\n",
    "def get_baselines_performance(df_train, df_val, use_only=None):\n",
    "    X_train, Y_train = get_ECFP4(df_train)\n",
    "    X_cold, Y_cold =  get_ECFP4(df_val)\n",
    "\n",
    "    if use_only is None:\n",
    "        use_only = baselines_map.keys()\n",
    "    metrics = {}\n",
    "    for name, model in baselines_map.items():\n",
    "        if name in use_only:\n",
    "            if name == 'simple-NN':\n",
    "                model.fit(X_train, Y_train, epochs=30, batch_size=32)\n",
    "            else:\n",
    "                model.fit(X_train, Y_train)\n",
    "\n",
    "            y_pred = model.predict(X_cold).squeeze()\n",
    "            y_true = Y_cold.squeeze()\n",
    "            metrics[name] = calculate_metrics(y_true, y_pred)\n",
    "\n",
    "    return pd.DataFrame(metrics).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1 = 'p38'\n",
    "base_path_1 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_1 = base_path_1+f'/data/{target_1}/data.csv'\n",
    "df_p38=pd.read_csv(data_fpath_1).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_1+f'/data/{target_1}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_p38 = dill.load(in_f)\n",
    "\n",
    "with open(base_path_1+f'/data/{target_1}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_p38 = dill.load(in_f)\n",
    "    \n",
    "target_2 = 'akt1'\n",
    "base_path_2 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_2 = base_path_2+f'/data/{target_2}/data.csv'\n",
    "df_akt1 = pd.read_csv(data_fpath_2).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_2+f'/data/{target_2}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_akt1 = dill.load(in_f)\n",
    "with open(base_path_2+f'/data/{target_2}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_akt1 = dill.load(in_f)\n",
    "    \n",
    "target_3 = 'pi3k'\n",
    "base_path_3 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_3 = base_path_3+f'/data/{target_3}/data.csv'\n",
    "df_pi3k = pd.read_csv(data_fpath_3).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_3+f'/data/{target_3}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_pi3k = dill.load(in_f)\n",
    "with open(base_path_3+f'/data/{target_3}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_pi3k = dill.load(in_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_p38 = [df_p38.loc[train_val_folds_p38[0][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[1][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[2][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[3][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[4][0]],\n",
    "                 df_p38.loc[train_val_folds_p38[5][0]],\n",
    "                 df_p38.loc[train_test_folds_p38[0]]\n",
    "                 ]\n",
    "validation_p38 = [df_p38.loc[train_val_folds_p38[0][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[1][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[2][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[3][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[4][1]],\n",
    "                   df_p38.loc[train_val_folds_p38[5][1]],\n",
    "                   df_p38.loc[train_test_folds_p38[1]]\n",
    "                   ]\n",
    "\n",
    "training_akt1 = [df_akt1.loc[train_val_folds_akt1[0][0]],\n",
    "                 df_akt1.loc[train_val_folds_akt1[1][0]],\n",
    "                 df_akt1.loc[train_val_folds_akt1[2][0]],\n",
    "                 df_akt1.loc[train_val_folds_akt1[3][0]],\n",
    "                 df_akt1.loc[train_val_folds_akt1[4][0]],\n",
    "                 df_akt1.loc[train_val_folds_akt1[5][0]],\n",
    "                 df_akt1.loc[train_test_folds_akt1[0]]\n",
    "                 ]\n",
    "validation_akt1 = [df_akt1.loc[train_val_folds_akt1[0][1]],\n",
    "                   df_akt1.loc[train_val_folds_akt1[1][1]],\n",
    "                   df_akt1.loc[train_val_folds_akt1[2][1]],\n",
    "                   df_akt1.loc[train_val_folds_akt1[3][1]],\n",
    "                   df_akt1.loc[train_val_folds_akt1[4][1]],\n",
    "                   df_akt1.loc[train_val_folds_akt1[5][1]],\n",
    "                   df_akt1.loc[train_test_folds_akt1[1]]\n",
    "                   ]\n",
    "\n",
    "training_pi3k = [df_pi3k.loc[train_val_folds_pi3k[0][0]],\n",
    "                 df_pi3k.loc[train_val_folds_pi3k[1][0]],\n",
    "                 df_pi3k.loc[train_val_folds_pi3k[2][0]],\n",
    "                 df_pi3k.loc[train_val_folds_pi3k[3][0]],\n",
    "                 df_pi3k.loc[train_val_folds_pi3k[4][0]],\n",
    "                 df_pi3k.loc[train_val_folds_pi3k[5][0]],\n",
    "                 df_pi3k.loc[train_test_folds_pi3k[0]]\n",
    "                 ]\n",
    "validation_pi3k = [df_pi3k.loc[train_val_folds_pi3k[0][1]],\n",
    "                   df_pi3k.loc[train_val_folds_pi3k[1][1]],\n",
    "                   df_pi3k.loc[train_val_folds_pi3k[2][1]],\n",
    "                   df_pi3k.loc[train_val_folds_pi3k[3][1]],\n",
    "                   df_pi3k.loc[train_val_folds_pi3k[4][1]],\n",
    "                   df_pi3k.loc[train_val_folds_pi3k[5][1]],\n",
    "                   df_pi3k.loc[train_test_folds_pi3k[1]]\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2680/2680 [==============================] - 0s 169us/step - loss: 1.0222 - acc: 0.6507\n",
      "Epoch 2/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.5050 - acc: 0.7187\n",
      "Epoch 3/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.4346 - acc: 0.7571\n",
      "Epoch 4/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.3803 - acc: 0.8015\n",
      "Epoch 5/30\n",
      "2680/2680 [==============================] - 0s 149us/step - loss: 0.3444 - acc: 0.8187\n",
      "Epoch 6/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.3222 - acc: 0.8354\n",
      "Epoch 7/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.2967 - acc: 0.8384\n",
      "Epoch 8/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.2746 - acc: 0.8582\n",
      "Epoch 9/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.2486 - acc: 0.8619\n",
      "Epoch 10/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.2346 - acc: 0.8750\n",
      "Epoch 11/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.2353 - acc: 0.8716\n",
      "Epoch 12/30\n",
      "2680/2680 [==============================] - 0s 141us/step - loss: 0.2190 - acc: 0.8776\n",
      "Epoch 13/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.2079 - acc: 0.8836\n",
      "Epoch 14/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.2055 - acc: 0.8832\n",
      "Epoch 15/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.1958 - acc: 0.8922\n",
      "Epoch 16/30\n",
      "2680/2680 [==============================] - 0s 149us/step - loss: 0.1821 - acc: 0.8955\n",
      "Epoch 17/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.1925 - acc: 0.8851\n",
      "Epoch 18/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.1798 - acc: 0.8925\n",
      "Epoch 19/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.1891 - acc: 0.8907\n",
      "Epoch 20/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.1769 - acc: 0.8937\n",
      "Epoch 21/30\n",
      "2680/2680 [==============================] - 0s 142us/step - loss: 0.1661 - acc: 0.9022\n",
      "Epoch 22/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.1717 - acc: 0.9030\n",
      "Epoch 23/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.1693 - acc: 0.8981\n",
      "Epoch 24/30\n",
      "2680/2680 [==============================] - 0s 145us/step - loss: 0.1530 - acc: 0.9101\n",
      "Epoch 25/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.1600 - acc: 0.9060\n",
      "Epoch 26/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.1658 - acc: 0.9022\n",
      "Epoch 27/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.1588 - acc: 0.8989\n",
      "Epoch 28/30\n",
      "2680/2680 [==============================] - 0s 140us/step - loss: 0.1508 - acc: 0.9138\n",
      "Epoch 29/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.1585 - acc: 0.9078\n",
      "Epoch 30/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.1611 - acc: 0.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2680/2680 [==============================] - 0s 162us/step - loss: 0.3943 - acc: 0.8437\n",
      "Epoch 2/30\n",
      "2680/2680 [==============================] - 0s 143us/step - loss: 0.2539 - acc: 0.8735\n",
      "Epoch 3/30\n",
      "2680/2680 [==============================] - 0s 141us/step - loss: 0.2274 - acc: 0.8873 0s - loss: 0.2395 - acc: 0\n",
      "Epoch 4/30\n",
      "2680/2680 [==============================] - 0s 151us/step - loss: 0.2168 - acc: 0.8858\n",
      "Epoch 5/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.2022 - acc: 0.8951\n",
      "Epoch 6/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.1967 - acc: 0.8970\n",
      "Epoch 7/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.1841 - acc: 0.8989\n",
      "Epoch 8/30\n",
      "2680/2680 [==============================] - 0s 143us/step - loss: 0.1868 - acc: 0.8985 0s - loss: 0.1873 - acc: 0.\n",
      "Epoch 9/30\n",
      "2680/2680 [==============================] - 0s 153us/step - loss: 0.1822 - acc: 0.9022\n",
      "Epoch 10/30\n",
      "2680/2680 [==============================] - 0s 177us/step - loss: 0.1753 - acc: 0.9011\n",
      "Epoch 11/30\n",
      "2680/2680 [==============================] - 0s 162us/step - loss: 0.1681 - acc: 0.9093\n",
      "Epoch 12/30\n",
      "2680/2680 [==============================] - 0s 152us/step - loss: 0.1684 - acc: 0.9049\n",
      "Epoch 13/30\n",
      "2680/2680 [==============================] - 0s 155us/step - loss: 0.1628 - acc: 0.9093\n",
      "Epoch 14/30\n",
      "2680/2680 [==============================] - 0s 166us/step - loss: 0.1564 - acc: 0.9131\n",
      "Epoch 15/30\n",
      "2680/2680 [==============================] - 0s 162us/step - loss: 0.1586 - acc: 0.9134\n",
      "Epoch 16/30\n",
      "2680/2680 [==============================] - 0s 154us/step - loss: 0.1511 - acc: 0.9138\n",
      "Epoch 17/30\n",
      "2680/2680 [==============================] - 0s 170us/step - loss: 0.1568 - acc: 0.9142\n",
      "Epoch 18/30\n",
      "2680/2680 [==============================] - 0s 169us/step - loss: 0.1446 - acc: 0.9187\n",
      "Epoch 19/30\n",
      "2680/2680 [==============================] - 0s 165us/step - loss: 0.1459 - acc: 0.9172\n",
      "Epoch 20/30\n",
      "2680/2680 [==============================] - 0s 173us/step - loss: 0.1423 - acc: 0.9250\n",
      "Epoch 21/30\n",
      "2680/2680 [==============================] - 0s 169us/step - loss: 0.1494 - acc: 0.9164\n",
      "Epoch 22/30\n",
      "2680/2680 [==============================] - 0s 184us/step - loss: 0.1467 - acc: 0.9146\n",
      "Epoch 23/30\n",
      "2680/2680 [==============================] - 0s 169us/step - loss: 0.1467 - acc: 0.9146\n",
      "Epoch 24/30\n",
      "2680/2680 [==============================] - 0s 153us/step - loss: 0.1440 - acc: 0.9198\n",
      "Epoch 25/30\n",
      "2680/2680 [==============================] - 0s 150us/step - loss: 0.1331 - acc: 0.9276\n",
      "Epoch 26/30\n",
      "2680/2680 [==============================] - 0s 147us/step - loss: 0.1445 - acc: 0.9157\n",
      "Epoch 27/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.1442 - acc: 0.9146\n",
      "Epoch 28/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.1416 - acc: 0.9190\n",
      "Epoch 29/30\n",
      "2680/2680 [==============================] - 0s 150us/step - loss: 0.1409 - acc: 0.9190\n",
      "Epoch 30/30\n",
      "2680/2680 [==============================] - 0s 159us/step - loss: 0.1361 - acc: 0.9213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2680/2680 [==============================] - 0s 165us/step - loss: 0.1590 - acc: 0.9134\n",
      "Epoch 2/30\n",
      "2680/2680 [==============================] - 0s 149us/step - loss: 0.1430 - acc: 0.9220\n",
      "Epoch 3/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.1476 - acc: 0.9097\n",
      "Epoch 4/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.1366 - acc: 0.9187\n",
      "Epoch 5/30\n",
      "2680/2680 [==============================] - 0s 143us/step - loss: 0.1339 - acc: 0.9205\n",
      "Epoch 6/30\n",
      "2680/2680 [==============================] - 0s 151us/step - loss: 0.1401 - acc: 0.9194\n",
      "Epoch 7/30\n",
      "2680/2680 [==============================] - 0s 138us/step - loss: 0.1303 - acc: 0.9250\n",
      "Epoch 8/30\n",
      "2680/2680 [==============================] - 0s 147us/step - loss: 0.1337 - acc: 0.9209\n",
      "Epoch 9/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.1253 - acc: 0.9239\n",
      "Epoch 10/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.1254 - acc: 0.9265\n",
      "Epoch 11/30\n",
      "2680/2680 [==============================] - 0s 145us/step - loss: 0.1209 - acc: 0.9254\n",
      "Epoch 12/30\n",
      "2680/2680 [==============================] - 0s 141us/step - loss: 0.1307 - acc: 0.9231\n",
      "Epoch 13/30\n",
      "2680/2680 [==============================] - 0s 142us/step - loss: 0.1250 - acc: 0.9261\n",
      "Epoch 14/30\n",
      "2680/2680 [==============================] - 0s 145us/step - loss: 0.1296 - acc: 0.9183\n",
      "Epoch 15/30\n",
      "2680/2680 [==============================] - 0s 143us/step - loss: 0.1295 - acc: 0.9265\n",
      "Epoch 16/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.1305 - acc: 0.9213\n",
      "Epoch 17/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.1239 - acc: 0.9216\n",
      "Epoch 18/30\n",
      "2680/2680 [==============================] - 0s 145us/step - loss: 0.1308 - acc: 0.9213\n",
      "Epoch 19/30\n",
      "2680/2680 [==============================] - 0s 150us/step - loss: 0.1235 - acc: 0.9194\n",
      "Epoch 20/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.1197 - acc: 0.9321\n",
      "Epoch 21/30\n",
      "2680/2680 [==============================] - 0s 147us/step - loss: 0.1195 - acc: 0.9317\n",
      "Epoch 22/30\n",
      "2680/2680 [==============================] - 0s 147us/step - loss: 0.1237 - acc: 0.9276\n",
      "Epoch 23/30\n",
      "2680/2680 [==============================] - 0s 139us/step - loss: 0.1279 - acc: 0.9272\n",
      "Epoch 24/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.1137 - acc: 0.9321\n",
      "Epoch 25/30\n",
      "2680/2680 [==============================] - 0s 142us/step - loss: 0.1155 - acc: 0.9343\n",
      "Epoch 26/30\n",
      "2680/2680 [==============================] - 0s 142us/step - loss: 0.1167 - acc: 0.9310\n",
      "Epoch 27/30\n",
      "2680/2680 [==============================] - 0s 145us/step - loss: 0.1079 - acc: 0.9369\n",
      "Epoch 28/30\n",
      "2680/2680 [==============================] - 0s 147us/step - loss: 0.1255 - acc: 0.9254\n",
      "Epoch 29/30\n",
      "2680/2680 [==============================] - 0s 143us/step - loss: 0.1159 - acc: 0.9284\n",
      "Epoch 30/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.1101 - acc: 0.9313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2680/2680 [==============================] - 0s 172us/step - loss: 0.1367 - acc: 0.9243\n",
      "Epoch 2/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.1317 - acc: 0.9272\n",
      "Epoch 3/30\n",
      "2680/2680 [==============================] - 0s 145us/step - loss: 0.1326 - acc: 0.9257\n",
      "Epoch 4/30\n",
      "2680/2680 [==============================] - 0s 150us/step - loss: 0.1179 - acc: 0.9366\n",
      "Epoch 5/30\n",
      "2680/2680 [==============================] - 0s 147us/step - loss: 0.1215 - acc: 0.9332\n",
      "Epoch 6/30\n",
      "2680/2680 [==============================] - 0s 147us/step - loss: 0.1159 - acc: 0.9373\n",
      "Epoch 7/30\n",
      "2680/2680 [==============================] - 0s 139us/step - loss: 0.1198 - acc: 0.9332\n",
      "Epoch 8/30\n",
      "2680/2680 [==============================] - 0s 137us/step - loss: 0.1180 - acc: 0.9347\n",
      "Epoch 9/30\n",
      "2680/2680 [==============================] - 0s 147us/step - loss: 0.1163 - acc: 0.9347\n",
      "Epoch 10/30\n",
      "2680/2680 [==============================] - 0s 145us/step - loss: 0.1147 - acc: 0.9369\n",
      "Epoch 11/30\n",
      "2680/2680 [==============================] - 0s 142us/step - loss: 0.1104 - acc: 0.9384\n",
      "Epoch 12/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.1154 - acc: 0.9354\n",
      "Epoch 13/30\n",
      "2680/2680 [==============================] - 0s 142us/step - loss: 0.1188 - acc: 0.9354\n",
      "Epoch 14/30\n",
      "2680/2680 [==============================] - 0s 143us/step - loss: 0.1140 - acc: 0.9373\n",
      "Epoch 15/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.1064 - acc: 0.9425\n",
      "Epoch 16/30\n",
      "2680/2680 [==============================] - 0s 138us/step - loss: 0.1142 - acc: 0.9351\n",
      "Epoch 17/30\n",
      "2680/2680 [==============================] - 0s 153us/step - loss: 0.1162 - acc: 0.9358\n",
      "Epoch 18/30\n",
      "2680/2680 [==============================] - 0s 140us/step - loss: 0.1110 - acc: 0.9399\n",
      "Epoch 19/30\n",
      "2680/2680 [==============================] - 0s 154us/step - loss: 0.1136 - acc: 0.9351\n",
      "Epoch 20/30\n",
      "2680/2680 [==============================] - 0s 142us/step - loss: 0.1074 - acc: 0.9369\n",
      "Epoch 21/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.1075 - acc: 0.9369\n",
      "Epoch 22/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.1064 - acc: 0.9403\n",
      "Epoch 23/30\n",
      "2680/2680 [==============================] - 0s 140us/step - loss: 0.1033 - acc: 0.9399\n",
      "Epoch 24/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.1048 - acc: 0.9396\n",
      "Epoch 25/30\n",
      "2680/2680 [==============================] - 0s 143us/step - loss: 0.1098 - acc: 0.9388\n",
      "Epoch 26/30\n",
      "2680/2680 [==============================] - 0s 147us/step - loss: 0.1020 - acc: 0.9429\n",
      "Epoch 27/30\n",
      "2680/2680 [==============================] - 0s 153us/step - loss: 0.1046 - acc: 0.9403\n",
      "Epoch 28/30\n",
      "2680/2680 [==============================] - 0s 139us/step - loss: 0.1085 - acc: 0.9392\n",
      "Epoch 29/30\n",
      "2680/2680 [==============================] - 0s 143us/step - loss: 0.1041 - acc: 0.9422\n",
      "Epoch 30/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.1013 - acc: 0.9422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2680/2680 [==============================] - 0s 168us/step - loss: 0.1298 - acc: 0.9276\n",
      "Epoch 2/30\n",
      "2680/2680 [==============================] - 0s 147us/step - loss: 0.1144 - acc: 0.9291\n",
      "Epoch 3/30\n",
      "2680/2680 [==============================] - 0s 142us/step - loss: 0.1082 - acc: 0.9388\n",
      "Epoch 4/30\n",
      "2680/2680 [==============================] - 0s 137us/step - loss: 0.1146 - acc: 0.9291\n",
      "Epoch 5/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.1121 - acc: 0.9358\n",
      "Epoch 6/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.1104 - acc: 0.9369\n",
      "Epoch 7/30\n",
      "2680/2680 [==============================] - 0s 142us/step - loss: 0.1064 - acc: 0.9392\n",
      "Epoch 8/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.1033 - acc: 0.9399\n",
      "Epoch 9/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.1041 - acc: 0.9388\n",
      "Epoch 10/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.1076 - acc: 0.9425 0s - loss: 0.0960 - acc: 0\n",
      "Epoch 11/30\n",
      "2680/2680 [==============================] - 0s 141us/step - loss: 0.1079 - acc: 0.9377\n",
      "Epoch 12/30\n",
      "2680/2680 [==============================] - 0s 139us/step - loss: 0.1077 - acc: 0.9410\n",
      "Epoch 13/30\n",
      "2680/2680 [==============================] - 0s 159us/step - loss: 0.0982 - acc: 0.9429\n",
      "Epoch 14/30\n",
      "2680/2680 [==============================] - 0s 164us/step - loss: 0.1093 - acc: 0.9384\n",
      "Epoch 15/30\n",
      "2680/2680 [==============================] - 0s 169us/step - loss: 0.1049 - acc: 0.9399\n",
      "Epoch 16/30\n",
      "2680/2680 [==============================] - 0s 166us/step - loss: 0.1035 - acc: 0.9377\n",
      "Epoch 17/30\n",
      "2680/2680 [==============================] - 0s 165us/step - loss: 0.1012 - acc: 0.9414\n",
      "Epoch 18/30\n",
      "2680/2680 [==============================] - 0s 156us/step - loss: 0.1028 - acc: 0.9396\n",
      "Epoch 19/30\n",
      "2680/2680 [==============================] - ETA: 0s - loss: 0.0947 - acc: 0.942 - 0s 148us/step - loss: 0.0949 - acc: 0.9425\n",
      "Epoch 20/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.0976 - acc: 0.9433\n",
      "Epoch 21/30\n",
      "2680/2680 [==============================] - 0s 151us/step - loss: 0.1106 - acc: 0.9377\n",
      "Epoch 22/30\n",
      "2680/2680 [==============================] - 0s 153us/step - loss: 0.1020 - acc: 0.9399\n",
      "Epoch 23/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.0987 - acc: 0.9418\n",
      "Epoch 24/30\n",
      "2680/2680 [==============================] - 0s 147us/step - loss: 0.0961 - acc: 0.9466\n",
      "Epoch 25/30\n",
      "2680/2680 [==============================] - 0s 154us/step - loss: 0.0944 - acc: 0.9455\n",
      "Epoch 26/30\n",
      "2680/2680 [==============================] - 0s 148us/step - loss: 0.0969 - acc: 0.9425\n",
      "Epoch 27/30\n",
      "2680/2680 [==============================] - 0s 150us/step - loss: 0.1011 - acc: 0.9410\n",
      "Epoch 28/30\n",
      "2680/2680 [==============================] - 0s 144us/step - loss: 0.0943 - acc: 0.9455\n",
      "Epoch 29/30\n",
      "2680/2680 [==============================] - 0s 143us/step - loss: 0.0965 - acc: 0.9451\n",
      "Epoch 30/30\n",
      "2680/2680 [==============================] - 0s 146us/step - loss: 0.1015 - acc: 0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2685/2685 [==============================] - 0s 165us/step - loss: 0.1104 - acc: 0.9430\n",
      "Epoch 2/30\n",
      "2685/2685 [==============================] - 0s 141us/step - loss: 0.0977 - acc: 0.9445\n",
      "Epoch 3/30\n",
      "2685/2685 [==============================] - 0s 143us/step - loss: 0.1034 - acc: 0.9460\n",
      "Epoch 4/30\n",
      "2685/2685 [==============================] - 0s 143us/step - loss: 0.0965 - acc: 0.9453\n",
      "Epoch 5/30\n",
      "2685/2685 [==============================] - 0s 142us/step - loss: 0.0961 - acc: 0.9441\n",
      "Epoch 6/30\n",
      "2685/2685 [==============================] - 0s 139us/step - loss: 0.0905 - acc: 0.9497\n",
      "Epoch 7/30\n",
      "2685/2685 [==============================] - 0s 144us/step - loss: 0.0991 - acc: 0.9441\n",
      "Epoch 8/30\n",
      "2685/2685 [==============================] - 0s 150us/step - loss: 0.0994 - acc: 0.9426\n",
      "Epoch 9/30\n",
      "2685/2685 [==============================] - 0s 144us/step - loss: 0.1060 - acc: 0.9382\n",
      "Epoch 10/30\n",
      "2685/2685 [==============================] - 0s 145us/step - loss: 0.0935 - acc: 0.9464\n",
      "Epoch 11/30\n",
      "2685/2685 [==============================] - 0s 152us/step - loss: 0.0931 - acc: 0.9493\n",
      "Epoch 12/30\n",
      "2685/2685 [==============================] - 0s 145us/step - loss: 0.0982 - acc: 0.9482\n",
      "Epoch 13/30\n",
      "2685/2685 [==============================] - 0s 143us/step - loss: 0.1060 - acc: 0.9423\n",
      "Epoch 14/30\n",
      "2685/2685 [==============================] - 0s 142us/step - loss: 0.0968 - acc: 0.9467\n",
      "Epoch 15/30\n",
      "2685/2685 [==============================] - 0s 144us/step - loss: 0.0931 - acc: 0.9456\n",
      "Epoch 16/30\n",
      "2685/2685 [==============================] - 0s 143us/step - loss: 0.0917 - acc: 0.9497\n",
      "Epoch 17/30\n",
      "2685/2685 [==============================] - 0s 145us/step - loss: 0.0925 - acc: 0.9479\n",
      "Epoch 18/30\n",
      "2685/2685 [==============================] - 0s 136us/step - loss: 0.1013 - acc: 0.9464\n",
      "Epoch 19/30\n",
      "2685/2685 [==============================] - 0s 148us/step - loss: 0.0950 - acc: 0.9479\n",
      "Epoch 20/30\n",
      "2685/2685 [==============================] - 0s 141us/step - loss: 0.0948 - acc: 0.9471\n",
      "Epoch 21/30\n",
      "2685/2685 [==============================] - 0s 144us/step - loss: 0.0998 - acc: 0.9445\n",
      "Epoch 22/30\n",
      "2685/2685 [==============================] - 0s 150us/step - loss: 0.0903 - acc: 0.9501\n",
      "Epoch 23/30\n",
      "2685/2685 [==============================] - 0s 146us/step - loss: 0.0957 - acc: 0.9475\n",
      "Epoch 24/30\n",
      "2685/2685 [==============================] - 0s 145us/step - loss: 0.0911 - acc: 0.9512\n",
      "Epoch 25/30\n",
      "2685/2685 [==============================] - 0s 140us/step - loss: 0.0852 - acc: 0.9527\n",
      "Epoch 26/30\n",
      "2685/2685 [==============================] - 0s 139us/step - loss: 0.0870 - acc: 0.9531\n",
      "Epoch 27/30\n",
      "2685/2685 [==============================] - 0s 151us/step - loss: 0.0917 - acc: 0.9479\n",
      "Epoch 28/30\n",
      "2685/2685 [==============================] - 0s 141us/step - loss: 0.0873 - acc: 0.9516\n",
      "Epoch 29/30\n",
      "2685/2685 [==============================] - 0s 154us/step - loss: 0.0864 - acc: 0.9512\n",
      "Epoch 30/30\n",
      "2685/2685 [==============================] - 0s 142us/step - loss: 0.0902 - acc: 0.9486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3217/3217 [==============================] - 1s 162us/step - loss: 0.1058 - acc: 0.9447\n",
      "Epoch 2/30\n",
      "3217/3217 [==============================] - 0s 144us/step - loss: 0.0922 - acc: 0.9484\n",
      "Epoch 3/30\n",
      "3217/3217 [==============================] - 0s 144us/step - loss: 0.0948 - acc: 0.9453\n",
      "Epoch 4/30\n",
      "3217/3217 [==============================] - 0s 143us/step - loss: 0.0887 - acc: 0.9490\n",
      "Epoch 5/30\n",
      "3217/3217 [==============================] - 0s 143us/step - loss: 0.0946 - acc: 0.9468\n",
      "Epoch 6/30\n",
      "3217/3217 [==============================] - 0s 143us/step - loss: 0.0895 - acc: 0.9481\n",
      "Epoch 7/30\n",
      "3217/3217 [==============================] - 0s 144us/step - loss: 0.0908 - acc: 0.9462\n",
      "Epoch 8/30\n",
      "3217/3217 [==============================] - 0s 143us/step - loss: 0.0899 - acc: 0.9472\n",
      "Epoch 9/30\n",
      "3217/3217 [==============================] - 0s 147us/step - loss: 0.0891 - acc: 0.9487\n",
      "Epoch 10/30\n",
      "3217/3217 [==============================] - 0s 148us/step - loss: 0.0829 - acc: 0.9528\n",
      "Epoch 11/30\n",
      "3217/3217 [==============================] - 0s 149us/step - loss: 0.0834 - acc: 0.9546\n",
      "Epoch 12/30\n",
      "3217/3217 [==============================] - 0s 145us/step - loss: 0.0848 - acc: 0.9540\n",
      "Epoch 13/30\n",
      "3217/3217 [==============================] - 0s 141us/step - loss: 0.0890 - acc: 0.9512\n",
      "Epoch 14/30\n",
      "3217/3217 [==============================] - 0s 141us/step - loss: 0.0846 - acc: 0.9503\n",
      "Epoch 15/30\n",
      "3217/3217 [==============================] - 0s 142us/step - loss: 0.0880 - acc: 0.9490\n",
      "Epoch 16/30\n",
      "3217/3217 [==============================] - 0s 145us/step - loss: 0.0872 - acc: 0.9531\n",
      "Epoch 17/30\n",
      "3217/3217 [==============================] - 0s 143us/step - loss: 0.0886 - acc: 0.9496\n",
      "Epoch 18/30\n",
      "3217/3217 [==============================] - 0s 142us/step - loss: 0.0878 - acc: 0.9481\n",
      "Epoch 19/30\n",
      "3217/3217 [==============================] - 0s 148us/step - loss: 0.0899 - acc: 0.9487\n",
      "Epoch 20/30\n",
      "3217/3217 [==============================] - 0s 143us/step - loss: 0.0941 - acc: 0.9425\n",
      "Epoch 21/30\n",
      "3217/3217 [==============================] - 0s 144us/step - loss: 0.0848 - acc: 0.9506\n",
      "Epoch 22/30\n",
      "3217/3217 [==============================] - 0s 142us/step - loss: 0.0909 - acc: 0.9468\n",
      "Epoch 23/30\n",
      "3217/3217 [==============================] - 0s 145us/step - loss: 0.0896 - acc: 0.9503\n",
      "Epoch 24/30\n",
      "3217/3217 [==============================] - 0s 144us/step - loss: 0.0874 - acc: 0.9478\n",
      "Epoch 25/30\n",
      "3217/3217 [==============================] - 0s 152us/step - loss: 0.0886 - acc: 0.9515\n",
      "Epoch 26/30\n",
      "3217/3217 [==============================] - 0s 147us/step - loss: 0.0803 - acc: 0.9549\n",
      "Epoch 27/30\n",
      "3217/3217 [==============================] - 0s 144us/step - loss: 0.0901 - acc: 0.9487\n",
      "Epoch 28/30\n",
      "3217/3217 [==============================] - 0s 146us/step - loss: 0.0830 - acc: 0.9518\n",
      "Epoch 29/30\n",
      "3217/3217 [==============================] - 0s 148us/step - loss: 0.0896 - acc: 0.9487\n",
      "Epoch 30/30\n",
      "3217/3217 [==============================] - 0s 149us/step - loss: 0.0862 - acc: 0.9509\n"
     ]
    }
   ],
   "source": [
    "metrics_all = list()\n",
    "for i in range(len(training_pi3k)):\n",
    "    metrics_all.append(get_baselines_performance(training_pi3k[i],validation_pi3k[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      roc_auc     tn    fp     fn     tp       map  precision  \\\n",
      "knn_clf              0.695398  282.0  47.0   97.0  111.0  0.555542   0.702532   \n",
      "random_forest        0.742372  305.0  24.0   92.0  116.0  0.633410   0.828571   \n",
      "logistic_regression  0.702361  285.0  44.0   96.0  112.0  0.565359   0.717949   \n",
      "svc                  0.541114  326.0   3.0  189.0   19.0  0.430845   0.863636   \n",
      "xgboost              0.692885  293.0  36.0  105.0  103.0  0.562472   0.741007   \n",
      "simple-NN            0.709076  268.0  61.0  109.0   99.0  0.625345   0.618750   \n",
      "\n",
      "                       recall  accuracy  \n",
      "knn_clf              0.533654  0.731844  \n",
      "random_forest        0.557692  0.783985  \n",
      "logistic_regression  0.538462  0.739292  \n",
      "svc                  0.091346  0.642458  \n",
      "xgboost              0.495192  0.737430  \n",
      "simple-NN            0.475962  0.683426  \n",
      "                      roc_auc     tn    fp     fn     tp       map  precision  \\\n",
      "knn_clf              0.734324  308.0  59.0   63.0  107.0  0.523024   0.644578   \n",
      "random_forest        0.782441  339.0  28.0   61.0  109.0  0.623727   0.795620   \n",
      "logistic_regression  0.765163  322.0  45.0   59.0  111.0  0.574462   0.711538   \n",
      "svc                  0.572383  364.0   3.0  144.0   26.0  0.405276   0.896552   \n",
      "xgboost              0.774114  348.0  19.0   68.0  102.0  0.632415   0.842975   \n",
      "simple-NN            0.976527  345.0  22.0   24.0  146.0  0.950077   0.869048   \n",
      "\n",
      "                       recall  accuracy  \n",
      "knn_clf              0.629412  0.772812  \n",
      "random_forest        0.641176  0.834264  \n",
      "logistic_regression  0.652941  0.806331  \n",
      "svc                  0.152941  0.726257  \n",
      "xgboost              0.600000  0.837989  \n",
      "simple-NN            0.858824  0.914339  \n",
      "                      roc_auc     tn    fp     fn     tp       map  precision  \\\n",
      "knn_clf              0.772616  279.0  60.0   55.0  143.0  0.611178   0.704433   \n",
      "random_forest        0.788035  310.0  29.0   67.0  131.0  0.666465   0.818750   \n",
      "logistic_regression  0.796438  302.0  37.0   59.0  139.0  0.664306   0.789773   \n",
      "svc                  0.603088  325.0  14.0  149.0   49.0  0.469948   0.777778   \n",
      "xgboost              0.755833  307.0  32.0   78.0  120.0  0.623720   0.789474   \n",
      "simple-NN            0.987634  320.0  19.0   12.0  186.0  0.973475   0.907317   \n",
      "\n",
      "                       recall  accuracy  \n",
      "knn_clf              0.722222  0.785847  \n",
      "random_forest        0.661616  0.821229  \n",
      "logistic_regression  0.702020  0.821229  \n",
      "svc                  0.247475  0.696462  \n",
      "xgboost              0.606061  0.795158  \n",
      "simple-NN            0.939394  0.942272  \n",
      "                      roc_auc     tn    fp     fn     tp       map  precision  \\\n",
      "knn_clf              0.756898  300.0  74.0   47.0  116.0  0.522008   0.610526   \n",
      "random_forest        0.761688  338.0  36.0   62.0  101.0  0.572265   0.737226   \n",
      "logistic_regression  0.791263  328.0  46.0   48.0  115.0  0.593329   0.714286   \n",
      "svc                  0.596273  368.0   6.0  129.0   34.0  0.417524   0.850000   \n",
      "xgboost              0.739428  342.0  32.0   71.0   92.0  0.550977   0.741935   \n",
      "simple-NN            0.989616  347.0  27.0    9.0  154.0  0.971905   0.850829   \n",
      "\n",
      "                       recall  accuracy  \n",
      "knn_clf              0.711656  0.774674  \n",
      "random_forest        0.619632  0.817505  \n",
      "logistic_regression  0.705521  0.824953  \n",
      "svc                  0.208589  0.748603  \n",
      "xgboost              0.564417  0.808194  \n",
      "simple-NN            0.944785  0.932961  \n",
      "                      roc_auc     tn    fp     fn     tp       map  precision  \\\n",
      "knn_clf              0.814440  294.0  48.0   45.0  150.0  0.666549   0.757576   \n",
      "random_forest        0.818421  309.0  33.0   52.0  143.0  0.692668   0.812500   \n",
      "logistic_regression  0.784728  307.0  35.0   64.0  131.0  0.649332   0.789157   \n",
      "svc                  0.632996  333.0   9.0  138.0   57.0  0.509431   0.863636   \n",
      "xgboost              0.788012  311.0  31.0   65.0  130.0  0.659345   0.807453   \n",
      "simple-NN            0.988754  319.0  23.0   13.0  182.0  0.973610   0.887805   \n",
      "\n",
      "                       recall  accuracy  \n",
      "knn_clf              0.769231  0.826816  \n",
      "random_forest        0.733333  0.841713  \n",
      "logistic_regression  0.671795  0.815642  \n",
      "svc                  0.292308  0.726257  \n",
      "xgboost              0.666667  0.821229  \n",
      "simple-NN            0.933333  0.932961  \n",
      "                      roc_auc     tn    fp     fn     tp       map  precision  \\\n",
      "knn_clf              0.797837  214.0  62.0   46.0  210.0  0.719796   0.772059   \n",
      "random_forest        0.833645  237.0  39.0   49.0  207.0  0.772507   0.841463   \n",
      "logistic_regression  0.819577  226.0  50.0   46.0  210.0  0.749026   0.807692   \n",
      "svc                  0.603799  274.0   2.0  201.0   55.0  0.585125   0.964912   \n",
      "xgboost              0.830842  243.0  33.0   56.0  200.0  0.775864   0.858369   \n",
      "simple-NN            0.994310  248.0  28.0    3.0  253.0  0.991927   0.900356   \n",
      "\n",
      "                       recall  accuracy  \n",
      "knn_clf              0.820312  0.796992  \n",
      "random_forest        0.808594  0.834586  \n",
      "logistic_regression  0.820312  0.819549  \n",
      "svc                  0.214844  0.618421  \n",
      "xgboost              0.781250  0.832707  \n",
      "simple-NN            0.988281  0.941729  \n",
      "                      roc_auc     tn    fp     fn     tp       map  precision  \\\n",
      "knn_clf              0.798070  298.0  62.0   41.0  136.0  0.604114   0.686869   \n",
      "random_forest        0.807274  327.0  33.0   52.0  125.0  0.655548   0.791139   \n",
      "logistic_regression  0.757274  291.0  69.0   52.0  125.0  0.551870   0.644330   \n",
      "svc                  0.508616  354.0   6.0  171.0    6.0  0.335385   0.500000   \n",
      "xgboost              0.704096  330.0  30.0   90.0   87.0  0.533091   0.743590   \n",
      "simple-NN            0.832917  292.0  68.0   51.0  126.0  0.681779   0.649485   \n",
      "\n",
      "                       recall  accuracy  \n",
      "knn_clf              0.768362  0.808194  \n",
      "random_forest        0.706215  0.841713  \n",
      "logistic_regression  0.706215  0.774674  \n",
      "svc                  0.033898  0.670391  \n",
      "xgboost              0.491525  0.776536  \n",
      "simple-NN            0.711864  0.778399  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(metrics_all)):\n",
    "    print(metrics_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(metrics_all[6]).to_csv(\"../../../../Desktop/binding/thesis english/Results/baselines_test_pi3k.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
