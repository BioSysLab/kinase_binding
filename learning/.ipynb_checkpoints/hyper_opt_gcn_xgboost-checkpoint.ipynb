{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.callbacks import History, ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from data_analysis import calculate_metrics\n",
    "from functools import partial\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pickle\n",
    "import dill\n",
    "from hyper_mining import objective_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fspace = {\n",
    "    'conv1' : hp.quniform('conv1', 32, 64, 8),\n",
    "    'conv2' : hp.quniform('conv2', 64, 128, 8),\n",
    "    'conv3' : hp.quniform('conv3', 128, 168, 8),\n",
    "    'fp' : hp.quniform('fp', 96, 196, 8),\n",
    "    'dense1' : hp.quniform('dense1',96,512,32),\n",
    "    'dense2' : hp.quniform('dense2',96,512,32),\n",
    "    'dense3' : hp.quniform('dense3',64,512,32),\n",
    "    'dropout_rate' : hp.uniform('dropout_rate',0.1,0.5),\n",
    "    'lr' : hp.uniform('lr',0.000001,0.01),\n",
    "    'n_epochs' : hp.quniform('n_epochs',15,60,5),\n",
    "    'batch_size' : hp.quniform('batch_size',64,256,16),\n",
    "    'colsample_bylevel' : hp.uniform('colsample_bylevel', 0.1, 1), \n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0.1, 1), \n",
    "    'gamma' : hp.uniform('gamma', 0.1, 1), \n",
    "    'learning_rate' : hp.uniform('learning_rate', 0.1, 1),\n",
    "    'max_delta_step' : hp.quniform('max_delta_step',1,10,1),\n",
    "    'max_depth' : hp.quniform('max_depth',6, 12, 1),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight',10 ,500 ,5),\n",
    "    'reg_alpha' : hp.uniform('reg_alpha',0.1,100),\n",
    "    'reg_lambda' : hp.uniform('reg_lambda',0.1,100),\n",
    "    'subsample' : hp.uniform('subsample',0.1,1.0),\n",
    "    'max_bin' : hp.quniform('max_bin',16,256,16)\n",
    "    #'margin' : hp.uniform('margin',0.2,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'p38'\n",
    "base_path = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath = base_path+f'/data/{target}/data.csv'\n",
    "df=pd.read_csv(data_fpath).set_index('biolab_index')\n",
    "\n",
    "with open(base_path+f'/data/{target}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds = dill.load(in_f)\n",
    "with open(base_path+f'/data/{target}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds = dill.load(in_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list = [df.loc[train_val_folds[0][0]],\n",
    "                 df.loc[train_val_folds[1][0]],\n",
    "                 df.loc[train_val_folds[2][0]],\n",
    "                 df.loc[train_val_folds[3][0]],\n",
    "                 df.loc[train_val_folds[4][0]],\n",
    "                 df.loc[train_val_folds[5][0]],\n",
    "                 ]\n",
    "validation_list = [df.loc[train_val_folds[0][1]],\n",
    "                   df.loc[train_val_folds[1][1]],\n",
    "                   df.loc[train_val_folds[2][1]],\n",
    "                   df.loc[train_val_folds[3][1]],\n",
    "                   df.loc[train_val_folds[4][1]],\n",
    "                   df.loc[train_val_folds[5][1]],\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin_objective = partial(objective_fn, train_sets = training_list, val_sets = validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials():\n",
    "\n",
    "    trials_step = 0  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    max_trials = 1  # initial max_trials. put something small to not have to wait\n",
    "\n",
    "    \n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(\"gcn_xgb.hyperopt\", \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    best = fmin(fn = fmin_objective, space = fspace, algo=tpe.suggest, max_evals=max_trials, trials=trials)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    \n",
    "    # save the trials object\n",
    "    with open(\"gcn_xgb.hyperopt\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "    return(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved Trials! Loading...\n",
      "Rerunning from 29 trials to 29 (+0) trials\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 29/29 [00:00<?, ?trial/s, best loss=?]\n",
      "Best: {'batch_size': 256.0, 'colsample_bylevel': 0.9742808465908208, 'colsample_bytree': 0.37580168789089324, 'conv1': 40.0, 'conv2': 80.0, 'conv3': 160.0, 'dense1': 256.0, 'dense2': 384.0, 'dense3': 384.0, 'dropout_rate': 0.11318471159226598, 'fp': 96.0, 'gamma': 0.6244840665119147, 'learning_rate': 0.10045460409368323, 'lr': 0.00022802502278976386, 'max_bin': 80.0, 'max_delta_step': 4.0, 'max_depth': 7.0, 'min_child_weight': 25.0, 'n_epochs': 15.0, 'reg_alpha': 50.67508723786807, 'reg_lambda': 18.383656172678286, 'subsample': 0.4818345443328064}\n"
     ]
    }
   ],
   "source": [
    "trials = run_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = trials.trials[0]['result']['loss']\n",
    "for i in range(1,len(trials.trials)):\n",
    "    if (trials.trials[i]['result']['loss'] <=  best_loss):\n",
    "        best_loss = trials.trials[i]['result']['loss']\n",
    "        index = i\n",
    "best_params = trials.trials[index]['misc']['vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': [256.0],\n",
       " 'colsample_bylevel': [0.9742808465908208],\n",
       " 'colsample_bytree': [0.37580168789089324],\n",
       " 'conv1': [40.0],\n",
       " 'conv2': [80.0],\n",
       " 'conv3': [160.0],\n",
       " 'dense1': [256.0],\n",
       " 'dense2': [384.0],\n",
       " 'dense3': [384.0],\n",
       " 'dropout_rate': [0.11318471159226598],\n",
       " 'fp': [96.0],\n",
       " 'gamma': [0.6244840665119147],\n",
       " 'learning_rate': [0.10045460409368323],\n",
       " 'lr': [0.00022802502278976386],\n",
       " 'max_bin': [80.0],\n",
       " 'max_delta_step': [4.0],\n",
       " 'max_depth': [7.0],\n",
       " 'min_child_weight': [25.0],\n",
       " 'n_epochs': [15.0],\n",
       " 'reg_alpha': [50.67508723786807],\n",
       " 'reg_lambda': [18.383656172678286],\n",
       " 'subsample': [0.4818345443328064]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyper_mining import XGB_predictor,GCN_online_mining_test\n",
    "from data_analysis import calculate_metrics\n",
    "es = EarlyStopping(monitor='loss',patience=8, min_delta=0)\n",
    "rlr = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=4, verbose=1, min_lr=0.0000001)\n",
    "gcn_best = {\n",
    "        \"num_layers\" : 3,\n",
    "        \"max_atoms\" : 70,\n",
    "        \"num_atom_features\" : 62,\n",
    "        \"num_atom_features_original\" : 62,\n",
    "        \"num_bond_features\" : 6,\n",
    "        \"max_degree\" : 5,\n",
    "        \"conv_width\" : [int(best_params['conv1'][0]), int(best_params['conv2'][0]), int(best_params['conv3'][0])],\n",
    "        \"fp_length\" : [int(best_params['fp'][0]), int(best_params['fp'][0]), int(best_params['fp'][0])],\n",
    "        \"activ_enc\" : \"selu\",\n",
    "        \"activ_dec\" : \"selu\",\n",
    "        \"learning_rates\" : [0.001,0.001,0.001],\n",
    "        \"learning_rates_fp\": [0.005,0.005,0.005],\n",
    "        \"losses_conv\" : {\n",
    "                    \"neighbor_output\": \"mean_squared_error\",\n",
    "                    \"self_output\": \"mean_squared_error\",\n",
    "                    },\n",
    "        \"lossWeights\" : {\"neighbor_output\": 1.0, \"self_output\": 1.0},\n",
    "        \"metrics\" : \"mse\",\n",
    "        \"loss_fp\" : \"mean_squared_error\",\n",
    "        \"enc_layer_names\" : [\"enc_1\", \"enc_2\", \"enc_3\"],\n",
    "        'callbacks' : [es,rlr],\n",
    "        'adam_decay': 0.0005329142291371636,\n",
    "        'beta': 5,\n",
    "        'p': 0.004465204118126482,\n",
    "        'dense_size' : [int(best_params['dense1'][0]), int(best_params['dense2'][0]), int(best_params['dense3'][0])],\n",
    "        'dropout_rate' : [best_params['dropout_rate'][0], best_params['dropout_rate'][0]],\n",
    "        'lr' : best_params['lr'][0],\n",
    "        'batch_size' : int(best_params['batch_size'][0]),\n",
    "        'n_epochs' : int(best_params['n_epochs'][0])\n",
    "        #'margin' : best_params['margin'][0]\n",
    "        }\n",
    "xgb_best = {\n",
    "        \"colsample_bylevel\" : best_params['colsample_bylevel'][0],\n",
    "        \"colsample_bytree\" : best_params['colsample_bytree'][0],\n",
    "        \"gamma\" : best_params['gamma'][0],\n",
    "        \"eta\" : best_params['learning_rate'][0],\n",
    "        \"max_delta_step\" : int(best_params['max_delta_step'][0]),\n",
    "        \"max_depth\" : int(best_params['max_depth'][0]),\n",
    "        \"min_child_weight\" : int(best_params['min_child_weight'][0]),\n",
    "        \"alpha\" : best_params['reg_alpha'][0],\n",
    "        \"lambda\" : best_params['reg_lambda'][0],\n",
    "        \"subsample\" : best_params['subsample'][0],\n",
    "        \"max_bin\" : int(best_params['max_bin'][0]),\n",
    "        \"eval_metric\":'auc',\n",
    "        \"objective\":'binary:logistic',\n",
    "        \"booster\":'gbtree'\n",
    "        #\"single_precision_histogram\" : True\n",
    "        }\n",
    "class_XGB = XGB_predictor(xgb_best)\n",
    "class_GCN = GCN_online_mining_test(gcn_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyper = {\n",
    "        \"colsample_bylevel\" : 0.5612301667238877,\n",
    "        \"colsample_bytree\" : 0.788688363076523,\n",
    "        \"gamma\" : 0.35376030016117566,\n",
    "        \"eta\" : 0.4023692255888918,\n",
    "        \"max_delta_step\" : int(3),\n",
    "        \"max_depth\" : int(8),\n",
    "        \"min_child_weight\" : int(70),\n",
    "        \"alpha\" : 0.15030685758880047,\n",
    "        \"lambda\" : 15.311721955443915,\n",
    "        \"subsample\" : 0.8303923929525608,\n",
    "        \"max_bin\" : int(208),\n",
    "        \"eval_metric\":'auc',\n",
    "        \"objective\":'binary:logistic',\n",
    "        \"booster\":'gbtree',\n",
    "        \"single_precision_histogram\" : True\n",
    "}\n",
    "class_XGB_2 = XGB_predictor(xgb_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/15\n",
      "2541/2541 [==============================] - 6s 2ms/step - loss: 0.9979 - val_loss: 0.9961\n",
      "Epoch 2/15\n",
      "2541/2541 [==============================] - 1s 243us/step - loss: 0.9973 - val_loss: 0.9922\n",
      "Epoch 3/15\n",
      "2541/2541 [==============================] - 1s 243us/step - loss: 0.9957 - val_loss: 0.9861\n",
      "Epoch 4/15\n",
      "2541/2541 [==============================] - 1s 240us/step - loss: 0.9925 - val_loss: 0.9806\n",
      "Epoch 5/15\n",
      "2541/2541 [==============================] - 1s 239us/step - loss: 0.9889 - val_loss: 0.9815\n",
      "Epoch 6/15\n",
      "2541/2541 [==============================] - 1s 241us/step - loss: 0.9862 - val_loss: 0.9778\n",
      "Epoch 7/15\n",
      "2541/2541 [==============================] - 1s 241us/step - loss: 0.9827 - val_loss: 0.9718\n",
      "Epoch 8/15\n",
      "2541/2541 [==============================] - 1s 238us/step - loss: 0.9792 - val_loss: 0.9698\n",
      "Epoch 9/15\n",
      "2541/2541 [==============================] - 1s 240us/step - loss: 0.9764 - val_loss: 0.9705\n",
      "Epoch 10/15\n",
      "2541/2541 [==============================] - 1s 241us/step - loss: 0.9702 - val_loss: 0.9781\n",
      "Epoch 11/15\n",
      "2541/2541 [==============================] - 1s 242us/step - loss: 0.9713 - val_loss: 0.9624\n",
      "Epoch 12/15\n",
      "2541/2541 [==============================] - 1s 244us/step - loss: 0.9677 - val_loss: 0.9658\n",
      "Epoch 13/15\n",
      "2541/2541 [==============================] - 1s 252us/step - loss: 0.9662 - val_loss: 0.9706\n",
      "Epoch 14/15\n",
      "2541/2541 [==============================] - 1s 246us/step - loss: 0.9654 - val_loss: 0.9689\n",
      "Epoch 15/15\n",
      "2541/2541 [==============================] - 1s 239us/step - loss: 0.9599 - val_loss: 0.9614\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/15\n",
      "2541/2541 [==============================] - 7s 3ms/step - loss: 0.9976 - val_loss: 0.9974\n",
      "Epoch 2/15\n",
      "2541/2541 [==============================] - 1s 243us/step - loss: 0.9972 - val_loss: 0.9954\n",
      "Epoch 3/15\n",
      "2541/2541 [==============================] - 1s 243us/step - loss: 0.9960 - val_loss: 0.9911\n",
      "Epoch 4/15\n",
      "2541/2541 [==============================] - 1s 243us/step - loss: 0.9940 - val_loss: 0.9872\n",
      "Epoch 5/15\n",
      "2541/2541 [==============================] - 1s 245us/step - loss: 0.9903 - val_loss: 0.9835\n",
      "Epoch 6/15\n",
      "2541/2541 [==============================] - 1s 243us/step - loss: 0.9867 - val_loss: 0.9808\n",
      "Epoch 7/15\n",
      "2541/2541 [==============================] - 1s 242us/step - loss: 0.9852 - val_loss: 0.9792\n",
      "Epoch 8/15\n",
      "2541/2541 [==============================] - 1s 242us/step - loss: 0.9812 - val_loss: 0.9704\n",
      "Epoch 9/15\n",
      "2541/2541 [==============================] - 1s 242us/step - loss: 0.9790 - val_loss: 0.9739\n",
      "Epoch 10/15\n",
      "2541/2541 [==============================] - 1s 241us/step - loss: 0.9776 - val_loss: 0.9757\n",
      "Epoch 11/15\n",
      "2541/2541 [==============================] - 1s 242us/step - loss: 0.9751 - val_loss: 0.9734\n",
      "Epoch 12/15\n",
      "2541/2541 [==============================] - 1s 241us/step - loss: 0.9716 - val_loss: 0.9681\n",
      "Epoch 13/15\n",
      "2541/2541 [==============================] - 1s 243us/step - loss: 0.9667 - val_loss: 0.9673\n",
      "Epoch 14/15\n",
      "2541/2541 [==============================] - 1s 241us/step - loss: 0.9661 - val_loss: 0.9621\n",
      "Epoch 15/15\n",
      "2541/2541 [==============================] - 1s 241us/step - loss: 0.9651 - val_loss: 0.9635\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/15\n",
      "2541/2541 [==============================] - 7s 3ms/step - loss: 0.9976 - val_loss: 0.9979\n",
      "Epoch 2/15\n",
      "2541/2541 [==============================] - 1s 245us/step - loss: 0.9973 - val_loss: 0.9966\n",
      "Epoch 3/15\n",
      "2541/2541 [==============================] - 1s 243us/step - loss: 0.9969 - val_loss: 0.9940\n",
      "Epoch 4/15\n",
      "2541/2541 [==============================] - 1s 243us/step - loss: 0.9953 - val_loss: 0.9888\n",
      "Epoch 5/15\n",
      "2541/2541 [==============================] - 1s 244us/step - loss: 0.9916 - val_loss: 0.9840\n",
      "Epoch 6/15\n",
      "2541/2541 [==============================] - 1s 243us/step - loss: 0.9875 - val_loss: 0.9816\n",
      "Epoch 7/15\n",
      "2541/2541 [==============================] - 1s 243us/step - loss: 0.9844 - val_loss: 0.9883\n",
      "Epoch 8/15\n",
      "2541/2541 [==============================] - 1s 243us/step - loss: 0.9830 - val_loss: 0.9816\n",
      "Epoch 9/15\n",
      "2541/2541 [==============================] - 1s 242us/step - loss: 0.9791 - val_loss: 0.9789\n",
      "Epoch 10/15\n",
      "2541/2541 [==============================] - 1s 242us/step - loss: 0.9722 - val_loss: 0.9779\n",
      "Epoch 11/15\n",
      "2541/2541 [==============================] - 1s 243us/step - loss: 0.9701 - val_loss: 0.9792\n",
      "Epoch 12/15\n",
      "2541/2541 [==============================] - 1s 243us/step - loss: 0.9702 - val_loss: 0.9763\n",
      "Epoch 13/15\n",
      "2541/2541 [==============================] - 1s 242us/step - loss: 0.9643 - val_loss: 0.9768\n",
      "Epoch 14/15\n",
      "2541/2541 [==============================] - 1s 242us/step - loss: 0.9652 - val_loss: 0.9661\n",
      "Epoch 15/15\n",
      "2541/2541 [==============================] - 1s 242us/step - loss: 0.9622 - val_loss: 0.9739\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/15\n",
      "2541/2541 [==============================] - 8s 3ms/step - loss: 0.9978 - val_loss: 0.9988\n",
      "Epoch 2/15\n",
      "2541/2541 [==============================] - 1s 247us/step - loss: 0.9975 - val_loss: 0.9980\n",
      "Epoch 3/15\n",
      "2541/2541 [==============================] - 1s 245us/step - loss: 0.9968 - val_loss: 0.9946\n",
      "Epoch 4/15\n",
      "2541/2541 [==============================] - 1s 247us/step - loss: 0.9951 - val_loss: 0.9888\n",
      "Epoch 5/15\n",
      "2541/2541 [==============================] - 1s 247us/step - loss: 0.9918 - val_loss: 0.9844\n",
      "Epoch 6/15\n",
      "2541/2541 [==============================] - 1s 246us/step - loss: 0.9875 - val_loss: 0.9816\n",
      "Epoch 7/15\n",
      "2541/2541 [==============================] - 1s 246us/step - loss: 0.9830 - val_loss: 0.9820\n",
      "Epoch 8/15\n",
      "2541/2541 [==============================] - 1s 245us/step - loss: 0.9814 - val_loss: 0.9800\n",
      "Epoch 9/15\n",
      "2541/2541 [==============================] - 1s 245us/step - loss: 0.9782 - val_loss: 0.9781\n",
      "Epoch 10/15\n",
      "2541/2541 [==============================] - 1s 246us/step - loss: 0.9760 - val_loss: 0.9779\n",
      "Epoch 11/15\n",
      "2541/2541 [==============================] - 1s 245us/step - loss: 0.9703 - val_loss: 0.9790\n",
      "Epoch 12/15\n",
      "2541/2541 [==============================] - 1s 245us/step - loss: 0.9703 - val_loss: 0.9752\n",
      "Epoch 13/15\n",
      "2541/2541 [==============================] - 1s 246us/step - loss: 0.9586 - val_loss: 0.9786\n",
      "Epoch 14/15\n",
      "2541/2541 [==============================] - 1s 245us/step - loss: 0.9659 - val_loss: 0.9739\n",
      "Epoch 15/15\n",
      "2541/2541 [==============================] - 1s 244us/step - loss: 0.9608 - val_loss: 0.9772\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00011401250958442688.\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/15\n",
      "2541/2541 [==============================] - 9s 4ms/step - loss: 0.9977 - val_loss: 0.9983\n",
      "Epoch 2/15\n",
      "2541/2541 [==============================] - 1s 248us/step - loss: 0.9974 - val_loss: 0.9973\n",
      "Epoch 3/15\n",
      "2541/2541 [==============================] - 1s 250us/step - loss: 0.9962 - val_loss: 0.9914\n",
      "Epoch 4/15\n",
      "2541/2541 [==============================] - 1s 248us/step - loss: 0.9936 - val_loss: 0.9860\n",
      "Epoch 5/15\n",
      "2541/2541 [==============================] - 1s 246us/step - loss: 0.9891 - val_loss: 0.9798\n",
      "Epoch 6/15\n",
      "2541/2541 [==============================] - 1s 249us/step - loss: 0.9869 - val_loss: 0.9726\n",
      "Epoch 7/15\n",
      "2541/2541 [==============================] - 1s 247us/step - loss: 0.9848 - val_loss: 0.9771\n",
      "Epoch 8/15\n",
      "2541/2541 [==============================] - 1s 248us/step - loss: 0.9817 - val_loss: 0.9753\n",
      "Epoch 9/15\n",
      "2541/2541 [==============================] - 1s 248us/step - loss: 0.9794 - val_loss: 0.9764\n",
      "Epoch 10/15\n",
      "2541/2541 [==============================] - 1s 248us/step - loss: 0.9750 - val_loss: 0.9737\n",
      "Epoch 11/15\n",
      "2541/2541 [==============================] - 1s 251us/step - loss: 0.9733 - val_loss: 0.9700\n",
      "Epoch 12/15\n",
      "2541/2541 [==============================] - 1s 249us/step - loss: 0.9682 - val_loss: 0.9709\n",
      "Epoch 13/15\n",
      "2541/2541 [==============================] - 1s 248us/step - loss: 0.9622 - val_loss: 0.9661\n",
      "Epoch 14/15\n",
      "2541/2541 [==============================] - 1s 248us/step - loss: 0.9624 - val_loss: 0.9687\n",
      "Epoch 15/15\n",
      "2541/2541 [==============================] - 1s 249us/step - loss: 0.9537 - val_loss: 0.9670\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2545 samples, validate on 505 samples\n",
      "Epoch 1/15\n",
      "2545/2545 [==============================] - 11s 4ms/step - loss: 0.9977 - val_loss: 0.9969\n",
      "Epoch 2/15\n",
      "2545/2545 [==============================] - 1s 253us/step - loss: 0.9974 - val_loss: 0.9957\n",
      "Epoch 3/15\n",
      "2545/2545 [==============================] - 1s 252us/step - loss: 0.9964 - val_loss: 0.9916\n",
      "Epoch 4/15\n",
      "2545/2545 [==============================] - 1s 252us/step - loss: 0.9942 - val_loss: 0.9870\n",
      "Epoch 5/15\n",
      "2545/2545 [==============================] - 1s 253us/step - loss: 0.9908 - val_loss: 0.9803\n",
      "Epoch 6/15\n",
      "2545/2545 [==============================] - 1s 253us/step - loss: 0.9866 - val_loss: 0.9768\n",
      "Epoch 7/15\n",
      "2545/2545 [==============================] - 1s 251us/step - loss: 0.9823 - val_loss: 0.9742\n",
      "Epoch 8/15\n",
      "2545/2545 [==============================] - 1s 251us/step - loss: 0.9788 - val_loss: 0.9741\n",
      "Epoch 9/15\n",
      "2545/2545 [==============================] - 1s 252us/step - loss: 0.9729 - val_loss: 0.9689\n",
      "Epoch 10/15\n",
      "2545/2545 [==============================] - 1s 250us/step - loss: 0.9712 - val_loss: 0.9668\n",
      "Epoch 11/15\n",
      "2545/2545 [==============================] - 1s 252us/step - loss: 0.9692 - val_loss: 0.9582\n",
      "Epoch 12/15\n",
      "2545/2545 [==============================] - 1s 252us/step - loss: 0.9672 - val_loss: 0.9648\n",
      "Epoch 13/15\n",
      "2545/2545 [==============================] - 1s 252us/step - loss: 0.9625 - val_loss: 0.9586\n",
      "Epoch 14/15\n",
      "2545/2545 [==============================] - 1s 251us/step - loss: 0.9590 - val_loss: 0.9686\n",
      "Epoch 15/15\n",
      "2545/2545 [==============================] - 1s 253us/step - loss: 0.9464 - val_loss: 0.9711\n"
     ]
    }
   ],
   "source": [
    "#K.clear_session()\n",
    "training_metrics = {}\n",
    "validation_metrics = {}\n",
    "es2 = EarlyStopping(monitor='loss',patience=15, min_delta=0)\n",
    "rlr2 = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=2, verbose=1, min_lr=0.000000001)\n",
    "for i in range(len(training_list)):\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = class_GCN.dataframe_to_gcn_input(validation_list[i])\n",
    "        Y_cold = validation_list[i].Binary \n",
    "        Y_dummy_cold = np.empty((X_atoms_cold.shape[0],gcn_best['dense_size'][2]+1))\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = class_GCN.dataframe_to_gcn_input(training_list[i])\n",
    "        Y = training_list[i].Binary\n",
    "        Y_dummy_train = np.empty((X_atoms_train.shape[0],gcn_best['dense_size'][2]+1))\n",
    "        \n",
    "        gcn_encoder = class_GCN.build_encoder()\n",
    "        gcn_model = class_GCN.build_model(gcn_encoder)\n",
    "        gcn_mining = class_GCN.build_mining(gcn_model)\n",
    "        \n",
    "        gcn_mining.fit([X_atoms_train,X_bonds_train,X_edges_train,Y],\n",
    "                       Y_dummy_train,\n",
    "                       epochs = gcn_best['n_epochs'],\n",
    "                       batch_size = gcn_best['batch_size'],\n",
    "                       shuffle = True,\n",
    "                       validation_data = ([X_atoms_cold,X_bonds_cold,X_edges_cold,Y_cold],Y_dummy_cold),\n",
    "                       callbacks=[es2,rlr2]\n",
    "                      )\n",
    "        #Predict Embeddings\n",
    "        embeddings_cold = gcn_model.predict([X_atoms_cold,X_bonds_cold,X_edges_cold])\n",
    "        embeddings_train = gcn_model.predict([X_atoms_train, X_bonds_train, X_edges_train])\n",
    "        \n",
    "        #Prepare data for XGBoost\n",
    "        dmatrix_train = class_XGB.to_xgb_input(Y,embeddings_train)\n",
    "        dmatrix_cold = class_XGB.to_xgb_input(Y_cold,embeddings_cold)\n",
    "        \n",
    "        evalist = [(dmatrix_train,'train'),(dmatrix_cold,'eval')]\n",
    "        xgb_model = class_XGB.build_model(dmatrix_train,evalist,300)\n",
    "        \n",
    "        xgb_pred_cold = xgb_model.predict(dmatrix_cold)\n",
    "        validation_metrics['Val_%s'%i] = calculate_metrics(np.array(Y_cold),xgb_pred_cold)\n",
    "        \n",
    "        xgb_pred_train = xgb_model.predict(dmatrix_train)\n",
    "        training_metrics['Train_%s'%i] = calculate_metrics(np.array(Y),xgb_pred_train)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Val_0': {'roc_auc': 0.8744913806491691,\n",
       "  'tn': 203,\n",
       "  'fp': 71,\n",
       "  'fn': 35,\n",
       "  'tp': 200,\n",
       "  'map': 0.858605979880733,\n",
       "  'precision': 0.7380073800738007,\n",
       "  'recall': 0.851063829787234,\n",
       "  'accuracy': 0.7917485265225933},\n",
       " 'Val_1': {'roc_auc': 0.9031126247425948,\n",
       "  'tn': 241,\n",
       "  'fp': 54,\n",
       "  'fn': 39,\n",
       "  'tp': 175,\n",
       "  'map': 0.8609554237843965,\n",
       "  'precision': 0.7641921397379913,\n",
       "  'recall': 0.8177570093457944,\n",
       "  'accuracy': 0.8172888015717092},\n",
       " 'Val_2': {'roc_auc': 0.8586200500788278,\n",
       "  'tn': 201,\n",
       "  'fp': 62,\n",
       "  'fn': 51,\n",
       "  'tp': 195,\n",
       "  'map': 0.8538456995808472,\n",
       "  'precision': 0.7587548638132295,\n",
       "  'recall': 0.7926829268292683,\n",
       "  'accuracy': 0.7779960707269156},\n",
       " 'Val_3': {'roc_auc': 0.8554640417838488,\n",
       "  'tn': 199,\n",
       "  'fp': 63,\n",
       "  'fn': 47,\n",
       "  'tp': 200,\n",
       "  'map': 0.8186819243729625,\n",
       "  'precision': 0.7604562737642585,\n",
       "  'recall': 0.8097165991902834,\n",
       "  'accuracy': 0.7838899803536346},\n",
       " 'Val_4': {'roc_auc': 0.90339170880939,\n",
       "  'tn': 184,\n",
       "  'fp': 44,\n",
       "  'fn': 50,\n",
       "  'tp': 231,\n",
       "  'map': 0.9100581991287505,\n",
       "  'precision': 0.84,\n",
       "  'recall': 0.8220640569395018,\n",
       "  'accuracy': 0.8153241650294696},\n",
       " 'Val_5': {'roc_auc': 0.85892060800201,\n",
       "  'tn': 182,\n",
       "  'fp': 62,\n",
       "  'fn': 46,\n",
       "  'tp': 215,\n",
       "  'map': 0.8520511003386371,\n",
       "  'precision': 0.776173285198556,\n",
       "  'recall': 0.8237547892720306,\n",
       "  'accuracy': 0.7861386138613862}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train_0': {'roc_auc': 0.9469330882662786,\n",
       "  'tn': 1087,\n",
       "  'fp': 205,\n",
       "  'fn': 132,\n",
       "  'tp': 1117,\n",
       "  'map': 0.939930309529514,\n",
       "  'precision': 0.8449319213313162,\n",
       "  'recall': 0.8943154523618895,\n",
       "  'accuracy': 0.867375049193231},\n",
       " 'Train_1': {'roc_auc': 0.9410852016825986,\n",
       "  'tn': 1080,\n",
       "  'fp': 191,\n",
       "  'fn': 143,\n",
       "  'tp': 1127,\n",
       "  'map': 0.9325836947433694,\n",
       "  'precision': 0.855083459787557,\n",
       "  'recall': 0.8874015748031496,\n",
       "  'accuracy': 0.8685556867375049},\n",
       " 'Train_2': {'roc_auc': 0.9450134956363901,\n",
       "  'tn': 1108,\n",
       "  'fp': 195,\n",
       "  'fn': 134,\n",
       "  'tp': 1104,\n",
       "  'map': 0.9376158463050164,\n",
       "  'precision': 0.8498845265588915,\n",
       "  'recall': 0.8917609046849758,\n",
       "  'accuracy': 0.8705234159779615},\n",
       " 'Train_3': {'roc_auc': 0.9451702615173262,\n",
       "  'tn': 1114,\n",
       "  'fp': 190,\n",
       "  'fn': 137,\n",
       "  'tp': 1100,\n",
       "  'map': 0.9358870527258855,\n",
       "  'precision': 0.8527131782945736,\n",
       "  'recall': 0.889248181083266,\n",
       "  'accuracy': 0.8713105076741441},\n",
       " 'Train_4': {'roc_auc': 0.9427514919726095,\n",
       "  'tn': 1145,\n",
       "  'fp': 193,\n",
       "  'fn': 150,\n",
       "  'tp': 1053,\n",
       "  'map': 0.9273995659067915,\n",
       "  'precision': 0.8451043338683788,\n",
       "  'recall': 0.8753117206982544,\n",
       "  'accuracy': 0.8650137741046832},\n",
       " 'Train_5': {'roc_auc': 0.9431839688868051,\n",
       "  'tn': 1114,\n",
       "  'fp': 208,\n",
       "  'fn': 131,\n",
       "  'tp': 1092,\n",
       "  'map': 0.9319220596994403,\n",
       "  'precision': 0.84,\n",
       "  'recall': 0.892886345053148,\n",
       "  'accuracy': 0.8667976424361493}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
