{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.callbacks import History, ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from data_analysis import calculate_metrics, load_weights_and_evaluate\n",
    "from model_builders import GCN_pretraining\n",
    "from hyperparameter_tuning_GCN import objective\n",
    "from functools import partial\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pickle\n",
    "import dill\n",
    "from hyper_mining import objective_fn\n",
    "import keras.backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, concatenate,Flatten\n",
    "from keras.models import Model, load_model\n",
    "import seaborn as sns\n",
    "from keras.callbacks import History, ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
    "from distance_and_mask_fn import pairwise_distance,masked_maximum,masked_minimum\n",
    "\n",
    "from NGF.preprocessing import tensorise_smiles\n",
    "from custom_layers.model_creator import encode_smiles, stage_creator\n",
    "from data_analysis import calculate_metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, roc_auc_score, auc, average_precision_score\n",
    "from hyperopt import STATUS_OK\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fspace = {\n",
    "    'conv1' : hp.quniform('conv1', 32, 64, 8),\n",
    "    'conv2' : hp.quniform('conv2', 64, 128, 8),\n",
    "    'conv3' : hp.quniform('conv3', 128, 168, 8),\n",
    "    'fp' : hp.quniform('fp', 96, 196, 8),\n",
    "    'dense1' : hp.quniform('dense1',96,512,32),\n",
    "    'dense2' : hp.quniform('dense2',96,512,32),\n",
    "    'dense3' : hp.quniform('dense3',64,512,32),\n",
    "    'dropout_rate' : hp.uniform('dropout_rate',0.1,0.5),\n",
    "    'lr' : hp.uniform('lr',0.00001,0.01),\n",
    "    'n_epochs' : hp.quniform('n_epochs',15,60,5),\n",
    "    'batch_size' : hp.quniform('batch_size',64,256,16),\n",
    "    'colsample_bylevel' : hp.uniform('colsample_bylevel', 0.1, 1), \n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0.1, 1), \n",
    "    'gamma' : hp.uniform('gamma', 0.1, 1), \n",
    "    'learning_rate' : hp.uniform('learning_rate', 0.1, 1),\n",
    "    'max_delta_step' : hp.quniform('max_delta_step',1,10,1),\n",
    "    'max_depth' : hp.quniform('max_depth',6, 12, 1),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight',10 ,500 ,5),\n",
    "    'reg_alpha' : hp.uniform('reg_alpha',0.1,100),\n",
    "    'reg_lambda' : hp.uniform('reg_lambda',0.1,100),\n",
    "    'subsample' : hp.uniform('subsample',0.1,1.0),\n",
    "    'max_bin' : hp.quniform('max_bin',16,256,16),\n",
    "    'margin' : hp.uniform('margin',0.2,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'p38'\n",
    "base_path = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath = base_path+f'/data/{target}/data.csv'\n",
    "df=pd.read_csv(data_fpath).set_index('biolab_index')\n",
    "\n",
    "with open(base_path+f'/data/{target}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds = dill.load(in_f)\n",
    "with open(base_path+f'/data/{target}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds = dill.load(in_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list = [df.loc[train_val_folds[0][0]],\n",
    "                 df.loc[train_val_folds[1][0]],\n",
    "                 df.loc[train_val_folds[2][0]],\n",
    "                 df.loc[train_val_folds[3][0]],\n",
    "                 df.loc[train_val_folds[4][0]],\n",
    "                 df.loc[train_val_folds[5][0]],\n",
    "                 ]\n",
    "validation_list = [df.loc[train_val_folds[0][1]],\n",
    "                   df.loc[train_val_folds[1][1]],\n",
    "                   df.loc[train_val_folds[2][1]],\n",
    "                   df.loc[train_val_folds[3][1]],\n",
    "                   df.loc[train_val_folds[4][1]],\n",
    "                   df.loc[train_val_folds[5][1]],\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin_objective = partial(objective, train_sets = training_list, val_sets = validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials():\n",
    "\n",
    "    trials_step = 0  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    max_trials = 1  # initial max_trials. put something small to not have to wait\n",
    "\n",
    "    \n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(\"gcn_xgb.hyperopt\", \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    best = fmin(fn = fmin_objective, space = fspace, algo=tpe.suggest, max_evals=max_trials, trials=trials)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    \n",
    "    # save the trials object\n",
    "    with open(\"gcn_xgb.hyperopt\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "    return(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved Trials! Loading...\n",
      "Rerunning from 73 trials to 73 (+0) trials\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 73/73 [00:00<?, ?trial/s, best loss=?]\n",
      "Best: {'batch_size': 96.0, 'colsample_bylevel': 0.19610957495328543, 'colsample_bytree': 0.9504328963958085, 'conv1': 88.0, 'conv2': 96.0, 'conv3': 160.0, 'dense1': 96.0, 'dense2': 416.0, 'dense3': 320.0, 'dropout_rate': 0.45547284530546817, 'fp': 160.0, 'gamma': 0.6648409346047757, 'learning_rate': 0.9073217403165388, 'lr': 0.0019499018534396453, 'margin': 0.6011401246738063, 'max_bin': 208.0, 'max_delta_step': 1.0, 'max_depth': 8.0, 'min_child_weight': 355.0, 'n_epochs': 60.0, 'reg_alpha': 56.190739151936484, 'reg_lambda': 52.450354051682496, 'subsample': 0.8316785844601918}\n"
     ]
    }
   ],
   "source": [
    "trials = run_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = trials.trials[0]['result']['loss']\n",
    "for i in range(1,len(trials.trials)):\n",
    "    if (trials.trials[i]['result']['loss'] <=  best_loss):\n",
    "        best_loss = trials.trials[i]['result']['loss']\n",
    "        index = i\n",
    "best_params = trials.trials[index]['misc']['vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': 2,\n",
       " 'tid': 28,\n",
       " 'spec': None,\n",
       " 'result': {'loss': -0.887696524475951, 'status': 'ok'},\n",
       " 'misc': {'tid': 28,\n",
       "  'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "  'workdir': None,\n",
       "  'idxs': {'batch_size': [28],\n",
       "   'colsample_bylevel': [28],\n",
       "   'colsample_bytree': [28],\n",
       "   'conv1': [28],\n",
       "   'conv2': [28],\n",
       "   'conv3': [28],\n",
       "   'dense1': [28],\n",
       "   'dense2': [28],\n",
       "   'dense3': [28],\n",
       "   'dropout_rate': [28],\n",
       "   'fp': [28],\n",
       "   'gamma': [28],\n",
       "   'learning_rate': [28],\n",
       "   'lr': [28],\n",
       "   'margin': [28],\n",
       "   'max_bin': [28],\n",
       "   'max_delta_step': [28],\n",
       "   'max_depth': [28],\n",
       "   'min_child_weight': [28],\n",
       "   'n_epochs': [28],\n",
       "   'reg_alpha': [28],\n",
       "   'reg_lambda': [28],\n",
       "   'subsample': [28]},\n",
       "  'vals': {'batch_size': [96.0],\n",
       "   'colsample_bylevel': [0.19610957495328543],\n",
       "   'colsample_bytree': [0.9504328963958085],\n",
       "   'conv1': [88.0],\n",
       "   'conv2': [96.0],\n",
       "   'conv3': [160.0],\n",
       "   'dense1': [96.0],\n",
       "   'dense2': [416.0],\n",
       "   'dense3': [320.0],\n",
       "   'dropout_rate': [0.45547284530546817],\n",
       "   'fp': [160.0],\n",
       "   'gamma': [0.6648409346047757],\n",
       "   'learning_rate': [0.9073217403165388],\n",
       "   'lr': [0.0019499018534396453],\n",
       "   'margin': [0.6011401246738063],\n",
       "   'max_bin': [208.0],\n",
       "   'max_delta_step': [1.0],\n",
       "   'max_depth': [8.0],\n",
       "   'min_child_weight': [355.0],\n",
       "   'n_epochs': [60.0],\n",
       "   'reg_alpha': [56.190739151936484],\n",
       "   'reg_lambda': [52.450354051682496],\n",
       "   'subsample': [0.8316785844601918]}},\n",
       " 'exp_key': None,\n",
       " 'owner': None,\n",
       " 'version': 0,\n",
       " 'book_time': datetime.datetime(2020, 7, 10, 18, 32, 49, 18000),\n",
       " 'refresh_time': datetime.datetime(2020, 7, 10, 18, 43, 58, 950000)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.trials[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyper_mining import XGB_predictor,GCN_online_mining_test\n",
    "from data_analysis import calculate_metrics\n",
    "es = EarlyStopping(monitor='loss',patience=8, min_delta=0)\n",
    "rlr = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=4, verbose=1, min_lr=0.0000001)\n",
    "gcn_best = {\n",
    "        \"num_layers\" : 3,\n",
    "        \"max_atoms\" : 70,\n",
    "        \"num_atom_features\" : 62,\n",
    "        \"num_atom_features_original\" : 62,\n",
    "        \"num_bond_features\" : 6,\n",
    "        \"max_degree\" : 5,\n",
    "        \"conv_width\" : [int(best_params['conv1'][0]), int(best_params['conv2'][0]), int(best_params['conv3'][0])],\n",
    "        \"fp_length\" : [int(best_params['fp'][0]), int(best_params['fp'][0]), int(best_params['fp'][0])],\n",
    "        \"activ_enc\" : \"selu\",\n",
    "        \"activ_dec\" : \"selu\",\n",
    "        \"learning_rates\" : [0.001,0.001,0.001],\n",
    "        \"learning_rates_fp\": [0.005,0.005,0.005],\n",
    "        \"losses_conv\" : {\n",
    "                    \"neighbor_output\": \"mean_squared_error\",\n",
    "                    \"self_output\": \"mean_squared_error\",\n",
    "                    },\n",
    "        \"lossWeights\" : {\"neighbor_output\": 1.0, \"self_output\": 1.0},\n",
    "        \"metrics\" : \"mse\",\n",
    "        \"loss_fp\" : \"mean_squared_error\",\n",
    "        \"enc_layer_names\" : [\"enc_1\", \"enc_2\", \"enc_3\"],\n",
    "        'callbacks' : [es,rlr],\n",
    "        'adam_decay': 0.0005329142291371636,\n",
    "        'beta': 5,\n",
    "        'p': 0.004465204118126482,\n",
    "        'dense_size' : [int(best_params['dense1'][0]), int(best_params['dense2'][0]), int(best_params['dense3'][0])],\n",
    "        'dropout_rate' : [best_params['dropout_rate'][0], best_params['dropout_rate'][0]],\n",
    "        'lr' : best_params['lr'][0],\n",
    "        'batch_size' : int(best_params['batch_size'][0]),\n",
    "        'n_epochs' : int(best_params['n_epochs'][0]),\n",
    "        'margin' : best_params['margin'][0]\n",
    "        }\n",
    "xgb_best = {\n",
    "        \"colsample_bylevel\" : best_params['colsample_bylevel'][0],\n",
    "        \"colsample_bytree\" : best_params['colsample_bytree'][0],\n",
    "        \"gamma\" : best_params['gamma'][0],\n",
    "        \"eta\" : best_params['learning_rate'][0],\n",
    "        \"max_delta_step\" : int(best_params['max_delta_step'][0]),\n",
    "        \"max_depth\" : int(best_params['max_depth'][0]),\n",
    "        \"min_child_weight\" : int(best_params['min_child_weight'][0]),\n",
    "        \"alpha\" : best_params['reg_alpha'][0],\n",
    "        \"lambda\" : best_params['reg_lambda'][0],\n",
    "        \"subsample\" : best_params['subsample'][0],\n",
    "        \"max_bin\" : int(best_params['max_bin'][0]),\n",
    "        \"eval_metric\":'auc',\n",
    "        \"objective\":'binary:logistic',\n",
    "        \"booster\":'gbtree'\n",
    "        #\"single_precision_histogram\" : True\n",
    "        }\n",
    "class_XGB = XGB_predictor(xgb_best)\n",
    "class_GCN = GCN_online_mining_test(gcn_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyper = {\n",
    "        \"colsample_bylevel\" : 0.5612301667238877,\n",
    "        \"colsample_bytree\" : 0.788688363076523,\n",
    "        \"gamma\" : 0.35376030016117566,\n",
    "        \"eta\" : 0.4023692255888918,\n",
    "        \"max_delta_step\" : int(3),\n",
    "        \"max_depth\" : int(8),\n",
    "        \"min_child_weight\" : int(70),\n",
    "        \"alpha\" : 0.15030685758880047,\n",
    "        \"lambda\" : 15.311721955443915,\n",
    "        \"subsample\" : 0.8303923929525608,\n",
    "        \"max_bin\" : int(208),\n",
    "        \"eval_metric\":'auc',\n",
    "        \"objective\":'binary:logistic',\n",
    "        \"booster\":'gbtree',\n",
    "        \"single_precision_histogram\" : True\n",
    "}\n",
    "class_XGB_2 = XGB_predictor(xgb_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2541 samples, validate on 509 samples\n",
      "Epoch 1/60\n",
      "2541/2541 [==============================] - 5s 2ms/step - loss: 0.5916 - val_loss: 0.5880\n",
      "Epoch 2/60\n",
      "2541/2541 [==============================] - 1s 395us/step - loss: 0.5805 - val_loss: 0.5770\n",
      "Epoch 3/60\n",
      "2541/2541 [==============================] - 1s 367us/step - loss: 0.5702 - val_loss: 0.5393\n",
      "Epoch 4/60\n",
      "2541/2541 [==============================] - 1s 380us/step - loss: 0.5606 - val_loss: 0.5404\n",
      "Epoch 5/60\n",
      "2541/2541 [==============================] - 1s 365us/step - loss: 0.5604 - val_loss: 0.5467\n",
      "Epoch 6/60\n",
      "2541/2541 [==============================] - 1s 379us/step - loss: 0.5580 - val_loss: 0.5460\n",
      "Epoch 7/60\n",
      "2541/2541 [==============================] - 1s 365us/step - loss: 0.5543 - val_loss: 0.5229\n",
      "Epoch 8/60\n",
      "2541/2541 [==============================] - 1s 374us/step - loss: 0.5582 - val_loss: 0.5319\n",
      "Epoch 9/60\n",
      "2541/2541 [==============================] - 1s 364us/step - loss: 0.5499 - val_loss: 0.5038\n",
      "Epoch 10/60\n",
      "2541/2541 [==============================] - 1s 375us/step - loss: 0.5520 - val_loss: 0.5493\n",
      "Epoch 11/60\n",
      "2541/2541 [==============================] - 1s 368us/step - loss: 0.5496 - val_loss: 0.5555\n",
      "Epoch 12/60\n",
      "2541/2541 [==============================] - 1s 377us/step - loss: 0.5376 - val_loss: 0.5554\n",
      "Epoch 13/60\n",
      "2541/2541 [==============================] - 1s 392us/step - loss: 0.5474 - val_loss: 0.5413\n",
      "Epoch 14/60\n",
      "2541/2541 [==============================] - 1s 374us/step - loss: 0.5401 - val_loss: 0.5310\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0009749509044922888.\n",
      "Epoch 15/60\n",
      "2541/2541 [==============================] - 1s 363us/step - loss: 0.5380 - val_loss: 0.5443\n",
      "Epoch 16/60\n",
      "2541/2541 [==============================] - 1s 379us/step - loss: 0.5334 - val_loss: 0.5228\n",
      "Epoch 17/60\n",
      "2541/2541 [==============================] - 1s 367us/step - loss: 0.5202 - val_loss: 0.5330\n",
      "Epoch 18/60\n",
      "2541/2541 [==============================] - 1s 378us/step - loss: 0.5275 - val_loss: 0.5294\n",
      "Epoch 19/60\n",
      "2541/2541 [==============================] - 3s 1ms/step - loss: 0.5124 - val_loss: 0.5431\n",
      "Epoch 20/60\n",
      "2541/2541 [==============================] - 5s 2ms/step - loss: 0.5124 - val_loss: 0.5175\n",
      "Epoch 21/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.5287 - val_loss: 0.5346\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0004874754522461444.\n",
      "Epoch 22/60\n",
      "2541/2541 [==============================] - 1s 379us/step - loss: 0.5064 - val_loss: 0.5450\n",
      "Epoch 23/60\n",
      "2541/2541 [==============================] - 1s 365us/step - loss: 0.5052 - val_loss: 0.5252\n",
      "Epoch 24/60\n",
      "2541/2541 [==============================] - 1s 400us/step - loss: 0.4961 - val_loss: 0.5316\n",
      "Epoch 25/60\n",
      "2541/2541 [==============================] - 2s 750us/step - loss: 0.5010 - val_loss: 0.5152\n",
      "Epoch 26/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.4969 - val_loss: 0.5322\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0002437377261230722.\n",
      "Epoch 27/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.4874 - val_loss: 0.5388\n",
      "Epoch 28/60\n",
      "2541/2541 [==============================] - 2s 755us/step - loss: 0.4849 - val_loss: 0.5399\n",
      "Epoch 29/60\n",
      "2541/2541 [==============================] - 1s 338us/step - loss: 0.4809 - val_loss: 0.5284\n",
      "Epoch 30/60\n",
      "2541/2541 [==============================] - 1s 345us/step - loss: 0.4777 - val_loss: 0.5317\n",
      "Epoch 31/60\n",
      "2541/2541 [==============================] - 1s 339us/step - loss: 0.4808 - val_loss: 0.5314\n",
      "Epoch 32/60\n",
      "2541/2541 [==============================] - 1s 359us/step - loss: 0.4843 - val_loss: 0.5401\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0001218688630615361.\n",
      "Epoch 33/60\n",
      "2541/2541 [==============================] - 1s 428us/step - loss: 0.4847 - val_loss: 0.5224\n",
      "Epoch 34/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.4781 - val_loss: 0.5244\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 6.093443153076805e-05.\n",
      "Epoch 35/60\n",
      "2541/2541 [==============================] - 4s 2ms/step - loss: 0.4696 - val_loss: 0.5263\n",
      "Epoch 36/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.4713 - val_loss: 0.5303\n",
      "Epoch 37/60\n",
      "2541/2541 [==============================] - 1s 478us/step - loss: 0.4644 - val_loss: 0.5221\n",
      "Epoch 38/60\n",
      "2541/2541 [==============================] - 1s 327us/step - loss: 0.4727 - val_loss: 0.5317\n",
      "Epoch 39/60\n",
      "2541/2541 [==============================] - 1s 345us/step - loss: 0.4699 - val_loss: 0.5275\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.0467215765384026e-05.\n",
      "Epoch 40/60\n",
      "2541/2541 [==============================] - 1s 326us/step - loss: 0.4581 - val_loss: 0.5306\n",
      "Epoch 41/60\n",
      "2541/2541 [==============================] - 1s 327us/step - loss: 0.4578 - val_loss: 0.5289\n",
      "Epoch 42/60\n",
      "2541/2541 [==============================] - 1s 454us/step - loss: 0.4713 - val_loss: 0.5314\n",
      "Epoch 43/60\n",
      "2541/2541 [==============================] - 4s 2ms/step - loss: 0.4514 - val_loss: 0.5303\n",
      "Epoch 44/60\n",
      "2541/2541 [==============================] - 5s 2ms/step - loss: 0.4728 - val_loss: 0.5314\n",
      "Epoch 45/60\n",
      "2541/2541 [==============================] - 4s 1ms/step - loss: 0.4604 - val_loss: 0.5270\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.5233607882692013e-05.\n",
      "Epoch 46/60\n",
      "2541/2541 [==============================] - 1s 365us/step - loss: 0.4721 - val_loss: 0.5315\n",
      "Epoch 47/60\n",
      "2541/2541 [==============================] - 1s 378us/step - loss: 0.4567 - val_loss: 0.5232\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 7.6168039413460065e-06.\n",
      "Epoch 48/60\n",
      "2541/2541 [==============================] - 1s 382us/step - loss: 0.4639 - val_loss: 0.5291\n",
      "Epoch 49/60\n",
      "2541/2541 [==============================] - 1s 380us/step - loss: 0.4645 - val_loss: 0.5219\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 3.8084019706730032e-06.\n",
      "Epoch 50/60\n",
      "2541/2541 [==============================] - 1s 398us/step - loss: 0.4545 - val_loss: 0.5263\n",
      "Epoch 51/60\n",
      "2541/2541 [==============================] - 2s 806us/step - loss: 0.4592 - val_loss: 0.5180\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.9042009853365016e-06.\n",
      "Epoch 52/60\n",
      "2541/2541 [==============================] - 4s 2ms/step - loss: 0.4722 - val_loss: 0.5219\n",
      "Epoch 53/60\n",
      "2541/2541 [==============================] - 4s 2ms/step - loss: 0.4576 - val_loss: 0.5248\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 9.521004926682508e-07.\n",
      "Epoch 54/60\n",
      "2541/2541 [==============================] - 5s 2ms/step - loss: 0.4623 - val_loss: 0.5280\n",
      "Epoch 55/60\n",
      "2541/2541 [==============================] - 5s 2ms/step - loss: 0.4662 - val_loss: 0.5263\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 4.760502463341254e-07.\n",
      "Epoch 56/60\n",
      "2541/2541 [==============================] - 1s 385us/step - loss: 0.4699 - val_loss: 0.5312\n",
      "Epoch 57/60\n",
      "2541/2541 [==============================] - 1s 379us/step - loss: 0.4690 - val_loss: 0.5238\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 2.380251231670627e-07.\n",
      "Epoch 58/60\n",
      "2541/2541 [==============================] - 1s 378us/step - loss: 0.4673 - val_loss: 0.5290\n",
      "[14:11:32] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\learner.cc:328: \n",
      "Parameters: { single_precision_histogram } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "training_metrics = {}\n",
    "validation_metrics = {}\n",
    "es2 = EarlyStopping(monitor='loss',patience=15, min_delta=0)\n",
    "rlr2 = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=2, verbose=1, min_lr=0.000000001)\n",
    "for i in range(0,1):#range(len(training_list)):\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = class_GCN.dataframe_to_gcn_input(validation_list[i])\n",
    "        Y_cold = validation_list[i].Binary \n",
    "        Y_dummy_cold = np.empty((X_atoms_cold.shape[0],gcn_params['dense_size'][2]+1))\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = class_GCN.dataframe_to_gcn_input(training_list[i])\n",
    "        Y = training_list[i].Binary\n",
    "        Y_dummy_train = np.empty((X_atoms_train.shape[0],gcn_params['dense_size'][2]+1))\n",
    "        \n",
    "        gcn_encoder = class_GCN.build_encoder()\n",
    "        gcn_model = class_GCN.build_model(gcn_encoder)\n",
    "        gcn_mining = class_GCN.build_mining(gcn_model)\n",
    "        \n",
    "        gcn_mining.fit([X_atoms_train,X_bonds_train,X_edges_train,Y],\n",
    "                       Y_dummy_train,\n",
    "                       epochs = gcn_params['n_epochs'],\n",
    "                       batch_size = gcn_params['batch_size'],\n",
    "                       shuffle = True,\n",
    "                       validation_data = ([X_atoms_cold,X_bonds_cold,X_edges_cold,Y_cold],Y_dummy_cold),\n",
    "                       callbacks=[es2,rlr2]\n",
    "                      )\n",
    "        #Predict Embeddings\n",
    "        embeddings_cold = gcn_model.predict([X_atoms_cold,X_bonds_cold,X_edges_cold])\n",
    "        embeddings_train = gcn_model.predict([X_atoms_train, X_bonds_train, X_edges_train])\n",
    "        \n",
    "        #Prepare data for XGBoost\n",
    "        dmatrix_train = class_XGB_2.to_xgb_input(Y,embeddings_train)\n",
    "        dmatrix_cold = class_XGB_2.to_xgb_input(Y_cold,embeddings_cold)\n",
    "        \n",
    "        evalist = [(dmatrix_train,'train'),(dmatrix_cold,'eval')]\n",
    "        xgb_model = class_XGB_2.build_model(dmatrix_train,evalist,300)\n",
    "        \n",
    "        xgb_pred_cold = xgb_model.predict(dmatrix_cold)\n",
    "        validation_metrics['Val_%s'%i] = calculate_metrics(np.array(Y_cold),xgb_pred_cold)\n",
    "        \n",
    "        xgb_pred_train = xgb_model.predict(dmatrix_train)\n",
    "        training_metrics['Train_%s'%i] = calculate_metrics(np.array(Y),xgb_pred_train)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Val_0': {'roc_auc': 0.8363488119273179,\n",
       "  'tn': 199,\n",
       "  'fp': 75,\n",
       "  'fn': 36,\n",
       "  'tp': 199,\n",
       "  'map': 0.7909681465148516,\n",
       "  'precision': 0.7262773722627737,\n",
       "  'recall': 0.8468085106382979,\n",
       "  'accuracy': 0.7819253438113949}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train_0': {'roc_auc': 0.9576354582117707,\n",
       "  'tn': 1081,\n",
       "  'fp': 211,\n",
       "  'fn': 85,\n",
       "  'tp': 1164,\n",
       "  'map': 0.9566848361539488,\n",
       "  'precision': 0.8465454545454546,\n",
       "  'recall': 0.9319455564451561,\n",
       "  'accuracy': 0.8835104289649744}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Val_0': {'roc_auc': 0.5,\n",
       "  'tn': 274,\n",
       "  'fp': 0,\n",
       "  'fn': 235,\n",
       "  'tp': 0,\n",
       "  'map': 0.46168958742632615,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.5383104125736738},\n",
       " 'Val_1': {'roc_auc': 0.5,\n",
       "  'tn': 295,\n",
       "  'fp': 0,\n",
       "  'fn': 214,\n",
       "  'tp': 0,\n",
       "  'map': 0.4204322200392927,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.5795677799607073},\n",
       " 'Val_2': {'roc_auc': 0.5,\n",
       "  'tn': 263,\n",
       "  'fp': 0,\n",
       "  'fn': 246,\n",
       "  'tp': 0,\n",
       "  'map': 0.48330058939096265,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.5166994106090373},\n",
       " 'Val_3': {'roc_auc': 0.5,\n",
       "  'tn': 262,\n",
       "  'fp': 0,\n",
       "  'fn': 247,\n",
       "  'tp': 0,\n",
       "  'map': 0.48526522593320237,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.5147347740667977},\n",
       " 'Val_4': {'roc_auc': 0.5,\n",
       "  'tn': 228,\n",
       "  'fp': 0,\n",
       "  'fn': 281,\n",
       "  'tp': 0,\n",
       "  'map': 0.5520628683693517,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.44793713163064836},\n",
       " 'Val_5': {'roc_auc': 0.5,\n",
       "  'tn': 244,\n",
       "  'fp': 0,\n",
       "  'fn': 261,\n",
       "  'tp': 0,\n",
       "  'map': 0.5168316831683168,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.48316831683168315}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train_0': {'roc_auc': 0.5,\n",
       "  'tn': 1292,\n",
       "  'fp': 0,\n",
       "  'fn': 1249,\n",
       "  'tp': 0,\n",
       "  'map': 0.491538764266037,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.508461235733963},\n",
       " 'Train_1': {'roc_auc': 0.5,\n",
       "  'tn': 1271,\n",
       "  'fp': 0,\n",
       "  'fn': 1270,\n",
       "  'tp': 0,\n",
       "  'map': 0.49980322707595437,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.5001967729240456},\n",
       " 'Train_2': {'roc_auc': 0.5,\n",
       "  'tn': 1303,\n",
       "  'fp': 0,\n",
       "  'fn': 1238,\n",
       "  'tp': 0,\n",
       "  'map': 0.48720975993703264,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.5127902400629674},\n",
       " 'Train_3': {'roc_auc': 0.5,\n",
       "  'tn': 1304,\n",
       "  'fp': 0,\n",
       "  'fn': 1237,\n",
       "  'tp': 0,\n",
       "  'map': 0.4868162140889414,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.5131837859110586},\n",
       " 'Train_4': {'roc_auc': 0.5,\n",
       "  'tn': 1338,\n",
       "  'fp': 0,\n",
       "  'fn': 1203,\n",
       "  'tp': 0,\n",
       "  'map': 0.4734356552538371,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.526564344746163},\n",
       " 'Train_5': {'roc_auc': 0.5,\n",
       "  'tn': 1322,\n",
       "  'fp': 0,\n",
       "  'fn': 1223,\n",
       "  'tp': 0,\n",
       "  'map': 0.4805500982318271,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'accuracy': 0.5194499017681729}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
