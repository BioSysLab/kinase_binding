{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.callbacks import History, ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from data_analysis import calculate_metrics\n",
    "from functools import partial\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pickle\n",
    "import dill\n",
    "from hyper_mining import objective_fn\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fspace = {\n",
    "    'conv1' : hp.quniform('conv1', 32, 64, 8),\n",
    "    'conv2' : hp.quniform('conv2', 64, 128, 8),\n",
    "    'conv3' : hp.quniform('conv3', 128, 168, 8),\n",
    "    'fp' : hp.quniform('fp', 96, 196, 8),\n",
    "    'dense1' : hp.quniform('dense1',96,512,32),\n",
    "    'dense2' : hp.quniform('dense2',96,512,32),\n",
    "    'dense3' : hp.quniform('dense3',64,512,32),\n",
    "    'dropout_rate' : hp.uniform('dropout_rate',0.1,0.5),\n",
    "    'lr' : hp.uniform('lr',0.000001,0.01),\n",
    "    'n_epochs' : hp.quniform('n_epochs',15,60,5),\n",
    "    'batch_size' : hp.quniform('batch_size',64,256,16),\n",
    "    'colsample_bylevel' : hp.uniform('colsample_bylevel', 0.1, 1), \n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0.1, 1), \n",
    "    'gamma' : hp.uniform('gamma', 0.1, 1), \n",
    "    'learning_rate' : hp.uniform('learning_rate', 0.1, 1),\n",
    "    'max_delta_step' : hp.quniform('max_delta_step',1,10,1),\n",
    "    'max_depth' : hp.quniform('max_depth',6, 12, 1),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight',10 ,500 ,5),\n",
    "    'reg_alpha' : hp.uniform('reg_alpha',0.1,100),\n",
    "    'reg_lambda' : hp.uniform('reg_lambda',0.1,100),\n",
    "    'subsample' : hp.uniform('subsample',0.1,1.0),\n",
    "    'max_bin' : hp.quniform('max_bin',16,256,16)\n",
    "    #'margin' : hp.uniform('margin',0.2,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'p38'\n",
    "base_path = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath = base_path+f'/data/{target}/data.csv'\n",
    "df=pd.read_csv(data_fpath).set_index('biolab_index')\n",
    "\n",
    "with open(base_path+f'/data/{target}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds = dill.load(in_f)\n",
    "with open(base_path+f'/data/{target}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds = dill.load(in_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list = [df.loc[train_val_folds[0][0]],\n",
    "                 df.loc[train_val_folds[1][0]],\n",
    "                 df.loc[train_val_folds[2][0]],\n",
    "                 df.loc[train_val_folds[3][0]],\n",
    "                 df.loc[train_val_folds[4][0]],\n",
    "                 df.loc[train_val_folds[5][0]],\n",
    "                 df.loc[train_test_folds[0]]\n",
    "                 ]\n",
    "validation_list = [df.loc[train_val_folds[0][1]],\n",
    "                   df.loc[train_val_folds[1][1]],\n",
    "                   df.loc[train_val_folds[2][1]],\n",
    "                   df.loc[train_val_folds[3][1]],\n",
    "                   df.loc[train_val_folds[4][1]],\n",
    "                   df.loc[train_val_folds[5][1]],\n",
    "                   df.loc[train_test_folds[1]]\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin_objective = partial(objective_fn, train_sets = training_list, val_sets = validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials():\n",
    "\n",
    "    trials_step = 0  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    max_trials = 1  # initial max_trials. put something small to not have to wait\n",
    "\n",
    "    \n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(\"gcn_xgb.hyperopt\", \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    best = fmin(fn = fmin_objective, space = fspace, algo=tpe.suggest, max_evals=max_trials, trials=trials)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    \n",
    "    # save the trials object\n",
    "    with open(\"gcn_xgb.hyperopt\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "    return(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved Trials! Loading...\n",
      "Rerunning from 154 trials to 154 (+0) trials\n",
      "100%|████████████████████████████████████████████████████████████████████████| 154/154 [00:00<?, ?trial/s, best loss=?]\n",
      "Best: {'batch_size': 288.0, 'colsample_bylevel': 0.4371082812232264, 'colsample_bytree': 0.4179415558635843, 'conv1': 56.0, 'conv2': 88.0, 'conv3': 136.0, 'dense1': 384.0, 'dense2': 288.0, 'dense3': 224.0, 'dropout_rate': 0.27225175676555935, 'fp': 152.0, 'gamma': 0.919836526180396, 'learning_rate': 0.41409388868400826, 'lr': 0.0008110012706176706, 'max_bin': 48.0, 'max_delta_step': 2.0, 'max_depth': 7.0, 'min_child_weight': 20.0, 'n_epochs': 25.0, 'reg_alpha': 42.8887552483495, 'reg_lambda': 12.306130216692438, 'subsample': 0.6038298323514097}\n"
     ]
    }
   ],
   "source": [
    "trials = run_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = trials.trials[0]['result']['loss']\n",
    "for i in range(1,len(trials.trials)):\n",
    "    if (trials.trials[i]['result']['loss'] <=  best_loss):\n",
    "        best_loss = trials.trials[i]['result']['loss']\n",
    "        index = i\n",
    "best_params = trials.trials[index]['misc']['vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyper_mining import XGB_predictor,GCN_online_mining_test\n",
    "from data_analysis import calculate_metrics\n",
    "es = EarlyStopping(monitor='loss',patience=8, min_delta=0)\n",
    "rlr = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=4, verbose=1, min_lr=0.0000001)\n",
    "gcn_best = {\n",
    "        \"num_layers\" : 3,\n",
    "        \"max_atoms\" : 70,\n",
    "        \"num_atom_features\" : 62,\n",
    "        \"num_atom_features_original\" : 62,\n",
    "        \"num_bond_features\" : 6,\n",
    "        \"max_degree\" : 5,\n",
    "        \"conv_width\" : [int(best_params['conv1'][0]), int(best_params['conv2'][0]), int(best_params['conv3'][0])],\n",
    "        \"fp_length\" : [int(best_params['fp'][0]), int(best_params['fp'][0]), int(best_params['fp'][0])],\n",
    "        \"activ_enc\" : \"selu\",\n",
    "        \"activ_dec\" : \"selu\",\n",
    "        \"learning_rates\" : [0.001,0.001,0.001],\n",
    "        \"learning_rates_fp\": [0.005,0.005,0.005],\n",
    "        \"losses_conv\" : {\n",
    "                    \"neighbor_output\": \"mean_squared_error\",\n",
    "                    \"self_output\": \"mean_squared_error\",\n",
    "                    },\n",
    "        \"lossWeights\" : {\"neighbor_output\": 1.0, \"self_output\": 1.0},\n",
    "        \"metrics\" : \"mse\",\n",
    "        \"loss_fp\" : \"mean_squared_error\",\n",
    "        \"enc_layer_names\" : [\"enc_1\", \"enc_2\", \"enc_3\"],\n",
    "        'callbacks' : [es,rlr],\n",
    "        'adam_decay': 0.0005329142291371636,\n",
    "        'beta': 5,\n",
    "        'p': 0.004465204118126482,\n",
    "        'dense_size' : [int(best_params['dense1'][0]), int(best_params['dense2'][0]), int(best_params['dense3'][0])],\n",
    "        'dropout_rate' : [best_params['dropout_rate'][0], best_params['dropout_rate'][0]],\n",
    "        'lr' : best_params['lr'][0],\n",
    "        'batch_size' : int(best_params['batch_size'][0]),\n",
    "        'n_epochs' : int(best_params['n_epochs'][0])\n",
    "        #'margin' : best_params['margin'][0]\n",
    "        }\n",
    "xgb_best = {\n",
    "        \"colsample_bylevel\" : best_params['colsample_bylevel'][0],\n",
    "        \"colsample_bytree\" : best_params['colsample_bytree'][0],\n",
    "        \"gamma\" : best_params['gamma'][0],\n",
    "        \"eta\" : best_params['learning_rate'][0],\n",
    "        \"max_delta_step\" : int(best_params['max_delta_step'][0]),\n",
    "        \"max_depth\" : int(best_params['max_depth'][0]),\n",
    "        \"min_child_weight\" : int(best_params['min_child_weight'][0]),\n",
    "        \"alpha\" : best_params['reg_alpha'][0],\n",
    "        \"lambda\" : best_params['reg_lambda'][0],\n",
    "        \"subsample\" : best_params['subsample'][0],\n",
    "        \"max_bin\" : int(best_params['max_bin'][0]),\n",
    "        \"eval_metric\":'auc',\n",
    "        \"objective\":'binary:logistic',\n",
    "        \"booster\":'gbtree'\n",
    "        #\"single_precision_histogram\" : True\n",
    "        }\n",
    "class_XGB = XGB_predictor(xgb_best)\n",
    "class_GCN = GCN_online_mining_test(gcn_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bylevel': 0.4371082812232264,\n",
       " 'colsample_bytree': 0.4179415558635843,\n",
       " 'gamma': 0.919836526180396,\n",
       " 'eta': 0.41409388868400826,\n",
       " 'max_delta_step': 2,\n",
       " 'max_depth': 7,\n",
       " 'min_child_weight': 20,\n",
       " 'alpha': 42.8887552483495,\n",
       " 'lambda': 12.306130216692438,\n",
       " 'subsample': 0.6038298323514097,\n",
       " 'max_bin': 48,\n",
       " 'eval_metric': 'auc',\n",
       " 'objective': 'binary:logistic',\n",
       " 'booster': 'gbtree'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2680 samples, validate on 537 samples\n",
      "Epoch 1/25\n",
      "2680/2680 [==============================] - 6s 2ms/step - loss: 0.9969 - val_loss: 0.9988\n",
      "Epoch 2/25\n",
      "2680/2680 [==============================] - 1s 281us/step - loss: 0.9961 - val_loss: 0.9970\n",
      "Epoch 3/25\n",
      "2680/2680 [==============================] - 1s 284us/step - loss: 0.9941 - val_loss: 0.9856\n",
      "Epoch 4/25\n",
      "2680/2680 [==============================] - 1s 284us/step - loss: 0.9888 - val_loss: 0.9818\n",
      "Epoch 5/25\n",
      "2680/2680 [==============================] - 1s 286us/step - loss: 0.9883 - val_loss: 0.9834\n",
      "Epoch 6/25\n",
      "2680/2680 [==============================] - 1s 280us/step - loss: 0.9810 - val_loss: 0.9841\n",
      "Epoch 7/25\n",
      "2680/2680 [==============================] - 1s 280us/step - loss: 0.9778 - val_loss: 0.9833\n",
      "Epoch 8/25\n",
      "2680/2680 [==============================] - 1s 286us/step - loss: 0.9773 - val_loss: 0.9820\n",
      "Epoch 9/25\n",
      "2680/2680 [==============================] - 1s 286us/step - loss: 0.9774 - val_loss: 0.9798\n",
      "Epoch 10/25\n",
      "2680/2680 [==============================] - 1s 291us/step - loss: 0.9732 - val_loss: 0.9814\n",
      "Epoch 11/25\n",
      "2680/2680 [==============================] - 1s 279us/step - loss: 0.9722 - val_loss: 0.9799\n",
      "Epoch 12/25\n",
      "2680/2680 [==============================] - 1s 281us/step - loss: 0.9663 - val_loss: 0.9831\n",
      "Epoch 13/25\n",
      "2680/2680 [==============================] - 1s 282us/step - loss: 0.9649 - val_loss: 0.9767\n",
      "Epoch 14/25\n",
      "2680/2680 [==============================] - 1s 286us/step - loss: 0.9674 - val_loss: 0.9759\n",
      "Epoch 15/25\n",
      "2680/2680 [==============================] - 1s 281us/step - loss: 0.9655 - val_loss: 0.9840\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 16/25\n",
      "2680/2680 [==============================] - 1s 279us/step - loss: 0.9653 - val_loss: 0.9793\n",
      "Epoch 17/25\n",
      "2680/2680 [==============================] - 1s 279us/step - loss: 0.9553 - val_loss: 0.9801\n",
      "Epoch 18/25\n",
      "2680/2680 [==============================] - ETA: 0s - loss: 0.968 - 1s 284us/step - loss: 0.9662 - val_loss: 0.9762\n",
      "Epoch 19/25\n",
      "2680/2680 [==============================] - 1s 279us/step - loss: 0.9347 - val_loss: 0.9782\n",
      "Epoch 20/25\n",
      "2680/2680 [==============================] - 1s 282us/step - loss: 0.9672 - val_loss: 0.9796\n",
      "Epoch 21/25\n",
      "2680/2680 [==============================] - 1s 281us/step - loss: 0.9508 - val_loss: 0.9750\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 22/25\n",
      "2680/2680 [==============================] - 1s 279us/step - loss: 0.9210 - val_loss: 0.9749\n",
      "Epoch 23/25\n",
      "2680/2680 [==============================] - 1s 281us/step - loss: 0.9523 - val_loss: 0.9816\n",
      "Epoch 24/25\n",
      "2680/2680 [==============================] - 1s 279us/step - loss: 0.9367 - val_loss: 0.9663\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 25/25\n",
      "2680/2680 [==============================] - 1s 280us/step - loss: 0.9183 - val_loss: 0.9768\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2680 samples, validate on 537 samples\n",
      "Epoch 1/25\n",
      "2680/2680 [==============================] - 6s 2ms/step - loss: 0.9967 - val_loss: 0.9986\n",
      "Epoch 2/25\n",
      "2680/2680 [==============================] - 1s 286us/step - loss: 0.9958 - val_loss: 0.9982\n",
      "Epoch 3/25\n",
      "2680/2680 [==============================] - 1s 284us/step - loss: 0.9950 - val_loss: 0.9970\n",
      "Epoch 4/25\n",
      "2680/2680 [==============================] - 1s 287us/step - loss: 0.9924 - val_loss: 0.9791\n",
      "Epoch 5/25\n",
      "2680/2680 [==============================] - 1s 279us/step - loss: 0.9852 - val_loss: 0.9751\n",
      "Epoch 6/25\n",
      "2680/2680 [==============================] - 1s 280us/step - loss: 0.9837 - val_loss: 0.9760\n",
      "Epoch 7/25\n",
      "2680/2680 [==============================] - 1s 296us/step - loss: 0.9785 - val_loss: 0.9644\n",
      "Epoch 8/25\n",
      "2680/2680 [==============================] - 1s 292us/step - loss: 0.9809 - val_loss: 0.9757\n",
      "Epoch 9/25\n",
      "2680/2680 [==============================] - 1s 286us/step - loss: 0.9791 - val_loss: 0.9775\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 10/25\n",
      "2680/2680 [==============================] - 1s 289us/step - loss: 0.9784 - val_loss: 0.9737\n",
      "Epoch 11/25\n",
      "2680/2680 [==============================] - 1s 281us/step - loss: 0.9740 - val_loss: 0.9756\n",
      "Epoch 12/25\n",
      "2680/2680 [==============================] - 1s 282us/step - loss: 0.9675 - val_loss: 0.9734\n",
      "Epoch 13/25\n",
      "2680/2680 [==============================] - 1s 287us/step - loss: 0.9746 - val_loss: 0.9734\n",
      "Epoch 14/25\n",
      "2680/2680 [==============================] - 1s 284us/step - loss: 0.9678 - val_loss: 0.9811\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 15/25\n",
      "2680/2680 [==============================] - 1s 285us/step - loss: 0.9749 - val_loss: 0.9740\n",
      "Epoch 16/25\n",
      "2680/2680 [==============================] - 1s 287us/step - loss: 0.9683 - val_loss: 0.9650\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 17/25\n",
      "2680/2680 [==============================] - 1s 285us/step - loss: 0.9580 - val_loss: 0.9655\n",
      "Epoch 18/25\n",
      "2680/2680 [==============================] - 1s 281us/step - loss: 0.9598 - val_loss: 0.9665\n",
      "Epoch 19/25\n",
      "2680/2680 [==============================] - 1s 293us/step - loss: 0.9529 - val_loss: 0.9568\n",
      "Epoch 20/25\n",
      "2680/2680 [==============================] - 1s 282us/step - loss: 0.9500 - val_loss: 0.9699\n",
      "Epoch 21/25\n",
      "2680/2680 [==============================] - 1s 287us/step - loss: 0.9520 - val_loss: 0.9642\n",
      "Epoch 22/25\n",
      "2680/2680 [==============================] - 1s 280us/step - loss: 0.9489 - val_loss: 0.9711\n",
      "Epoch 23/25\n",
      "2680/2680 [==============================] - 1s 282us/step - loss: 0.9218 - val_loss: 0.9607\n",
      "Epoch 24/25\n",
      "2680/2680 [==============================] - 1s 282us/step - loss: 0.9389 - val_loss: 0.9642\n",
      "Epoch 25/25\n",
      "2680/2680 [==============================] - 1s 281us/step - loss: 0.9391 - val_loss: 0.9586\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 5.06875803694129e-05.\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2680 samples, validate on 537 samples\n",
      "Epoch 1/25\n",
      "2680/2680 [==============================] - 7s 3ms/step - loss: 0.9968 - val_loss: 0.9989\n",
      "Epoch 2/25\n",
      "2680/2680 [==============================] - 1s 286us/step - loss: 0.9964 - val_loss: 0.9980\n",
      "Epoch 3/25\n",
      "2680/2680 [==============================] - 1s 288us/step - loss: 0.9957 - val_loss: 0.9872\n",
      "Epoch 4/25\n",
      "2680/2680 [==============================] - 1s 288us/step - loss: 0.9928 - val_loss: 0.9850\n",
      "Epoch 5/25\n",
      "2680/2680 [==============================] - 1s 287us/step - loss: 0.9878 - val_loss: 0.9837\n",
      "Epoch 6/25\n",
      "2680/2680 [==============================] - 1s 296us/step - loss: 0.9838 - val_loss: 0.9793\n",
      "Epoch 7/25\n",
      "2680/2680 [==============================] - 1s 292us/step - loss: 0.9819 - val_loss: 0.9813\n",
      "Epoch 8/25\n",
      "2680/2680 [==============================] - 1s 285us/step - loss: 0.9797 - val_loss: 0.9807\n",
      "Epoch 9/25\n",
      "2680/2680 [==============================] - 1s 284us/step - loss: 0.9769 - val_loss: 0.9806\n",
      "Epoch 10/25\n",
      "2680/2680 [==============================] - 1s 287us/step - loss: 0.9778 - val_loss: 0.9804\n",
      "Epoch 11/25\n",
      "2680/2680 [==============================] - 1s 292us/step - loss: 0.9723 - val_loss: 0.9802\n",
      "Epoch 12/25\n",
      "2680/2680 [==============================] - 1s 284us/step - loss: 0.9786 - val_loss: 0.9784\n",
      "Epoch 13/25\n",
      "2680/2680 [==============================] - 1s 285us/step - loss: 0.9755 - val_loss: 0.9798\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 14/25\n",
      "2680/2680 [==============================] - 1s 285us/step - loss: 0.9656 - val_loss: 0.9765\n",
      "Epoch 15/25\n",
      "2680/2680 [==============================] - 1s 283us/step - loss: 0.9539 - val_loss: 0.9769\n",
      "Epoch 16/25\n",
      "2680/2680 [==============================] - 1s 284us/step - loss: 0.9695 - val_loss: 0.9785\n",
      "Epoch 17/25\n",
      "2680/2680 [==============================] - 1s 289us/step - loss: 0.9612 - val_loss: 0.9652\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 18/25\n",
      "2680/2680 [==============================] - 1s 285us/step - loss: 0.9635 - val_loss: 0.9771\n",
      "Epoch 19/25\n",
      "2680/2680 [==============================] - 1s 283us/step - loss: 0.9687 - val_loss: 0.9799\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 20/25\n",
      "2680/2680 [==============================] - 1s 290us/step - loss: 0.9607 - val_loss: 0.9741\n",
      "Epoch 21/25\n",
      "2680/2680 [==============================] - 1s 287us/step - loss: 0.9576 - val_loss: 0.9740\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 5.06875803694129e-05.\n",
      "Epoch 22/25\n",
      "2680/2680 [==============================] - 1s 287us/step - loss: 0.9528 - val_loss: 0.9709\n",
      "Epoch 23/25\n",
      "2680/2680 [==============================] - 1s 285us/step - loss: 0.9555 - val_loss: 0.9711\n",
      "Epoch 24/25\n",
      "2680/2680 [==============================] - 1s 291us/step - loss: 0.9537 - val_loss: 0.9738\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 2.534379018470645e-05.\n",
      "Epoch 25/25\n",
      "2680/2680 [==============================] - 1s 283us/step - loss: 0.9475 - val_loss: 0.9711\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2680 samples, validate on 537 samples\n",
      "Epoch 1/25\n",
      "2680/2680 [==============================] - 8s 3ms/step - loss: 0.9968 - val_loss: 0.9987\n",
      "Epoch 2/25\n",
      "2680/2680 [==============================] - 1s 287us/step - loss: 0.9965 - val_loss: 0.9980\n",
      "Epoch 3/25\n",
      "2680/2680 [==============================] - 1s 286us/step - loss: 0.9951 - val_loss: 0.9880\n",
      "Epoch 4/25\n",
      "2680/2680 [==============================] - 1s 288us/step - loss: 0.9905 - val_loss: 0.9978\n",
      "Epoch 5/25\n",
      "2680/2680 [==============================] - 1s 285us/step - loss: 0.9852 - val_loss: 0.9799\n",
      "Epoch 6/25\n",
      "2680/2680 [==============================] - 1s 284us/step - loss: 0.9818 - val_loss: 0.9783\n",
      "Epoch 7/25\n",
      "2680/2680 [==============================] - 1s 287us/step - loss: 0.9802 - val_loss: 0.9657\n",
      "Epoch 8/25\n",
      "2680/2680 [==============================] - 1s 286us/step - loss: 0.9778 - val_loss: 0.9748\n",
      "Epoch 9/25\n",
      "2680/2680 [==============================] - 1s 285us/step - loss: 0.9751 - val_loss: 0.9787\n",
      "Epoch 10/25\n",
      "2680/2680 [==============================] - 1s 284us/step - loss: 0.9759 - val_loss: 0.9648\n",
      "Epoch 11/25\n",
      "2680/2680 [==============================] - 1s 287us/step - loss: 0.9742 - val_loss: 0.9724\n",
      "Epoch 12/25\n",
      "2680/2680 [==============================] - 1s 287us/step - loss: 0.9710 - val_loss: 0.9637\n",
      "Epoch 13/25\n",
      "2680/2680 [==============================] - 1s 289us/step - loss: 0.9678 - val_loss: 0.9843\n",
      "Epoch 14/25\n",
      "2680/2680 [==============================] - 1s 292us/step - loss: 0.9728 - val_loss: 0.9829\n",
      "Epoch 15/25\n",
      "2680/2680 [==============================] - 1s 288us/step - loss: 0.9656 - val_loss: 0.9808\n",
      "Epoch 16/25\n",
      "2680/2680 [==============================] - 1s 292us/step - loss: 0.9913 - val_loss: 1.0002\n",
      "Epoch 17/25\n",
      "2680/2680 [==============================] - 1s 288us/step - loss: 0.9988 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 18/25\n",
      "2680/2680 [==============================] - 1s 287us/step - loss: 0.9988 - val_loss: 1.0000\n",
      "Epoch 19/25\n",
      "2680/2680 [==============================] - 1s 291us/step - loss: 0.9988 - val_loss: 1.0001\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 20/25\n",
      "2680/2680 [==============================] - 1s 286us/step - loss: 0.9985 - val_loss: 1.0000\n",
      "Epoch 21/25\n",
      "2680/2680 [==============================] - 1s 286us/step - loss: 0.9985 - val_loss: 0.9999\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 22/25\n",
      "2680/2680 [==============================] - 1s 294us/step - loss: 0.9983 - val_loss: 0.9998\n",
      "Epoch 23/25\n",
      "2680/2680 [==============================] - 1s 291us/step - loss: 0.9983 - val_loss: 0.9994\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 5.06875803694129e-05.\n",
      "Epoch 24/25\n",
      "2680/2680 [==============================] - 1s 292us/step - loss: 0.9981 - val_loss: 0.9995\n",
      "Epoch 25/25\n",
      "2680/2680 [==============================] - 1s 286us/step - loss: 0.9979 - val_loss: 0.9994\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.534379018470645e-05.\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2680 samples, validate on 537 samples\n",
      "Epoch 1/25\n",
      "2680/2680 [==============================] - 9s 3ms/step - loss: 0.9967 - val_loss: 0.9979\n",
      "Epoch 2/25\n",
      "2680/2680 [==============================] - 1s 299us/step - loss: 0.9963 - val_loss: 0.9957\n",
      "Epoch 3/25\n",
      "2680/2680 [==============================] - 1s 286us/step - loss: 0.9946 - val_loss: 0.9827\n",
      "Epoch 4/25\n",
      "2680/2680 [==============================] - 1s 295us/step - loss: 0.9902 - val_loss: 0.9758\n",
      "Epoch 5/25\n",
      "2680/2680 [==============================] - 1s 301us/step - loss: 0.9859 - val_loss: 0.9748\n",
      "Epoch 6/25\n",
      "2680/2680 [==============================] - 1s 295us/step - loss: 0.9813 - val_loss: 0.9776\n",
      "Epoch 7/25\n",
      "2680/2680 [==============================] - 1s 298us/step - loss: 0.9840 - val_loss: 0.9749\n",
      "Epoch 8/25\n",
      "2680/2680 [==============================] - 1s 302us/step - loss: 0.9797 - val_loss: 0.9710\n",
      "Epoch 9/25\n",
      "2680/2680 [==============================] - 1s 306us/step - loss: 0.9770 - val_loss: 0.9700\n",
      "Epoch 10/25\n",
      "2680/2680 [==============================] - 1s 308us/step - loss: 0.9736 - val_loss: 0.9767\n",
      "Epoch 11/25\n",
      "2680/2680 [==============================] - 1s 310us/step - loss: 0.9746 - val_loss: 0.9730\n",
      "Epoch 12/25\n",
      "2680/2680 [==============================] - 1s 316us/step - loss: 0.9703 - val_loss: 0.9755\n",
      "Epoch 13/25\n",
      "2680/2680 [==============================] - 1s 308us/step - loss: 0.9739 - val_loss: 0.9642\n",
      "Epoch 14/25\n",
      "2680/2680 [==============================] - 1s 302us/step - loss: 0.9710 - val_loss: 0.9716\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 15/25\n",
      "2680/2680 [==============================] - 1s 329us/step - loss: 0.9778 - val_loss: 0.9708\n",
      "Epoch 16/25\n",
      "2680/2680 [==============================] - 1s 299us/step - loss: 0.9660 - val_loss: 0.9740\n",
      "Epoch 17/25\n",
      "2680/2680 [==============================] - 1s 296us/step - loss: 0.9695 - val_loss: 0.9717\n",
      "Epoch 18/25\n",
      "2680/2680 [==============================] - 1s 297us/step - loss: 0.9621 - val_loss: 0.9610\n",
      "Epoch 19/25\n",
      "2680/2680 [==============================] - 1s 291us/step - loss: 0.9544 - val_loss: 0.9745\n",
      "Epoch 20/25\n",
      "2680/2680 [==============================] - ETA: 0s - loss: 0.971 - 1s 294us/step - loss: 0.9702 - val_loss: 0.9687\n",
      "Epoch 21/25\n",
      "2680/2680 [==============================] - 1s 286us/step - loss: 0.9500 - val_loss: 0.9649\n",
      "Epoch 22/25\n",
      "2680/2680 [==============================] - 1s 294us/step - loss: 0.9494 - val_loss: 0.9741\n",
      "Epoch 23/25\n",
      "2680/2680 [==============================] - 1s 294us/step - loss: 0.9607 - val_loss: 0.9683\n",
      "Epoch 24/25\n",
      "2680/2680 [==============================] - 1s 292us/step - loss: 0.9399 - val_loss: 0.9352\n",
      "Epoch 25/25\n",
      "2680/2680 [==============================] - 1s 288us/step - loss: 0.9437 - val_loss: 0.9736\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2685 samples, validate on 532 samples\n",
      "Epoch 1/25\n",
      "2685/2685 [==============================] - 10s 4ms/step - loss: 0.9966 - val_loss: 0.9984\n",
      "Epoch 2/25\n",
      "2685/2685 [==============================] - 1s 292us/step - loss: 0.9961 - val_loss: 0.9971\n",
      "Epoch 3/25\n",
      "2685/2685 [==============================] - 1s 290us/step - loss: 0.9949 - val_loss: 0.9874\n",
      "Epoch 4/25\n",
      "2685/2685 [==============================] - 1s 288us/step - loss: 0.9923 - val_loss: 0.9828\n",
      "Epoch 5/25\n",
      "2685/2685 [==============================] - 1s 295us/step - loss: 0.9870 - val_loss: 0.9790\n",
      "Epoch 6/25\n",
      "2685/2685 [==============================] - 1s 291us/step - loss: 0.9851 - val_loss: 0.9700\n",
      "Epoch 7/25\n",
      "2685/2685 [==============================] - 1s 292us/step - loss: 0.9817 - val_loss: 0.9753\n",
      "Epoch 8/25\n",
      "2685/2685 [==============================] - 1s 291us/step - loss: 0.9840 - val_loss: 0.9768\n",
      "Epoch 9/25\n",
      "2685/2685 [==============================] - 1s 288us/step - loss: 0.9784 - val_loss: 0.9764\n",
      "Epoch 10/25\n",
      "2685/2685 [==============================] - 1s 289us/step - loss: 0.9801 - val_loss: 0.9744\n",
      "Epoch 11/25\n",
      "2685/2685 [==============================] - 1s 287us/step - loss: 0.9772 - val_loss: 0.9785\n",
      "Epoch 12/25\n",
      "2685/2685 [==============================] - 1s 286us/step - loss: 0.9746 - val_loss: 0.9747\n",
      "Epoch 13/25\n",
      "2685/2685 [==============================] - 1s 288us/step - loss: 0.9751 - val_loss: 0.9762\n",
      "Epoch 14/25\n",
      "2685/2685 [==============================] - 1s 291us/step - loss: 0.9721 - val_loss: 0.9764\n",
      "Epoch 15/25\n",
      "2685/2685 [==============================] - 1s 286us/step - loss: 0.9617 - val_loss: 0.9698\n",
      "Epoch 16/25\n",
      "2685/2685 [==============================] - 1s 290us/step - loss: 0.9720 - val_loss: 0.9757\n",
      "Epoch 17/25\n",
      "2685/2685 [==============================] - 1s 288us/step - loss: 0.9644 - val_loss: 0.9774\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 18/25\n",
      "2685/2685 [==============================] - 1s 304us/step - loss: 0.9711 - val_loss: 0.9710\n",
      "Epoch 19/25\n",
      "2685/2685 [==============================] - 1s 297us/step - loss: 0.9663 - val_loss: 0.9723\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 20/25\n",
      "2685/2685 [==============================] - 1s 291us/step - loss: 0.9583 - val_loss: 0.9721\n",
      "Epoch 21/25\n",
      "2685/2685 [==============================] - 1s 286us/step - loss: 0.9529 - val_loss: 0.9699\n",
      "Epoch 22/25\n",
      "2685/2685 [==============================] - 1s 291us/step - loss: 0.9523 - val_loss: 0.9742\n",
      "Epoch 23/25\n",
      "2685/2685 [==============================] - 1s 290us/step - loss: 0.9445 - val_loss: 0.9556\n",
      "Epoch 24/25\n",
      "2685/2685 [==============================] - 1s 292us/step - loss: 0.9391 - val_loss: 0.9638\n",
      "Epoch 25/25\n",
      "2685/2685 [==============================] - 1s 290us/step - loss: 0.9519 - val_loss: 0.9711\n",
      "LAYER 0\n",
      "LAYER 1\n",
      "LAYER 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3217 samples, validate on 537 samples\n",
      "Epoch 1/25\n",
      "3217/3217 [==============================] - 11s 3ms/step - loss: 0.9968 - val_loss: 0.9976\n",
      "Epoch 2/25\n",
      "3217/3217 [==============================] - 1s 297us/step - loss: 0.9962 - val_loss: 0.9909\n",
      "Epoch 3/25\n",
      "3217/3217 [==============================] - 1s 296us/step - loss: 0.9937 - val_loss: 0.9835\n",
      "Epoch 4/25\n",
      "3217/3217 [==============================] - 1s 289us/step - loss: 0.9872 - val_loss: 0.9676\n",
      "Epoch 5/25\n",
      "3217/3217 [==============================] - 1s 293us/step - loss: 0.9843 - val_loss: 0.9787\n",
      "Epoch 6/25\n",
      "3217/3217 [==============================] - 1s 292us/step - loss: 0.9823 - val_loss: 0.9738\n",
      "Epoch 7/25\n",
      "3217/3217 [==============================] - 1s 290us/step - loss: 0.9814 - val_loss: 0.9745\n",
      "Epoch 8/25\n",
      "3217/3217 [==============================] - 1s 291us/step - loss: 0.9789 - val_loss: 0.9747\n",
      "Epoch 9/25\n",
      "3217/3217 [==============================] - 1s 293us/step - loss: 0.9753 - val_loss: 0.9816\n",
      "Epoch 10/25\n",
      "3217/3217 [==============================] - 1s 290us/step - loss: 0.9707 - val_loss: 0.9840\n",
      "Epoch 11/25\n",
      "3217/3217 [==============================] - 1s 291us/step - loss: 0.9723 - val_loss: 0.9714\n",
      "Epoch 12/25\n",
      "3217/3217 [==============================] - 1s 289us/step - loss: 0.9770 - val_loss: 0.9808\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0004055006429553032.\n",
      "Epoch 13/25\n",
      "3217/3217 [==============================] - 1s 307us/step - loss: 0.9716 - val_loss: 0.9776\n",
      "Epoch 14/25\n",
      "3217/3217 [==============================] - 1s 289us/step - loss: 0.9642 - val_loss: 0.9804\n",
      "Epoch 15/25\n",
      "3217/3217 [==============================] - 1s 290us/step - loss: 0.9772 - val_loss: 0.9832\n",
      "Epoch 16/25\n",
      "3217/3217 [==============================] - 1s 293us/step - loss: 0.9673 - val_loss: 0.9693\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0002027503214776516.\n",
      "Epoch 17/25\n",
      "3217/3217 [==============================] - 1s 291us/step - loss: 0.9613 - val_loss: 0.9761\n",
      "Epoch 18/25\n",
      "3217/3217 [==============================] - 1s 293us/step - loss: 0.9625 - val_loss: 0.9787\n",
      "Epoch 19/25\n",
      "3217/3217 [==============================] - 1s 291us/step - loss: 0.9700 - val_loss: 0.9827\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0001013751607388258.\n",
      "Epoch 20/25\n",
      "3217/3217 [==============================] - 1s 293us/step - loss: 0.9699 - val_loss: 0.9803\n",
      "Epoch 21/25\n",
      "3217/3217 [==============================] - 1s 291us/step - loss: 0.9528 - val_loss: 0.9740\n",
      "Epoch 22/25\n",
      "3217/3217 [==============================] - 1s 300us/step - loss: 0.9386 - val_loss: 0.9727\n",
      "Epoch 23/25\n",
      "3217/3217 [==============================] - 1s 293us/step - loss: 0.9532 - val_loss: 0.9801\n",
      "Epoch 24/25\n",
      "3217/3217 [==============================] - 1s 290us/step - loss: 0.9497 - val_loss: 0.9753\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 5.06875803694129e-05.\n",
      "Epoch 25/25\n",
      "3217/3217 [==============================] - 1s 292us/step - loss: 0.9291 - val_loss: 0.9675\n"
     ]
    }
   ],
   "source": [
    "#K.clear_session()\n",
    "training_metrics = {}\n",
    "validation_metrics = {}\n",
    "es2 = EarlyStopping(monitor='loss',patience=15, min_delta=0)\n",
    "rlr2 = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=2, verbose=1, min_lr=0.000000001)\n",
    "for i in range(len(training_list)):\n",
    "        X_atoms_cold,X_bonds_cold,X_edges_cold = class_GCN.dataframe_to_gcn_input(validation_list[i])\n",
    "        Y_cold = validation_list[i].Binary \n",
    "        Y_dummy_cold = np.empty((X_atoms_cold.shape[0],gcn_best['dense_size'][2]+1))\n",
    "        X_atoms_train, X_bonds_train, X_edges_train = class_GCN.dataframe_to_gcn_input(training_list[i])\n",
    "        Y = training_list[i].Binary\n",
    "        Y_dummy_train = np.empty((X_atoms_train.shape[0],gcn_best['dense_size'][2]+1))\n",
    "        \n",
    "        gcn_encoder = class_GCN.build_encoder()\n",
    "        gcn_model = class_GCN.build_model(gcn_encoder)\n",
    "        gcn_mining = class_GCN.build_mining(gcn_model)\n",
    "        \n",
    "        gcn_mining.fit([X_atoms_train,X_bonds_train,X_edges_train,Y],\n",
    "                       Y_dummy_train,\n",
    "                       epochs = gcn_best['n_epochs'],\n",
    "                       batch_size = gcn_best['batch_size'],\n",
    "                       shuffle = True,\n",
    "                       validation_data = ([X_atoms_cold,X_bonds_cold,X_edges_cold,Y_cold],Y_dummy_cold),\n",
    "                       callbacks=[es2,rlr2]\n",
    "                      )\n",
    "        #Predict Embeddings\n",
    "        embeddings_cold = gcn_model.predict([X_atoms_cold,X_bonds_cold,X_edges_cold])\n",
    "        embeddings_train = gcn_model.predict([X_atoms_train, X_bonds_train, X_edges_train])\n",
    "        \n",
    "        #Prepare data for XGBoost\n",
    "        dmatrix_train = class_XGB.to_xgb_input(Y,embeddings_train)\n",
    "        dmatrix_cold = class_XGB.to_xgb_input(Y_cold,embeddings_cold)\n",
    "        \n",
    "        evalist = [(dmatrix_train,'train'),(dmatrix_cold,'eval')]\n",
    "        xgb_model = class_XGB.build_model(dmatrix_train,evalist,300)\n",
    "        xgb_pred_train = xgb_model.predict(dmatrix_train)\n",
    "        xgb_pred_cold = xgb_model.predict(dmatrix_cold)\n",
    "        \n",
    "        if i < 6 :\n",
    "            validation_metrics['Fold_%s'%i] = calculate_metrics(np.array(Y_cold),xgb_pred_cold)\n",
    "            training_metrics['Fold_%s'%i] = calculate_metrics(np.array(Y),xgb_pred_train)\n",
    "        elif i == 6:\n",
    "            validation_metrics['Test'] = calculate_metrics(np.array(Y_cold),xgb_pred_cold)\n",
    "            training_metrics['Test'] = calculate_metrics(np.array(Y),xgb_pred_train)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>map</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fold_0</th>\n",
       "      <td>0.808145</td>\n",
       "      <td>275.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.710781</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.765363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_1</th>\n",
       "      <td>0.849271</td>\n",
       "      <td>291.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.716986</td>\n",
       "      <td>0.629268</td>\n",
       "      <td>0.758824</td>\n",
       "      <td>0.782123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_2</th>\n",
       "      <td>0.852865</td>\n",
       "      <td>293.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.773155</td>\n",
       "      <td>0.754011</td>\n",
       "      <td>0.712121</td>\n",
       "      <td>0.808194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_3</th>\n",
       "      <td>0.843115</td>\n",
       "      <td>317.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.733481</td>\n",
       "      <td>0.632258</td>\n",
       "      <td>0.601227</td>\n",
       "      <td>0.772812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_4</th>\n",
       "      <td>0.899018</td>\n",
       "      <td>299.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.810340</td>\n",
       "      <td>0.758427</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.808194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_5</th>\n",
       "      <td>0.842660</td>\n",
       "      <td>185.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>0.802736</td>\n",
       "      <td>0.707395</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.761278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.841000</td>\n",
       "      <td>303.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.678572</td>\n",
       "      <td>0.681564</td>\n",
       "      <td>0.689266</td>\n",
       "      <td>0.791434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         roc_auc     tn    fp    fn     tp       map  precision    recall  \\\n",
       "Fold_0  0.808145  275.0  54.0  72.0  136.0  0.710781   0.715789  0.653846   \n",
       "Fold_1  0.849271  291.0  76.0  41.0  129.0  0.716986   0.629268  0.758824   \n",
       "Fold_2  0.852865  293.0  46.0  57.0  141.0  0.773155   0.754011  0.712121   \n",
       "Fold_3  0.843115  317.0  57.0  65.0   98.0  0.733481   0.632258  0.601227   \n",
       "Fold_4  0.899018  299.0  43.0  60.0  135.0  0.810340   0.758427  0.692308   \n",
       "Fold_5  0.842660  185.0  91.0  36.0  220.0  0.802736   0.707395  0.859375   \n",
       "Test    0.841000  303.0  57.0  55.0  122.0  0.678572   0.681564  0.689266   \n",
       "\n",
       "        accuracy  \n",
       "Fold_0  0.765363  \n",
       "Fold_1  0.782123  \n",
       "Fold_2  0.808194  \n",
       "Fold_3  0.772812  \n",
       "Fold_4  0.808194  \n",
       "Fold_5  0.761278  \n",
       "Test    0.791434  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(validation_metrics).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>map</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fold_0</th>\n",
       "      <td>0.952009</td>\n",
       "      <td>1499.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>882.0</td>\n",
       "      <td>0.899147</td>\n",
       "      <td>0.815911</td>\n",
       "      <td>0.898167</td>\n",
       "      <td>0.888433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_1</th>\n",
       "      <td>0.941182</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>0.897686</td>\n",
       "      <td>0.805009</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.873881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_2</th>\n",
       "      <td>0.941510</td>\n",
       "      <td>1494.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>860.0</td>\n",
       "      <td>0.896704</td>\n",
       "      <td>0.815939</td>\n",
       "      <td>0.866935</td>\n",
       "      <td>0.878358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_3</th>\n",
       "      <td>0.919076</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>0.868734</td>\n",
       "      <td>0.798677</td>\n",
       "      <td>0.822785</td>\n",
       "      <td>0.852612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_4</th>\n",
       "      <td>0.941236</td>\n",
       "      <td>1491.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>849.0</td>\n",
       "      <td>0.893681</td>\n",
       "      <td>0.813998</td>\n",
       "      <td>0.853266</td>\n",
       "      <td>0.873134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_5</th>\n",
       "      <td>0.939323</td>\n",
       "      <td>1554.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>789.0</td>\n",
       "      <td>0.877803</td>\n",
       "      <td>0.800203</td>\n",
       "      <td>0.844754</td>\n",
       "      <td>0.872626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.938083</td>\n",
       "      <td>1786.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.886696</td>\n",
       "      <td>0.808274</td>\n",
       "      <td>0.853782</td>\n",
       "      <td>0.870998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         roc_auc      tn     fp     fn      tp       map  precision    recall  \\\n",
       "Fold_0  0.952009  1499.0  199.0  100.0   882.0  0.899147   0.815911  0.898167   \n",
       "Fold_1  0.941182  1442.0  218.0  120.0   900.0  0.897686   0.805009  0.882353   \n",
       "Fold_2  0.941510  1494.0  194.0  132.0   860.0  0.896704   0.815939  0.866935   \n",
       "Fold_3  0.919076  1440.0  213.0  182.0   845.0  0.868734   0.798677  0.822785   \n",
       "Fold_4  0.941236  1491.0  194.0  146.0   849.0  0.893681   0.813998  0.853266   \n",
       "Fold_5  0.939323  1554.0  197.0  145.0   789.0  0.877803   0.800203  0.844754   \n",
       "Test    0.938083  1786.0  241.0  174.0  1016.0  0.886696   0.808274  0.853782   \n",
       "\n",
       "        accuracy  \n",
       "Fold_0  0.888433  \n",
       "Fold_1  0.873881  \n",
       "Fold_2  0.878358  \n",
       "Fold_3  0.852612  \n",
       "Fold_4  0.873134  \n",
       "Fold_5  0.872626  \n",
       "Test    0.870998  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(training_metrics).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
