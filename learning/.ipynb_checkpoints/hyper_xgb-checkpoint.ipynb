{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\tomas\\miniconda3\\envs\\binding\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import functools\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, roc_auc_score, \\\n",
    "    auc, average_precision_score, pairwise_distances\n",
    "import scikitplot as skplt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgb_hyper import objective\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pickle\n",
    "import dill\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following GPU devices ara available: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)\n",
    "print(\"The following GPU devices ara available: %s\"%tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, plots=False):\n",
    "    assert isinstance(y_true, np.ndarray), 'y_true should be np.array'\n",
    "    assert len(y_true.shape) == len(y_pred.shape) == 1, 'y_true or y_pred shapes are not 1 (probably not squeezed)'\n",
    "    y_pred_bin = y_pred > 0.5\n",
    "\n",
    "    cf = confusion_matrix(y_true, y_pred_bin)\n",
    "    tn, fp, fn, tp = cf.ravel()\n",
    "\n",
    "    metrics = {\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred),\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tp': tp,\n",
    "        'map': average_precision_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred_bin),\n",
    "        'recall': recall_score(y_true, y_pred_bin),\n",
    "        'accuracy': accuracy_score(y_true, y_pred_bin),\n",
    "    }\n",
    "\n",
    "    if plots:\n",
    "        print('predictions histogram')\n",
    "        plt.figure()\n",
    "        plt.hist(y_pred, bins=int(len(y_pred) / 3))\n",
    "        plt.show()\n",
    "\n",
    "        print('confusion matrix')\n",
    "        plt.figure()\n",
    "        group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
    "        group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                        cf.flatten()]\n",
    "        group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                             cf.flatten() / np.sum(cf)]\n",
    "        labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n",
    "                  zip(group_names, group_counts, group_percentages)]\n",
    "        labels = np.asarray(labels).reshape(2, 2)\n",
    "        sns.heatmap(cf, annot=labels, fmt='', cmap='Blues')\n",
    "        plt.show()\n",
    "\n",
    "        print('roc curve')\n",
    "        random_probs = [0 for _ in range(len(y_true))]\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "        print('Logistic: ROC AUC=%.3f' % (auc))\n",
    "        ns_fpr, ns_tpr, _ = roc_curve(y_true, random_probs)\n",
    "        lr_fpr, lr_tpr, _ = roc_curve(y_true, y_pred)\n",
    "        plt.plot(ns_fpr, ns_tpr, linestyle='--', label='random')\n",
    "        plt.plot(lr_fpr, lr_tpr, marker='.')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'p38'\n",
    "base_path = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath = base_path+f'/data/{target}/data.csv'\n",
    "df=pd.read_csv(data_fpath).set_index('biolab_index')\n",
    "\n",
    "with open(base_path+f'/data/{target}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds = dill.load(in_f)\n",
    "with open(base_path+f'/data/{target}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds = dill.load(in_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list = [df.loc[train_val_folds[0][0]],\n",
    "                 df.loc[train_val_folds[1][0]],\n",
    "                 df.loc[train_val_folds[2][0]],\n",
    "                 df.loc[train_val_folds[3][0]],\n",
    "                 df.loc[train_val_folds[4][0]],\n",
    "                 df.loc[train_val_folds[5][0]],\n",
    "                 ]\n",
    "validation_list = [df.loc[train_val_folds[0][1]],\n",
    "                   df.loc[train_val_folds[1][1]],\n",
    "                   df.loc[train_val_folds[2][1]],\n",
    "                   df.loc[train_val_folds[3][1]],\n",
    "                   df.loc[train_val_folds[4][1]],\n",
    "                   df.loc[train_val_folds[5][1]],\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the space\n",
    "fspace = {\n",
    "    'colsample_bylevel' : hp.uniform('colsample_bylevel', 0.1, 1), #+\n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0.1, 1), #+\n",
    "    'gamma' : hp.uniform('gamma', 0.1, 1), #+\n",
    "    'learning_rate' : hp.uniform('learning_rate', 0.1, 1),\n",
    "    'max_delta_step' : hp.quniform('max_delta_step',1,10,1),\n",
    "    'max_depth' : hp.quniform('max_depth',6, 12, 1),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight',10 ,500 ,5),\n",
    "    'reg_alpha' : hp.uniform('reg_alpha',0.1,100),\n",
    "    'reg_lambda' : hp.uniform('reg_lambda',0.1,100),\n",
    "    'subsample' : hp.uniform('subsample',0.1,1.0),\n",
    "    'max_bin' : hp.quniform('max_bin',16,256,16)\n",
    "    # add sampling method,max bin,predicto,monotone_constraints,interaction_constraints,single_precision_histogram\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin_objective = partial(objective, train_sets = training_list, val_sets = validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials():\n",
    "\n",
    "    trials_step = 1  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    max_trials = 1  # initial max_trials. put something small to not have to wait\n",
    "\n",
    "    \n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(\"xgboost.hyperopt\", \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    best = fmin(fn = fmin_objective, space = fspace, algo=tpe.suggest, max_evals=max_trials, trials=trials)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    \n",
    "    # save the trials object\n",
    "    with open(\"xgboost.hyperopt\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "    return(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved Trials! Loading...\n",
      "Rerunning from 509 trials to 510 (+1) trials\n",
      "100%|█████████████████████████████████████████████| 510/510 [00:30<00:00, 16.47trial/s, best loss: -0.8434333737766622]\n",
      "Best: {'colsample_bylevel': 0.5612301667238877, 'colsample_bytree': 0.788688363076523, 'gamma': 0.35376030016117566, 'learning_rate': 0.4023692255888918, 'max_bin': 16.0, 'max_delta_step': 3.0, 'max_depth': 8.0, 'min_child_weight': 70.0, 'reg_alpha': 0.15030685758880047, 'reg_lambda': 15.311721955443915, 'subsample': 0.8303923929525608}\n"
     ]
    }
   ],
   "source": [
    "trials = run_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = trials.trials[0]['result']['loss']\n",
    "for i in range(1,len(trials.trials)):\n",
    "    if (trials.trials[i]['result']['loss'] <=  best_loss):\n",
    "        best_loss = trials.trials[i]['result']['loss']\n",
    "        index = i\n",
    "\n",
    "    \n",
    "best_params = trials.trials[index]['misc']['vals']\n",
    "hyper_params = {\n",
    "        \"colsample_bylevel\" : best_params['colsample_bylevel'][0],\n",
    "        \"colsample_bytree\" : best_params['colsample_bytree'][0],\n",
    "        \"gamma\" : best_params['gamma'][0],\n",
    "        \"eta\" : best_params['learning_rate'][0],\n",
    "        \"max_delta_step\" : int(best_params['max_delta_step'][0]),\n",
    "        \"max_depth\" : int(best_params['max_depth'][0]),\n",
    "        \"min_child_weight\" : int(best_params['min_child_weight'][0]),\n",
    "        \"alpha\" : best_params['reg_alpha'][0],\n",
    "        \"lambda\" : best_params['reg_lambda'][0],\n",
    "        \"subsample\" : best_params['subsample'][0],\n",
    "        \"eval_metric\":'auc',\n",
    "        \"objective\":'binary:logistic',\n",
    "        \"booster\":'gbtree',\n",
    "        \"tree_method\" : 'gpu_hist',\n",
    "        \"single_precision_histogram\" : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgb_hyper import XGB_hyper\n",
    "class_xgb = XGB_hyper(hyper_params)\n",
    "training_metrics = {}\n",
    "validation_metrics = {}\n",
    "test_metrics = {}\n",
    "dmatrix_test,test_labels = class_xgb.to_xgb_input(test_data)\n",
    "for i in range(len(training_list)):\n",
    "    dmatrix_train,train_labels = class_xgb.to_xgb_input(training_list[i])\n",
    "    dmatrix_val,val_labels = class_xgb.to_xgb_input(validation_list[i])\n",
    "    evalist = [(dmatrix_val,'eval'),(dmatrix_train,'train')]\n",
    "    assert len(validation_list[i].index.intersection(training_list[i].index)) == 0\n",
    "    model = class_xgb.build_model(dmatrix_train,evalist,True,300)\n",
    "    \n",
    "    y_pred_val = model.predict(dmatrix_val)\n",
    "    validation_metrics['Val_%s'%i] = calculate_metrics(np.array(val_labels),y_pred_val)\n",
    "    \n",
    "    #y_pred_test = model.predict(dmatrix_test)\n",
    "    #test_metrics['Fold_%s'%i] = calculate_metrics(np.array(test_labels),y_pred_test)\n",
    "    \n",
    "    y_pred_train = model.predict(dmatrix_train)\n",
    "    training_metrics['Train_%s'%i] = calculate_metrics(np.array(train_labels),y_pred_train)\n",
    "    \n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_metrics = pd.DataFrame(training_metrics)\n",
    "training_metrics['Average'] = training_metrics.mean(axis=1)\n",
    "training_metrics = training_metrics.T\n",
    "training_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_metrics = pd.DataFrame(validation_metrics)\n",
    "validation_metrics['Average'] = validation_metrics.mean(axis=1)\n",
    "validation_metrics = validation_metrics.T\n",
    "validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = df.loc[train_test_folds[0]]\n",
    "test_ = df.loc[train_test_folds[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = {}\n",
    "dmatrix_train,train_labels = class_xgb.to_xgb_input(train_)\n",
    "dmatrix_test,test_labels = class_xgb.to_xgb_input(test_)\n",
    "evalist = [(dmatrix_val,'eval'),(dmatrix_train,'train')]\n",
    "assert len(test_.index.intersection(train_.index)) == 0\n",
    "model = class_xgb.build_model(dmatrix_train,evalist,True,300)\n",
    "y_pred_test = model.predict(dmatrix_test)\n",
    "test_metrics['Test'] = calculate_metrics(np.array(test_labels),y_pred_test)\n",
    "test_metrics = pd.DataFrame(test_metrics)\n",
    "test_metrics = test_metrics.T\n",
    "test_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
