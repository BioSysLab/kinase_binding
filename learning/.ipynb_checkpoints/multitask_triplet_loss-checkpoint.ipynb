{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.callbacks import History, ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "from data_analysis import calculate_metrics, load_weights_and_evaluate\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, roc_auc_score, auc, average_precision_score\n",
    "#import scikitplot as skplt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from NGF.preprocessing import tensorise_smiles\n",
    "from custom_layers.model_creator import encode_smiles, stage_creator\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, concatenate,Flatten\n",
    "from keras.models import Model, load_model\n",
    "import dill\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import xgboost as xgb\n",
    "from math import ceil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(feature, squared):\n",
    "    \"\"\"Computes the pairwise distance matrix with numerical stability.\n",
    "\n",
    "    output[i, j] = || feature[i, :] - feature[j, :] ||_2\n",
    "\n",
    "    Args:\n",
    "      feature: 2-D Tensor of size [number of data, feature dimension].\n",
    "      squared: Boolean, whether or not to square the pairwise distances.\n",
    "\n",
    "    Returns:\n",
    "      pairwise_distances: 2-D Tensor of size [number of data, number of data].\n",
    "    \"\"\"\n",
    "    pairwise_distances_squared = math_ops.add(\n",
    "        math_ops.reduce_sum(math_ops.square(feature), axis=[1], keepdims=True),\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.square(array_ops.transpose(feature)),\n",
    "            axis=[0],\n",
    "            keepdims=True)) - 2.0 * math_ops.matmul(feature,\n",
    "                                                    array_ops.transpose(feature))\n",
    "\n",
    "    # Deal with numerical inaccuracies. Set small negatives to zero.\n",
    "    pairwise_distances_squared = math_ops.maximum(pairwise_distances_squared, 0.0)\n",
    "    # Get the mask where the zero distances are at.\n",
    "    error_mask = math_ops.less_equal(pairwise_distances_squared, 0.0)\n",
    "\n",
    "    # Optionally take the sqrt.\n",
    "    if squared:\n",
    "        pairwise_distances = pairwise_distances_squared\n",
    "    else:\n",
    "        pairwise_distances = math_ops.sqrt(\n",
    "            pairwise_distances_squared + math_ops.to_float(error_mask) * 1e-16)\n",
    "\n",
    "    # Undo conditionally adding 1e-16.\n",
    "    pairwise_distances = math_ops.multiply(\n",
    "        pairwise_distances, math_ops.to_float(math_ops.logical_not(error_mask)))\n",
    "\n",
    "    num_data = array_ops.shape(feature)[0]\n",
    "    # Explicitly set diagonals to zero.\n",
    "    mask_offdiagonals = array_ops.ones_like(pairwise_distances) - array_ops.diag(\n",
    "        array_ops.ones([num_data]))\n",
    "    pairwise_distances = math_ops.multiply(pairwise_distances, mask_offdiagonals)\n",
    "    return pairwise_distances\n",
    "\n",
    "def masked_maximum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the maximum.\n",
    "\n",
    "    Returns:\n",
    "      masked_maximums: N-D `Tensor`.\n",
    "        The maximized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_minimums = math_ops.reduce_min(data, dim, keepdims=True)\n",
    "    masked_maximums = math_ops.reduce_max(\n",
    "        math_ops.multiply(data - axis_minimums, mask), dim,\n",
    "        keepdims=True) + axis_minimums\n",
    "    return masked_maximums\n",
    "\n",
    "def masked_minimum(data, mask, dim=1):\n",
    "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
    "\n",
    "    Args:\n",
    "      data: 2-D float `Tensor` of size [n, m].\n",
    "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
    "      dim: The dimension over which to compute the minimum.\n",
    "\n",
    "    Returns:\n",
    "      masked_minimums: N-D `Tensor`.\n",
    "        The minimized dimension is of size 1 after the operation.\n",
    "    \"\"\"\n",
    "    axis_maximums = math_ops.reduce_max(data, dim, keepdims=True)\n",
    "    masked_minimums = math_ops.reduce_min(\n",
    "        math_ops.multiply(data - axis_maximums, mask), dim,\n",
    "        keepdims=True) + axis_maximums\n",
    "    return masked_minimums\n",
    "def triplet_loss_adapted_from_tf(y_true, y_pred):\n",
    "    del y_true\n",
    "    margin = 0.5\n",
    "    labels = y_pred[:, :1]\n",
    "    labels = tf.cast(labels, dtype='int32')\n",
    "    embeddings = y_pred[:, 1:]\n",
    "    \n",
    "    ### Code from Tensorflow function [tf.contrib.losses.metric_learning.triplet_semihard_loss] starts here:\n",
    "    \n",
    "    # Reshape [batch_size] label tensor to a [batch_size, 1] label tensor.\n",
    "    # lshape=array_ops.shape(labels)\n",
    "    # assert lshape.shape == 1\n",
    "    # labels = array_ops.reshape(labels, [lshape[0], 1])\n",
    "\n",
    "    # Build pairwise squared distance matrix.\n",
    "    pdist_matrix = pairwise_distance(embeddings, squared=False)\n",
    "    # Build pairwise binary adjacency matrix.\n",
    "    adjacency = math_ops.equal(labels, array_ops.transpose(labels))\n",
    "    # Invert so we can select negatives only.\n",
    "    adjacency_not = math_ops.logical_not(adjacency)\n",
    "\n",
    "    # global batch_size  \n",
    "    batch_size = array_ops.size(labels) # was 'array_ops.size(labels)'\n",
    "\n",
    "    # Compute the mask.\n",
    "    pdist_matrix_tile = array_ops.tile(pdist_matrix, [batch_size, 1])\n",
    "    mask = math_ops.logical_and(\n",
    "        array_ops.tile(adjacency_not, [batch_size, 1]),\n",
    "        math_ops.greater(\n",
    "            pdist_matrix_tile, array_ops.reshape(\n",
    "                array_ops.transpose(pdist_matrix), [-1, 1])))\n",
    "    mask_final = array_ops.reshape(\n",
    "        math_ops.greater(\n",
    "            math_ops.reduce_sum(\n",
    "                math_ops.cast(mask, dtype=dtypes.float32), 1, keepdims=True),\n",
    "            0.0), [batch_size, batch_size])\n",
    "    mask_final = array_ops.transpose(mask_final)\n",
    "\n",
    "    adjacency_not = math_ops.cast(adjacency_not, dtype=dtypes.float32)\n",
    "    mask = math_ops.cast(mask, dtype=dtypes.float32)\n",
    "\n",
    "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
    "    negatives_outside = array_ops.reshape(\n",
    "        masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])\n",
    "    negatives_outside = array_ops.transpose(negatives_outside)\n",
    "\n",
    "    # negatives_inside: largest D_an.\n",
    "    negatives_inside = array_ops.tile(\n",
    "        masked_maximum(pdist_matrix, adjacency_not), [1, batch_size])\n",
    "    semi_hard_negatives = array_ops.where(\n",
    "        mask_final, negatives_outside, negatives_inside)\n",
    "\n",
    "    loss_mat = math_ops.add(margin, pdist_matrix - semi_hard_negatives)\n",
    "\n",
    "    mask_positives = math_ops.cast(\n",
    "        adjacency, dtype=dtypes.float32) - array_ops.diag(\n",
    "        array_ops.ones([batch_size]))\n",
    "\n",
    "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
    "    #   in semihard, they take all positive pairs except the diagonal.\n",
    "    num_positives = math_ops.reduce_sum(mask_positives)\n",
    "\n",
    "    semi_hard_triplet_loss_distance = math_ops.truediv(\n",
    "        math_ops.reduce_sum(\n",
    "            math_ops.maximum(\n",
    "                math_ops.multiply(loss_mat, mask_positives), 0.0)),\n",
    "        num_positives,\n",
    "        name='triplet_semihard_loss')\n",
    "    \n",
    "    ### Code from Tensorflow function semi-hard triplet loss ENDS here.\n",
    "    return semi_hard_triplet_loss_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_1 = 'p38'\n",
    "base_path_1 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_1 = base_path_1+f'/data/{target_1}/data.csv'\n",
    "df_p38=pd.read_csv(data_fpath_1).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_1+f'/data/{target_1}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_p38 = dill.load(in_f)\n",
    "\n",
    "with open(base_path_1+f'/data/{target_1}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_p38 = dill.load(in_f)\n",
    "    \n",
    "target_2 = 'akt1'\n",
    "base_path_2 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_2 = base_path_2+f'/data/{target_2}/data.csv'\n",
    "df_akt1 = pd.read_csv(data_fpath_2).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_2+f'/data/{target_2}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_akt1 = dill.load(in_f)\n",
    "with open(base_path_2+f'/data/{target_2}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_akt1 = dill.load(in_f)\n",
    "    \n",
    "target_3 = 'pi3k'\n",
    "base_path_3 = f'C:/Users/tomas/Documents/GitHub/kinase_binding'\n",
    "\n",
    "data_fpath_3 = base_path_3+f'/data/{target_3}/data.csv'\n",
    "df_pi3k = pd.read_csv(data_fpath_3).set_index('biolab_index')\n",
    "\n",
    "with open(base_path_3+f'/data/{target_3}/train_val_folds.pkl', \"rb\") as in_f:\n",
    "    train_val_folds_pi3k = dill.load(in_f)\n",
    "with open(base_path_3+f'/data/{target_3}/train_test_folds.pkl', \"rb\") as in_f:\n",
    "    train_test_folds_pi3k = dill.load(in_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p38 = df_p38.loc[train_test_folds_p38[0]]            \n",
    "val_p38 = df_p38.loc[train_test_folds_p38[1]]\n",
    "                   \n",
    "\n",
    "train_akt1 = df_akt1.loc[train_test_folds_akt1[0]]           \n",
    "val_akt1 = df_akt1.loc[train_test_folds_akt1[1]]\n",
    "                   \n",
    "\n",
    "train_pi3k = df_pi3k.loc[train_test_folds_pi3k[0]]        \n",
    "val_pi3k = df_pi3k.loc[train_test_folds_pi3k[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random splits with sklearn (on our test set)\n",
    "df_p38 = df_p38.reset_index(drop=True)\n",
    "X_train_p38, X_val_p38, Y_train_p38, Y_val_p38 = train_test_split(df_p38.rdkit,\n",
    "                                                                  df_p38.Binary,\n",
    "                                                                  test_size = 0.15,\n",
    "                                                                  train_size = 0.85,\n",
    "                                                                  shuffle = True)\n",
    "random_train_p38 = pd.DataFrame(X_train_p38)\n",
    "random_train_p38['Binary'] = Y_train_p38\n",
    "random_val_p38 = pd.DataFrame(X_val_p38)\n",
    "random_val_p38['Binary'] = Y_val_p38\n",
    "del X_train_p38,X_val_p38,Y_train_p38,Y_val_p38\n",
    "\n",
    "\n",
    "df_akt1 = df_akt1.reset_index(drop=True)\n",
    "X_train_akt1, X_val_akt1, Y_train_akt1, Y_val_akt1 = train_test_split(df_akt1.rdkit,\n",
    "                                                                     df_akt1.Binary,\n",
    "                                                                     test_size = 0.15,\n",
    "                                                                     train_size = 0.85,\n",
    "                                                                     shuffle = True)\n",
    "random_train_akt1 = pd.DataFrame(X_train_akt1)\n",
    "random_train_akt1['Binary'] = Y_train_akt1\n",
    "random_val_akt1 = pd.DataFrame(X_val_akt1)\n",
    "random_val_akt1['Binary'] = Y_val_akt1\n",
    "del X_train_akt1,X_val_akt1,Y_train_akt1,Y_val_akt1\n",
    "\n",
    "\n",
    "df_pi3k = df_pi3k.reset_index(drop=True)\n",
    "X_train_pi3k, X_val_pi3k, Y_train_pi3k, Y_val_pi3k = train_test_split(df_pi3k.rdkit,\n",
    "                                                                      df_pi3k.Binary,\n",
    "                                                                      test_size = 0.15,\n",
    "                                                                      train_size = 0.85,\n",
    "                                                                      shuffle = True)\n",
    "random_train_pi3k = pd.DataFrame(X_train_pi3k)\n",
    "random_train_pi3k['Binary'] = Y_train_pi3k\n",
    "random_val_pi3k = pd.DataFrame(X_val_pi3k)\n",
    "random_val_pi3k['Binary'] = Y_val_pi3k\n",
    "del X_train_pi3k,X_val_pi3k,Y_train_pi3k,Y_val_pi3k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_generator_mining(bs,df_p1,df_p2,df_p3,\n",
    "                         Y_dummy_p1,Y_dummy_p2,Y_dummy_p3,\n",
    "                         smiles_list_p1,smiles_list_p2,smiles_list_p3,\n",
    "                         Y_p1,Y_p2,Y_p3,\n",
    "                         atoms_p1,bonds_p1,edges_p1,\n",
    "                         atoms_p2,bonds_p2,edges_p2,\n",
    "                         atoms_p3,bonds_p3,edges_p3,mode = 'train',aug = None):\n",
    "    \n",
    "    counter_1=int(0)\n",
    "    counter_2=int(0)\n",
    "    counter_3=int(0)\n",
    "    #Keep looping indefinetely\n",
    "    df_p1 = df_p1.reset_index(drop=True)\n",
    "    df_p2 = df_p2.reset_index(drop=True)\n",
    "    df_p3 = df_p3.reset_index(drop=True)\n",
    "    Y_p1 = np.array(list(Y_p1))\n",
    "    Y_p2 = np.array(list(Y_p2))\n",
    "    Y_p3 = np.array(list(Y_p3))\n",
    "    while True:\n",
    "        \n",
    "        #Initialize batches of inputs and outputs\n",
    "        ind1 = []\n",
    "        ind2 = []\n",
    "        ind3 = []\n",
    "        \n",
    "        d_p1=[]\n",
    "        d_p2=[]\n",
    "        d_p3=[]\n",
    "        \n",
    "        #Keep looping until we reach batch size\n",
    "        while len(ind3)<=bs: #doesn't matter if it is smi1 or smi2 since they have the same len\n",
    "            if counter_3==len(df_p3):\n",
    "                counter_3=int(0)\n",
    "                df_p3 = df_p3.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            smi_p3 = df_p3['rdkit'][counter_3]\n",
    "            ind3.append(smiles_list_p3.index(smi_p3))\n",
    "            d_p3.append(Y_dummy_p3[counter_3])\n",
    "            counter_3+=1\n",
    "            while len(ind1)<=bs:\n",
    "                if counter_1==len(df_p1):\n",
    "                    counter_1=int(0)\n",
    "                    df_p1 = df_p1.sample(frac=1).reset_index(drop=True)\n",
    "                \n",
    "                smi_p1 = df_p1['rdkit'][counter_1]\n",
    "                ind1.append(smiles_list_p1.index(smi_p1))\n",
    "                d_p1.append(Y_dummy_p1[counter_1])\n",
    "                counter_1+=1\n",
    "                \n",
    "                \n",
    "                while len(ind2)<=bs:\n",
    "            # check to see if you reached the end of the frame\n",
    "                    if counter_2==len(df_p2):\n",
    "                        counter_2=int(0)\n",
    "                        df_p2 = df_p2.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "                    smi_p2 = df_p2['rdkit'][counter_2]\n",
    "                    ind2.append(smiles_list_p2.index(smi_p2))\n",
    "                    d_p2.append(Y_dummy_p2[counter_2])\n",
    "                    counter_2+=1\n",
    "                # if we are evaluating we should now break from our \n",
    "                # loop to ensure we don't continue to fill up the batch from samples at the beginning of the file\n",
    "                \n",
    "                   \n",
    "            \n",
    "            \n",
    "\n",
    "            #smi_p1 = df_p1['rdkit'][counter]\n",
    "            #smi_p2 = df_p2['rdkit'][counter]\n",
    "            #smi_p3 = df_p3['rdkit'][counter]\n",
    "            #ind1.append(smiles_list_p1.index(smi_p1))\n",
    "            #ind2.append(smiles_list_p2.index(smi_p2))\n",
    "            #ind3.append(smiles_list_p3.index(smi_p3))\n",
    "            #d_p1.append(Y_dummy_p1[counter])\n",
    "            #d_p2.append(Y_dummy_p2[counter])\n",
    "            #d_p3.append(Y_dummy_p3[counter])\n",
    "            #counter+=1\n",
    "            \n",
    "        atoms_target_1 = np.array(atoms_p1[ind1],dtype = 'float32')\n",
    "        bonds_target_1 = np.array(bonds_p1[ind1],dtype = 'float32')\n",
    "        edges_target_1 = np.array(edges_p1[ind1],dtype = 'int32')\n",
    "        labels_target_1 = Y_p1[ind1]\n",
    "        \n",
    "        atoms_target_2 = np.array(atoms_p2[ind2],dtype = 'float32')\n",
    "        bonds_target_2 = np.array(bonds_p2[ind2],dtype = 'float32')\n",
    "        edges_target_2 = np.array(edges_p2[ind2],dtype = 'int32')\n",
    "        labels_target_2 = Y_p2[ind2]\n",
    "        \n",
    "        atoms_target_3 = np.array(atoms_p3[ind3],dtype = 'float32')\n",
    "        bonds_target_3 = np.array(bonds_p3[ind3],dtype = 'float32')\n",
    "        edges_target_3 = np.array(edges_p3[ind3],dtype = 'int32')\n",
    "        labels_target_3 = Y_p3[ind3]\n",
    "        \n",
    "        yield ({'atom_inputs_1':atoms_target_1,\n",
    "                'bond_inputs_1':bonds_target_1,\n",
    "                'edge_inputs_1':edges_target_1,\n",
    "                'labels_inputs_1':labels_target_1,\n",
    "                'atom_inputs_2':atoms_target_2,\n",
    "                'bond_inputs_2':bonds_target_2,\n",
    "                'edge_inputs_2':edges_target_2,\n",
    "                'labels_inputs_2':labels_target_2,\n",
    "                'atom_inputs_3':atoms_target_3,\n",
    "                'bond_inputs_3':bonds_target_3,\n",
    "                'edge_inputs_3':edges_target_3,\n",
    "                'labels_inputs_3':labels_target_3},{'Protein_1':np.array(d_p1,dtype = 'float32'),\n",
    "                                                    'Protein_2':np.array(d_p2,dtype = 'float32'),\n",
    "                                                    'Protein_3':np.array(d_p3,dtype = 'float32')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(encoder_params):\n",
    "        model_enc_1 = stage_creator(encoder_params, 1, conv=True)[0]\n",
    "        model_enc_2 = stage_creator(encoder_params, 2, conv=True)[0]\n",
    "        model_enc_3 = stage_creator(encoder_params, 3, conv=True)[0]\n",
    "\n",
    "        model_enc_fp_1 = stage_creator(encoder_params, 1, conv=False)[1]\n",
    "        model_enc_fp_2 = stage_creator(encoder_params, 2, conv=False)[1]\n",
    "        model_enc_fp_3 = stage_creator(encoder_params, 3, conv=False)[1]\n",
    "\n",
    "        atoms, bonds, edges = encode_smiles(encoder_params[\"max_atoms\"],\n",
    "                                            encoder_params[\"num_atom_features\"],\n",
    "                                            encoder_params[\"max_degree\"],\n",
    "                                            encoder_params[\"num_bond_features\"])\n",
    "\n",
    "        graph_conv_1 = model_enc_1([atoms, bonds, edges])\n",
    "        graph_conv_2 = model_enc_2([graph_conv_1, bonds, edges])\n",
    "        graph_conv_3 = model_enc_3([graph_conv_2, bonds, edges])\n",
    "\n",
    "        fingerprint_1 = model_enc_fp_1([graph_conv_1, bonds, edges])\n",
    "        fingerprint_1 = Lambda(lambda x: K.sum(x, axis=1), output_shape=lambda s: (s[0], s[2]))(fingerprint_1)\n",
    "\n",
    "        fingerprint_2 = model_enc_fp_2([graph_conv_2, bonds, edges])\n",
    "        fingerprint_2 = Lambda(lambda x: K.sum(x, axis=1), output_shape=lambda s: (s[0], s[2]))(fingerprint_2)\n",
    "\n",
    "        fingerprint_3 = model_enc_fp_3([graph_conv_3, bonds, edges])\n",
    "        fingerprint_3 = Lambda(lambda x: K.sum(x, axis=1), output_shape=lambda s: (s[0], s[2]))(fingerprint_3)\n",
    "\n",
    "        final_fingerprint = keras.layers.add([fingerprint_1, fingerprint_2, fingerprint_3])\n",
    "\n",
    "        return Model([atoms, bonds, edges], [final_fingerprint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_params, encoder, verbose=False):\n",
    "        atoms_1 = Input(name='atom_inputs_1',\n",
    "                      shape=(model_params['max_atoms'], model_params['num_atom_features']), dtype='float32')\n",
    "        bonds_1 = Input(name='bond_inputs_1', shape=(\n",
    "            model_params['max_atoms'], model_params['max_degree'], model_params['num_bond_features']),\n",
    "                      dtype='float32')\n",
    "        edges_1 = Input(name='edge_inputs_1', shape=(model_params['max_atoms'], model_params['max_degree']),\n",
    "                      dtype='int32')\n",
    "        encode_drug_1 = encoder([atoms_1, bonds_1, edges_1])\n",
    "        \n",
    "        atoms_2 = Input(name='atom_inputs_2',\n",
    "                      shape=(model_params['max_atoms'], model_params['num_atom_features']), dtype='float32')\n",
    "        bonds_2 = Input(name='bond_inputs_2', shape=(\n",
    "            model_params['max_atoms'], model_params['max_degree'], model_params['num_bond_features']),\n",
    "                      dtype='float32')\n",
    "        edges_2 = Input(name='edge_inputs_2', shape=(model_params['max_atoms'], model_params['max_degree']),\n",
    "                      dtype='int32')\n",
    "        encode_drug_2 = encoder([atoms_2, bonds_2, edges_2])\n",
    "        \n",
    "        atoms_3 = Input(name='atom_inputs_3',\n",
    "                      shape=(model_params['max_atoms'], model_params['num_atom_features']), dtype='float32')\n",
    "        bonds_3 = Input(name='bond_inputs_3', shape=(\n",
    "            model_params['max_atoms'], model_params['max_degree'], model_params['num_bond_features']),\n",
    "                      dtype='float32')\n",
    "        edges_3 = Input(name='edge_inputs_3', shape=(model_params['max_atoms'], model_params['max_degree']),\n",
    "                      dtype='int32')\n",
    "        encode_drug_3 = encoder([atoms_3, bonds_3, edges_3])\n",
    "        \n",
    "        conc = keras.layers.Concatenate(axis = -1)([encode_drug_1,encode_drug_2,encode_drug_3])\n",
    "        # Fully connected\n",
    "        FC1 = Dense(model_params[\"dense_size\"][0], activation='relu',kernel_initializer='random_normal')(conc)\n",
    "        FC2 = Dropout(model_params[\"dropout_rate\"][0])(FC1)\n",
    "        FC2 = Dense(model_params[\"dense_size\"][1], activation='relu',kernel_initializer='random_normal')(FC2)\n",
    "        FC2 = Dropout(model_params[\"dropout_rate\"][1])(FC2)\n",
    "        FC2 = Dense(model_params[\"dense_size\"][2], activation = None,kernel_initializer='random_normal')(FC2)\n",
    "        \n",
    "        \n",
    "        embeddings_conc = Lambda(lambda x: K.l2_normalize(x,axis=1),name = 'Embeddings_Concatenated')(FC2)\n",
    "        embeddings_1 = crop(1,0,int(model_params['dense_size'][2]/3),name = 'Target_1')(embeddings_conc) #0 to 255(256)\n",
    "        embeddings_2 = crop(1,int(model_params['dense_size'][2]/3),2*int(model_params['dense_size'][2]/3),name = 'Target_2')(embeddings_conc) #256 to 511(256)\n",
    "        embeddings_3 = crop(1,2*int(model_params['dense_size'][2]/3),3*int(model_params['dense_size'][2]/3),name = 'Target_3')(embeddings_conc)\n",
    "        \n",
    "        gcn_model = Model(inputs=[atoms_1, bonds_1, edges_1,\n",
    "                                  atoms_2, bonds_2, edges_2,\n",
    "                                  atoms_3, bonds_3, edges_3], \n",
    "                                  outputs = [embeddings_1,\n",
    "                                             embeddings_2,\n",
    "                                             embeddings_3]\n",
    "                         )\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            #print('encoder')\n",
    "            #encoder.summary()\n",
    "            print('GCN_model')\n",
    "            gcn_model.summary()\n",
    "        \n",
    "            \n",
    "        return gcn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mining(model_params,gcn_model):\n",
    "        #First Target\n",
    "        atoms_1 = Input(name='atom_inputs_1',shape=(model_params['max_atoms'], \n",
    "                                                    model_params['num_atom_features']), \n",
    "                                                    dtype='float32')\n",
    "        bonds_1 = Input(name='bond_inputs_1', shape=(model_params['max_atoms'], \n",
    "                                                     model_params['max_degree'], \n",
    "                                                     model_params['num_bond_features']),\n",
    "                                                     dtype='float32')\n",
    "        edges_1 = Input(name='edge_inputs_1', shape=(model_params['max_atoms'], \n",
    "                                                     model_params['max_degree']),\n",
    "                                                     dtype='int32')\n",
    "        \n",
    "        labels_1 = Input(name = 'labels_inputs_1',shape = (1,),dtype = 'float32')\n",
    "        \n",
    "        #Second Target\n",
    "        atoms_2 = Input(name='atom_inputs_2',shape=(model_params['max_atoms'], \n",
    "                                                    model_params['num_atom_features']), \n",
    "                                                    dtype='float32')\n",
    "        bonds_2 = Input(name='bond_inputs_2', shape=(model_params['max_atoms'], \n",
    "                                                     model_params['max_degree'], \n",
    "                                                     model_params['num_bond_features']),\n",
    "                                                     dtype='float32')\n",
    "        edges_2 = Input(name='edge_inputs_2', shape=(model_params['max_atoms'], \n",
    "                                                     model_params['max_degree']),\n",
    "                                                     dtype='int32')\n",
    "        \n",
    "        labels_2 = Input(name = 'labels_inputs_2',shape = (1,),dtype = 'float32')\n",
    "        \n",
    "        #Third Target\n",
    "        atoms_3 = Input(name='atom_inputs_3',shape=(model_params['max_atoms'], \n",
    "                                                    model_params['num_atom_features']), \n",
    "                                                    dtype='float32')\n",
    "        bonds_3 = Input(name='bond_inputs_3', shape=(model_params['max_atoms'], \n",
    "                                                     model_params['max_degree'], \n",
    "                                                     model_params['num_bond_features']),\n",
    "                                                     dtype='float32')\n",
    "        edges_3 = Input(name='edge_inputs_3', shape=(model_params['max_atoms'], model_params['max_degree']),\n",
    "                      dtype='int32')\n",
    "        \n",
    "        labels_3 = Input(name = 'labels_inputs_3',shape = (1,),dtype = 'float32')\n",
    "        encoded_1,encoded_2,encoded_3 = gcn_model([atoms_1,bonds_1,edges_1,\n",
    "                                                   atoms_2,bonds_2,edges_2,\n",
    "                                                   atoms_3,bonds_3,edges_3])\n",
    "        \n",
    "        labels_plus_embeddings_1 = concatenate([labels_1, encoded_1],name = 'Protein_1')\n",
    "        labels_plus_embeddings_2 = concatenate([labels_2, encoded_2],name = 'Protein_2')\n",
    "        labels_plus_embeddings_3 = concatenate([labels_3, encoded_3],name = 'Protein_3')\n",
    "        \n",
    "        mining_net = Model(inputs = [atoms_1,bonds_1,edges_1,labels_1,\n",
    "                                     atoms_2,bonds_2,edges_2,labels_2,\n",
    "                                     atoms_3,bonds_3,edges_3,labels_3],\n",
    "                           outputs = [labels_plus_embeddings_1,\n",
    "                                      labels_plus_embeddings_2,\n",
    "                                      labels_plus_embeddings_3])\n",
    "        adam = keras.optimizers.Adam(lr = model_params[\"lr\"], \n",
    "                                     beta_1=0.9, \n",
    "                                     beta_2=0.999, \n",
    "                                     decay=0.0, \n",
    "                                     amsgrad=False)\n",
    "    \n",
    "    \n",
    "        mining_net.compile(optimizer=adam , loss = {'Protein_1' : triplet_loss_adapted_from_tf,\n",
    "                                                    'Protein_2' : triplet_loss_adapted_from_tf,\n",
    "                                                    'Protein_3' : triplet_loss_adapted_from_tf})\n",
    "        mining_net.summary()\n",
    "        return mining_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_gcn_input(input_data):\n",
    "        x_atoms_cold, x_bonds_cold, x_edges_cold = tensorise_smiles(input_data['rdkit'],\n",
    "                                                                    max_degree=model_params['max_degree'],\n",
    "                                                                    max_atoms=model_params['max_atoms'])\n",
    "        return [x_atoms_cold, x_bonds_cold, x_edges_cold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss',patience=8, min_delta=0)\n",
    "rlr = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=4, verbose=1, min_lr=0.0000001)\n",
    "model_params = {\n",
    "        \"num_layers\" : 3,\n",
    "        \"max_atoms\" : 70,\n",
    "        \"num_atom_features\" : 62,\n",
    "        \"num_atom_features_original\" : 62,\n",
    "        \"num_bond_features\" : 6,\n",
    "        \"max_degree\" : 5,\n",
    "        \"conv_width\" : [int(96), int(104), int(120)],\n",
    "        \"fp_length\" : [int(160), int(160), int(160)],\n",
    "        \"activ_enc\" : \"selu\",\n",
    "        \"activ_dec\" : \"selu\",\n",
    "        \"learning_rates\" : [0.001,0.001,0.001],\n",
    "        \"learning_rates_fp\": [0.005,0.005,0.005],\n",
    "        \"losses_conv\" : {\n",
    "                    \"neighbor_output\": \"mean_squared_error\",\n",
    "                    \"self_output\": \"mean_squared_error\",\n",
    "                    },\n",
    "        \"lossWeights\" : {\"neighbor_output\": 1.0, \"self_output\": 1.0},\n",
    "        \"metrics\" : \"mse\",\n",
    "        \"loss_fp\" : \"mean_squared_error\",\n",
    "        \"enc_layer_names\" : [\"enc_1\", \"enc_2\", \"enc_3\"],\n",
    "        'callbacks' : [es,rlr],\n",
    "        'adam_decay': 0.0005329142291371636,\n",
    "        'beta': 5,\n",
    "        'p': 0.004465204118126482,\n",
    "        'dense_size' : [int(3*160), int(3*160), int(3*96)], \n",
    "        'dropout_rate' : [0.27225175676555935, 0.27225175676555935],\n",
    "        'lr' : 0.008110012706176706,\n",
    "        'batch_size' : int(256),\n",
    "        'n_epochs' : int(20),\n",
    "        'margin' : 1\n",
    "        }\n",
    "xgb_hyper = {\n",
    "        \"colsample_bylevel\" : 0.4371082812232264,\n",
    "        \"colsample_bytree\" : 0.4179415558635843,\n",
    "        \"gamma\" : 0.919836526180396,\n",
    "        \"eta\" : 0.41409388868400826,\n",
    "        \"max_delta_step\" : int(2),\n",
    "        \"max_depth\" : int(7),\n",
    "        \"min_child_weight\" : int(20),\n",
    "        \"alpha\" : 0.15030685758880047,\n",
    "        \"lambda\" : 12.306130216692438,\n",
    "        \"subsample\" : 0.6038298323514097,\n",
    "        \"max_bin\" : int(48.0),\n",
    "        \"eval_metric\":'auc',\n",
    "        \"objective\":'binary:logistic',\n",
    "        \"booster\":'gbtree'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(dimension, start, end,name):\n",
    "    # Crops (or slices) a Tensor on a given dimension from start to end\n",
    "    # example : to crop tensor x[:, :, 5:10]\n",
    "    # call slice(2, 5, 10) as you want to crop on the second dimension\n",
    "    def func(x):\n",
    "        if dimension == 0:\n",
    "            return x[start: end]\n",
    "        if dimension == 1:\n",
    "            return x[:, start: end]\n",
    "        if dimension == 2:\n",
    "            return x[:, :, start: end]\n",
    "        if dimension == 3:\n",
    "            return x[:, :, :, start: end]\n",
    "        if dimension == 4:\n",
    "            return x[:, :, :, :, start: end]\n",
    "    return Lambda(func,name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_p38 = [train_p38,random_train_p38]\n",
    "validation_p38 = [val_p38,random_val_p38]\n",
    "\n",
    "training_akt1 = [train_akt1,random_train_akt1]\n",
    "validation_akt1 = [val_akt1,random_val_akt1]\n",
    "\n",
    "training_pi3k = [train_pi3k,random_train_pi3k]\n",
    "validation_pi3k = [val_pi3k,random_val_pi3k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_p38 = {}\n",
    "eval_akt1 = {}\n",
    "eval_pi3k = {}\n",
    "es2 = EarlyStopping(monitor='loss',patience=15, min_delta=0)\n",
    "rlr2 = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=2, verbose=1, min_lr=0.00001)\n",
    "for i in range(len(training_p38)):\n",
    "    \n",
    "        #First Target\n",
    "        X_atoms_cold_p38,X_bonds_cold_p38,X_edges_cold_p38 = dataframe_to_gcn_input(validation_p38[i])\n",
    "        Y_cold_p38 = validation_p38[i].Binary\n",
    "        Y_dummy_cold_p38 = np.empty((X_atoms_cold_p38.shape[0],int(model_params['dense_size'][2]/3)+1))\n",
    "        X_atoms_train_p38, X_bonds_train_p38, X_edges_train_p38 = dataframe_to_gcn_input(training_p38[i])\n",
    "        Y_p38 = training_p38[i].Binary\n",
    "        Y_dummy_train_p38 = np.empty((X_atoms_train_p38.shape[0],int(model_params['dense_size'][2]/3)+1))\n",
    "        smiles_list_p38 = list(training_p38[i]['rdkit'])\n",
    "        \n",
    "        #Second Target\n",
    "        X_atoms_cold_akt1, X_bonds_cold_akt1, X_edges_cold_akt1 = dataframe_to_gcn_input(validation_akt1[i])\n",
    "        Y_cold_akt1 = validation_akt1[i].Binary\n",
    "        Y_dummy_cold_akt1 = np.empty((X_atoms_cold_akt1.shape[0],int(model_params['dense_size'][2]/3)+1))\n",
    "        X_atoms_train_akt1, X_bonds_train_akt1, X_edges_train_akt1 = dataframe_to_gcn_input(training_akt1[i])\n",
    "        Y_akt1 = training_akt1[i].Binary\n",
    "        Y_dummy_train_akt1 = np.empty((X_atoms_train_akt1.shape[0],int(model_params['dense_size'][2]/3)+1))\n",
    "        smiles_list_akt1 = list(training_akt1[i]['rdkit'])\n",
    "        \n",
    "        #Third Target\n",
    "        X_atoms_cold_pi3k,X_bonds_cold_pi3k,X_edges_cold_pi3k = dataframe_to_gcn_input(validation_pi3k[i])\n",
    "        Y_cold_pi3k = validation_pi3k[i].Binary\n",
    "        Y_dummy_cold_pi3k = np.empty((X_atoms_cold_pi3k.shape[0],int(model_params['dense_size'][2]/3)+1))\n",
    "        X_atoms_train_pi3k, X_bonds_train_pi3k, X_edges_train_pi3k = dataframe_to_gcn_input(training_pi3k[i])\n",
    "        Y_pi3k = training_pi3k[i].Binary\n",
    "        Y_dummy_train_pi3k = np.empty((X_atoms_train_pi3k.shape[0],int(model_params['dense_size'][2]/3)+1))\n",
    "        smiles_list_pi3k = list(training_pi3k[i]['rdkit'])\n",
    "        \n",
    "        gcn_encoder = build_encoder(model_params)\n",
    "        gcn_model = build_model(model_params,gcn_encoder)\n",
    "        gcn_mining = build_mining(model_params,gcn_model)\n",
    "        train_GEN = csv_generator_mining(model_params['batch_size'],training_p38[i],training_akt1[i],training_pi3k[i],\n",
    "                                  Y_dummy_train_p38,Y_dummy_train_akt1,Y_dummy_train_pi3k,\n",
    "                                  smiles_list_p38,smiles_list_akt1,smiles_list_pi3k,\n",
    "                                  Y_p38,Y_akt1,Y_pi3k,\n",
    "                                  X_atoms_train_p38, X_bonds_train_p38, X_edges_train_p38,\n",
    "                                  X_atoms_train_akt1, X_bonds_train_akt1, X_edges_train_akt1,\n",
    "                                  X_atoms_train_pi3k, X_bonds_train_pi3k, X_edges_train_pi3k)\n",
    "        NUM_TRAIN = np.sum([len(training_p38[i]),len(training_akt1[i]),len(training_pi3k[i])])\n",
    "        gcn_mining.fit_generator(train_GEN,\n",
    "                      steps_per_epoch = ceil(3*NUM_TRAIN/model_params['batch_size']),\n",
    "                      epochs = model_params['n_epochs'],\n",
    "                      shuffle = True,\n",
    "                      validation_data = None,\n",
    "                      callbacks = [es2,rlr2])\n",
    "        emb_train_1,t1,t2 = gcn_model.predict([X_atoms_train_p38,X_bonds_train_p38,X_edges_train_p38,\n",
    "                                        X_atoms_train_p38,X_bonds_train_p38,X_edges_train_p38,\n",
    "                                        X_atoms_train_p38,X_bonds_train_p38,X_edges_train_p38])\n",
    "        t1,emb_train_2,t2 = gcn_model.predict([X_atoms_train_akt1,X_bonds_train_akt1,X_edges_train_akt1,\n",
    "                                        X_atoms_train_akt1,X_bonds_train_akt1,X_edges_train_akt1,\n",
    "                                        X_atoms_train_akt1,X_bonds_train_akt1,X_edges_train_akt1])\n",
    "        t1,t2,emb_train_3 = gcn_model.predict([X_atoms_train_pi3k,X_bonds_train_pi3k,X_edges_train_pi3k,\n",
    "                                        X_atoms_train_pi3k,X_bonds_train_pi3k,X_edges_train_pi3k,\n",
    "                                        X_atoms_train_pi3k,X_bonds_train_pi3k,X_edges_train_pi3k])\n",
    "\n",
    "\n",
    "        emb_val_1,t1,t2 = gcn_model.predict([X_atoms_cold_p38,X_bonds_cold_p38,X_edges_cold_p38,\n",
    "                                            X_atoms_cold_p38,X_bonds_cold_p38,X_edges_cold_p38,\n",
    "                                            X_atoms_cold_p38,X_bonds_cold_p38,X_edges_cold_p38])\n",
    "        t1,emb_val_2,t2 = gcn_model.predict([X_atoms_cold_akt1,X_bonds_cold_akt1,X_edges_cold_akt1,\n",
    "                                            X_atoms_cold_akt1,X_bonds_cold_akt1,X_edges_cold_akt1,\n",
    "                                            X_atoms_cold_akt1,X_bonds_cold_akt1,X_edges_cold_akt1])\n",
    "        t1,t2,emb_val_3 = gcn_model.predict([X_atoms_cold_pi3k,X_bonds_cold_pi3k,X_edges_cold_pi3k,\n",
    "                                            X_atoms_cold_pi3k,X_bonds_cold_pi3k,X_edges_cold_pi3k,\n",
    "                                            X_atoms_cold_pi3k,X_bonds_cold_pi3k,X_edges_cold_pi3k])\n",
    "        \n",
    "        del t1,t2, gcn_mining,gcn_encoder,gcn_model,X_atoms_cold_p38,X_bonds_cold_p38,X_edges_cold_p38\n",
    "        del X_atoms_train_p38, X_bonds_train_p38, X_edges_train_p38,X_atoms_cold_pi3k,X_bonds_cold_pi3k,X_edges_cold_pi3k\n",
    "        del X_atoms_train_pi3k, X_bonds_train_pi3k, X_edges_train_pi3k,X_atoms_cold_akt1, X_bonds_cold_akt1, X_edges_cold_akt1\n",
    "        del X_atoms_train_akt1, X_bonds_train_akt1, X_edges_train_akt1,train_GEN\n",
    "        \n",
    "        #First\n",
    "        dmatrix_train_1 = xgb.DMatrix(data = emb_train_1,label = Y_p38)\n",
    "        dmatrix_cold_1  = xgb.DMatrix(data = emb_val_1,label = Y_cold_p38)\n",
    "        evalist_1 = [(dmatrix_train_1,'train'),(dmatrix_cold_1,'eval')]\n",
    "        xgb_model_1 = xgb.train(xgb_hyper,dmatrix_train_1,300,evalist_1,verbose_eval=True)\n",
    "        xgb_pred_cold_1 = xgb_model_1.predict(dmatrix_cold_1)\n",
    "        \n",
    "        \n",
    "        #Second\n",
    "        dmatrix_train_2 = xgb.DMatrix(data = emb_train_2,label = Y_akt1)\n",
    "        dmatrix_cold_2  = xgb.DMatrix(data = emb_val_2,label = Y_cold_akt1)\n",
    "        evalist_2 = [(dmatrix_train_2,'train'),(dmatrix_cold_2,'eval')]\n",
    "        xgb_model_2 = xgb.train(xgb_hyper,dmatrix_train_2,300,evalist_2,verbose_eval=True)\n",
    "        xgb_pred_cold_2 = xgb_model_2.predict(dmatrix_cold_2)\n",
    "        \n",
    "        \n",
    "        #Third\n",
    "        dmatrix_train_3 = xgb.DMatrix(data = emb_train_3,label = Y_pi3k)\n",
    "        dmatrix_cold_3  = xgb.DMatrix(data = emb_val_3,label = Y_cold_pi3k)\n",
    "        evalist_3 = [(dmatrix_train_3,'train'),(dmatrix_cold_3,'eval')]\n",
    "        xgb_model_3 = xgb.train(xgb_hyper,dmatrix_train_3,300,evalist_3,verbose_eval=True)\n",
    "        xgb_pred_cold_3 = xgb_model_3.predict(dmatrix_cold_3)\n",
    "        \n",
    "        if i == 0:\n",
    "            eval_p38['Test'] = calculate_metrics(np.array(Y_cold_p38),xgb_pred_cold_1)\n",
    "            eval_akt1['Test'] = calculate_metrics(np.array(Y_cold_akt1),xgb_pred_cold_2)\n",
    "            eval_pi3k['Test'] = calculate_metrics(np.array(Y_cold_pi3k),xgb_pred_cold_3)\n",
    "        elif i == 1:\n",
    "            eval_p38['Random'] = calculate_metrics(np.array(Y_cold_p38),xgb_pred_cold_1)\n",
    "            eval_akt1['Random'] = calculate_metrics(np.array(Y_cold_akt1),xgb_pred_cold_2)\n",
    "            eval_pi3k['Random'] = calculate_metrics(np.array(Y_cold_pi3k),xgb_pred_cold_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_p38 = pd.DataFrame(eval_p38).T\n",
    "eval_p38.to_csv('../../../../Desktop/binding/thesis english/Results/4-Multitask/Online/p38.csv')\n",
    "\n",
    "eval_akt1 = pd.DataFrame(eval_akt1).T\n",
    "eval_akt1.to_csv('../../../../Desktop/binding/thesis english/Results/4-Multitask/Online/akt1.csv')\n",
    "\n",
    "eval_pi3k = pd.DataFrame(eval_pi3k).T\n",
    "eval_pi3k.to_csv('../../../../Desktop/binding/thesis english/Results/4-Multitask/Online/pi3k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_p38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_akt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pi3k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
